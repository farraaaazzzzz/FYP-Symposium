{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31147c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 1: Load All Files and Print Columns Neatly\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "\n",
    "# Set paths\n",
    "merged_log_path = r'E:\\FYP\\FYP Symposium\\Merged Log (49).xlsx'\n",
    "stock_data_path = r'E:\\FYP\\FYP Symposium\\Trading Simulation Experiment Data Turn Wise.xlsx'\n",
    "dta_path = r'E:\\FYP\\FYP Symposium\\SurveysClean.dta'\n",
    "\n",
    "# Load Data\n",
    "merged_log = pd.read_excel(merged_log_path)\n",
    "stock_xls = pd.ExcelFile(stock_data_path)\n",
    "stock_tables = {stock: pd.read_excel(stock_xls, sheet_name=stock) for stock in ['TSLA', 'XOM', 'NFLX', 'PG']}\n",
    "strategy = pd.read_stata(dta_path)\n",
    "\n",
    "# Rename columns in each stock sheet\n",
    "rename_map = {\n",
    "    'trend': 'Close_price_diff',\n",
    "    'trend direction': 'price_trend_1',\n",
    "    'trend summary': 'price_trend_7',\n",
    "    'volume trend change': 'volume_diff',\n",
    "    'volume trend direction': 'volume_trend_1',\n",
    "    'volume trend summary': 'volume_trend_7',\n",
    "    'Technical Decision': 'MACD_trend',\n",
    "    'Bollinger Classification': 'bollinger_trend'\n",
    "}\n",
    "\n",
    "for stock_name, df in stock_tables.items():\n",
    "    stock_tables[stock_name].rename(columns=rename_map, inplace=True)\n",
    "\n",
    "\n",
    "# Show loaded columns for verification (pretty print using tabulate)\n",
    "print(\"Merged Log Columns:\\n\")\n",
    "print(tabulate([[col] for col in merged_log.columns], headers=[\"Merged Log Columns\"], tablefmt=\"github\"))\n",
    "\n",
    "print(\"\\nStrategy Columns:\\n\")\n",
    "print(tabulate([[col] for col in strategy.columns], headers=[\"Strategy Columns\"], tablefmt=\"github\"))\n",
    "\n",
    "print(merged_log.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ec9120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 2: Rename Columns for Consistency\n",
    "\n",
    "# Rename Merged Log Columns to match our expectations\n",
    "merged_log.rename(columns={\n",
    "    'News Sentiment': 'news_sentiment',\n",
    "    'News Truth': 'news_truth'\n",
    "}, inplace=True)\n",
    "\n",
    "# Rename Strategy Columns if needed\n",
    "if 'participant_id' not in strategy.columns:\n",
    "    if 'ResponseID' in strategy.columns:\n",
    "        strategy.rename(columns={'ResponseID': 'participant_id'}, inplace=True)\n",
    "    elif 'ResponseId' in strategy.columns:\n",
    "        strategy.rename(columns={'ResponseId': 'participant_id'}, inplace=True)\n",
    "\n",
    "# Check after renaming\n",
    "print(\"Merged Log Columns (After Rename):\\n\")\n",
    "print(tabulate([[col] for col in merged_log.columns], headers=[\"Merged Log Columns\"], tablefmt=\"github\"))\n",
    "\n",
    "print(\"\\nStrategy Columns (After Rename):\\n\")\n",
    "print(tabulate([[col] for col in strategy.columns], headers=[\"Strategy Columns\"], tablefmt=\"github\"))\n",
    "\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming merged_log and strategy are already available after renaming\n",
    "\n",
    "# Export both merged_log and strategy to Excel with separate sheets\n",
    "output_file = r'E:\\FYP\\FYP Symposium\\Outputs\\Renamed_Merged_Log_and_Strategy.xlsx'  # Specify the file path\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "    merged_log.to_excel(writer, sheet_name='Merged_Log', index=False)  # Save merged_log to the first sheet\n",
    "    strategy.to_excel(writer, sheet_name='Strategy', index=False)  # Save strategy to the second sheet\n",
    "\n",
    "print(f\"The renamed merged_log and strategy have been exported to: {output_file}\")\n",
    "strategy.head(15)\n",
    "\n",
    "# Define scoring function\n",
    "def assign_score(row, method_name):\n",
    "    score = 0\n",
    "    for i, weight in zip(['1st', '2nd', '3rd'], [5, 3.5, 2]):\n",
    "        factor = row.get(f'DecisionFactor_{i}', None)\n",
    "        if factor == method_name:\n",
    "            score = weight\n",
    "            break\n",
    "    return score\n",
    "\n",
    "# Add scoring for 'Graph'\n",
    "strategy['Graph Scoring'] = strategy.apply(lambda row: assign_score(row, 'Graph'), axis=1)\n",
    "\n",
    "# Add scoring for 'Data Table'\n",
    "strategy['Data Table Scoring'] = strategy.apply(lambda row: assign_score(row, 'Data Table'), axis=1)\n",
    "\n",
    "# Add column for average of Graph Scoring and Data Table Scoring\n",
    "strategy['Average Scoring'] = strategy[['Graph Scoring', 'Data Table Scoring']].mean(axis=1)\n",
    "\n",
    "# View result\n",
    "print(strategy[['Participant_ID', 'DecisionFactor_1st', 'DecisionFactor_2nd', 'DecisionFactor_3rd',\n",
    "                'Graph Scoring', 'Data Table Scoring', 'Average Scoring']].head(10))\n",
    "\n",
    "# Final export with both sheets: updated strategy and merged_log\n",
    "final_output_file = r'E:\\FYP\\FYP Symposium\\Outputs\\Final_Merged_Log_and_Strategy_With_Scoring.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(final_output_file, engine='xlsxwriter') as writer:\n",
    "    merged_log.to_excel(writer, sheet_name='Merged_Log', index=False)\n",
    "    strategy.to_excel(writer, sheet_name='Strategy', index=False)\n",
    "\n",
    "print(f\"✅ Both updated sheets exported to: {final_output_file}\")\n",
    "\n",
    "print(\"The updated strategy with scoring has been exported to: E:\\\\FYP\\\\FYP Symposium\\\\Outputs\\\\Updated_Strategy_With_Scoring.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5e1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BLOCK 3: Extract PRM2b_14a indicator knowledge cleanly\n",
    "\n",
    "def check_knowledge(prm2b_14a_response):\n",
    "    if pd.isna(prm2b_14a_response):\n",
    "        return {'MACD': False, 'Bollinger': False, 'Volume': False, 'HighLow_OpenClose': False}\n",
    "\n",
    "    text = prm2b_14a_response.lower()\n",
    "    \n",
    "    knowledge = {\n",
    "        'MACD': False,\n",
    "        'Bollinger': False,\n",
    "        'Volume': False,\n",
    "        'HighLow_OpenClose': False\n",
    "    }\n",
    "\n",
    "    # Check MACD knowledge\n",
    "    if 'macd' in text:\n",
    "        knowledge['MACD'] = True\n",
    "    \n",
    "    # Check Bollinger Bands\n",
    "    if 'bollinger' in text or 'bb' in text:\n",
    "        knowledge['Bollinger'] = True\n",
    "\n",
    "    # Check Volume\n",
    "    if 'volume' in text:\n",
    "        knowledge['Volume'] = True\n",
    "\n",
    "    # Check High Low Open Close\n",
    "    if 'high' in text or 'low' in text or 'open' in text or 'close' in text or 'ohlc' in text:\n",
    "        knowledge['HighLow_OpenClose'] = True\n",
    "\n",
    "    return knowledge\n",
    "\n",
    "# --- Apply to all participants ---\n",
    "\n",
    "knowledge_records = []\n",
    "\n",
    "for idx, row in strategy.iterrows():\n",
    "    participant_id = row['Participant_ID']\n",
    "    prm2b_14a = row.get('PRM2b_14a', None)\n",
    "    knowledge = check_knowledge(prm2b_14a)\n",
    "\n",
    "    knowledge_records.append({\n",
    "        'Participant_ID': participant_id,\n",
    "        'Knows_MACD': 'Yes' if knowledge['MACD'] else 'No',\n",
    "        'Knows_Bollinger': 'Yes' if knowledge['Bollinger'] else 'No',\n",
    "        'Knows_Volume': 'Yes' if knowledge['Volume'] else 'No',\n",
    "        'Knows_HighLow_OpenClose': 'Yes' if knowledge['HighLow_OpenClose'] else 'No'\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "knowledge_df = pd.DataFrame(knowledge_records)\n",
    "\n",
    "# Drop 'Knows_HighLow_OpenClose' — no longer needed\n",
    "if 'Knows_HighLow_OpenClose' in knowledge_df.columns:\n",
    "    knowledge_df.drop(columns=['Knows_HighLow_OpenClose'], inplace=True)\n",
    "\n",
    "# --- Merge Cleaned Knowledge into Strategy ---\n",
    "\n",
    "# Drop old versions to avoid MergeError\n",
    "strategy.drop(columns=['Knows_MACD', 'Knows_Bollinger', 'Knows_Volume', 'Knows_HighLow_OpenClose', 'Knowledge_Score'], errors='ignore', inplace=True)\n",
    "\n",
    "# Merge new knowledge\n",
    "strategy = pd.merge(strategy, knowledge_df, on='Participant_ID', how='left')\n",
    "print(\"\\n✅ Merged updated knowledge into strategy successfully.\")\n",
    "\n",
    "# --- Scoring Logic ---\n",
    "\n",
    "# Map Yes/No with new scoring logic (Volume: Yes = 1, No = 0.5)\n",
    "knowledge_df_numeric = knowledge_df.copy()\n",
    "knowledge_df_numeric['Knows_MACD'] = knowledge_df_numeric['Knows_MACD'].map({'Yes': 1, 'No': 0})\n",
    "knowledge_df_numeric['Knows_Bollinger'] = knowledge_df_numeric['Knows_Bollinger'].map({'Yes': 1, 'No': 0})\n",
    "knowledge_df_numeric['Knows_Volume'] = knowledge_df_numeric['Knows_Volume'].map({'Yes': 1, 'No': 0.5})\n",
    "\n",
    "# Compute score\n",
    "knowledge_df['Knowledge_Score'] = (\n",
    "    knowledge_df_numeric['Knows_MACD'] +\n",
    "    knowledge_df_numeric['Knows_Bollinger'] +\n",
    "    knowledge_df_numeric['Knows_Volume']\n",
    ")\n",
    "\n",
    "# Show sample output\n",
    "print(\"\\n✅ Final Knowledge Score (range 0.5–3.0):\")\n",
    "display(knowledge_df[['Participant_ID', 'Knows_MACD', 'Knows_Bollinger', 'Knows_Volume', 'Knowledge_Score']].head())\n",
    "\n",
    "# Optional documentation string\n",
    "knowledge_score_note = \"Knowledge_Score = Knows_MACD (1/0) + Knows_Bollinger (1/0) + Knows_Volume (1 if Yes, 0.5 if No); Range = 0.5 to 3.0\"\n",
    "\n",
    "# Export to Excel\n",
    "output_knowledge_path = r'E:\\FYP\\FYP Symposium\\Outputs\\Knowledge_Score_Updated.xlsx'\n",
    "knowledge_df.to_excel(output_knowledge_path, index=False)\n",
    "print(f\"\\n✅ Updated Knowledge Data (with Knowledge_Score) exported to: {output_knowledge_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebd495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== BLOCK 4: Factual trends (MACD/Bollinger/Price/Volume signals) ==========\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 3.1: Check if participant knows indicator\n",
    "def participant_knows_indicator(prm2b_14a_response, indicator):\n",
    "    if pd.isnull(prm2b_14a_response):\n",
    "        return False\n",
    "    return indicator.lower() in prm2b_14a_response.lower()\n",
    "\n",
    "# --- 3.2: Detect market signals from price data\n",
    "def detect_market_signals(stock_df, turn, days=5):\n",
    "    visible_data = stock_df[stock_df['Turn'] <= turn].sort_values(by='Turn', ascending=False).head(days)\n",
    "\n",
    "    price_trend = 'neutral'\n",
    "    volume_spike = False\n",
    "    macd_signal = 'neutral'\n",
    "    bb_trend = 'neutral'\n",
    "\n",
    "    if len(visible_data) >= 3:\n",
    "        close_now = visible_data.iloc[0]['Close']\n",
    "        close_past = visible_data.iloc[-1]['Close']\n",
    "        change_pct = (close_now - close_past) / close_past * 100\n",
    "\n",
    "        if change_pct > 2.0:\n",
    "            price_trend = 'uptrend'\n",
    "        elif change_pct < -2.0:\n",
    "            price_trend = 'downtrend'\n",
    "        else:\n",
    "            price_trend = 'neutral'\n",
    "\n",
    "        avg_volume = visible_data['Volume'].mean()\n",
    "        curr_volume = visible_data.iloc[0]['Volume']\n",
    "        if curr_volume > 1.2 * avg_volume:\n",
    "            volume_spike = True\n",
    "\n",
    "        macd = visible_data.iloc[0]['MACD (12,26,9)']\n",
    "        signal = visible_data.iloc[0]['Signal (12,26,9)']\n",
    "        macd_hist = visible_data.iloc[0]['MACD Histogram (12,26,9)']\n",
    "\n",
    "        if macd > signal and macd_hist > 0:\n",
    "            macd_signal = 'buy'\n",
    "        elif macd < signal and macd_hist < 0:\n",
    "            macd_signal = 'sell'\n",
    "\n",
    "        close = visible_data.iloc[0]['Close']\n",
    "        top_bb = visible_data.iloc[0]['Top Bollinger Bands (20,O,2,ma,n)']\n",
    "        bottom_bb = visible_data.iloc[0]['Bottom Bollinger Bands (20,O,2,ma,n)']\n",
    "        if close > top_bb:\n",
    "            bb_trend = 'overbought'\n",
    "        elif close < bottom_bb:\n",
    "            bb_trend = 'oversold'\n",
    "        else:\n",
    "            bb_trend = 'neutral'\n",
    "\n",
    "    return price_trend, volume_spike, macd_signal, bb_trend\n",
    "\n",
    "# --- Suggest Buy Logic\n",
    "def suggest_buy_decision(price_trend, macd_signal, bb_trend, volume_spike, volatility):\n",
    "    score = 0\n",
    "    if price_trend == 'uptrend': score += 1\n",
    "    if macd_signal == 'buy': score += 1\n",
    "    if bb_trend == 'oversold': score += 1\n",
    "    if volume_spike: score += 1\n",
    "    if volatility < 10: score += 1\n",
    "    return 'Attractive' if score >= 2 else 'Risky'\n",
    "\n",
    "# --- Load raw stock data\n",
    "experiment_file_path = r'E:\\FYP\\FYP Symposium\\Trading Simulation Experiment Data Turn Wise.xlsx'\n",
    "stock_tables = {}\n",
    "\n",
    "for stock in ['TSLA', 'XOM', 'NFLX', 'PG']:\n",
    "    df = pd.read_excel(experiment_file_path, sheet_name=stock)\n",
    "    df = df.sort_values(by='Turn', ascending=False).reset_index(drop=True)\n",
    "    stock_tables[stock] = df\n",
    "\n",
    "# ========== TURN-WISE TREND METRICS CREATION ==========\n",
    "file_path = r'E:\\FYP\\FYP Symposium\\Turn Data in descending order.xlsx'\n",
    "sheet_names = ['TSLA', 'NFLX', 'PG', 'XOM']\n",
    "turn_data = {}\n",
    "\n",
    "for sheet in sheet_names:\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet)\n",
    "\n",
    "    # Price trend\n",
    "    df['trend change'] = df['Close'] - df['Close'].shift(-1)\n",
    "\n",
    "    def get_trend_direction(change):\n",
    "        if pd.isna(change): return None\n",
    "        elif change > 0: return 'uptrend'\n",
    "        elif change < 0: return 'downtrend'\n",
    "        else: return 'neutral'\n",
    "\n",
    "    df['price_trend_1'] = df['trend change'].apply(get_trend_direction)\n",
    "\n",
    "    def summarize_trend_block(group):\n",
    "        up = (group['price_trend_1'] == 'uptrend').sum()\n",
    "        down = (group['price_trend_1'] == 'downtrend').sum()\n",
    "        group['price_trend_7'] = 'uptrend' if up > down else 'downtrend' if down > up else 'equal'\n",
    "        return group\n",
    "\n",
    "    df = df.groupby('Turn', group_keys=False).apply(summarize_trend_block)\n",
    "\n",
    "    # Volume trend\n",
    "    df['volume_diff'] = df['Volume'] - df['Volume'].shift(-1)\n",
    "\n",
    "    def get_volume_trend_direction(change):\n",
    "        if pd.isna(change): return None\n",
    "        elif change > 0: return 'uptrend'\n",
    "        elif change < 0: return 'downtrend'\n",
    "        else: return 'neutral'\n",
    "\n",
    "    df['volume_trend_1'] = df['volume_diff'].apply(get_volume_trend_direction)\n",
    "\n",
    "    def summarize_volume_trend_block(group):\n",
    "        up = (group['volume_trend_1'] == 'uptrend').sum()\n",
    "        down = (group['volume_trend_1'] == 'downtrend').sum()\n",
    "        group['volume_trend_7'] = 'uptrend' if up > down else 'downtrend' if down > up else 'equal'\n",
    "        return group\n",
    "\n",
    "    df = df.groupby('Turn', group_keys=False).apply(summarize_volume_trend_block)\n",
    "\n",
    "    # MACD\n",
    "    def make_technical_decision(row):\n",
    "        try:\n",
    "            macd = row['MACD (12,26,9)']\n",
    "            signal = row['Signal (12,26,9)']\n",
    "            hist = row['MACD Histogram (12,26,9)']\n",
    "            if macd > signal > hist: return 'Buy'\n",
    "            elif macd < signal and macd < hist: return 'Sell'\n",
    "            else: return 'Neutral'\n",
    "        except: return 'Neutral'\n",
    "\n",
    "    df['MACD_trend'] = df.apply(make_technical_decision, axis=1)\n",
    "\n",
    "    # Bollinger\n",
    "    def classify_bollinger(row):\n",
    "        try:\n",
    "            if row['Close'] > row['Top Bollinger Bands (20,O,2,ma,n)']:\n",
    "                return 'Over Bought'\n",
    "            elif row['Close'] < row['Bottom Bollinger Bands (20,O,2,ma,n)']:\n",
    "                return 'Over Sold'\n",
    "            else:\n",
    "                return 'Neutral'\n",
    "        except:\n",
    "            return 'Neutral'\n",
    "\n",
    "    df['bollinger_trend'] = df.apply(classify_bollinger, axis=1)\n",
    "\n",
    "    df['MACD_trend_7'] = df.groupby('Turn')['MACD_trend'].transform(lambda x: x.mode().iloc[0] if not x.mode().empty else None)\n",
    "    df['bollinger_trend_7'] = df.groupby('Turn')['bollinger_trend'].transform(lambda x: x.mode().iloc[0] if not x.mode().empty else None)\n",
    "\n",
    "    turn_data[sheet] = df\n",
    "\n",
    "# === Export full trend summary ===\n",
    "output_path = r'E:\\FYP\\FYP Symposium\\Outputs\\Combined_Turn_With_Trend_Summary.xlsx'\n",
    "with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "    for stock, df in turn_data.items():\n",
    "        df.to_excel(writer, sheet_name=stock, index=False)\n",
    "\n",
    "print(f\"✅ Exported successfully with trend summaries (Close & Volume) for all sheets to: {output_path}\")\n",
    "\n",
    "# === NEW: Load only Turn 1–5 top rows for enrichment ===\n",
    "turn_summary_1to6 = {}\n",
    "for stock in ['TSLA', 'NFLX', 'PG', 'XOM']:\n",
    "    df = pd.read_excel(output_path, sheet_name=stock)\n",
    "    top_rows = df.drop_duplicates(subset='Turn', keep='first')\n",
    "    top_rows = top_rows[top_rows['Turn'].isin([6, 5, 4, 3, 2, 1])]\n",
    "    top_rows = top_rows.sort_values(by='Turn', ascending=False)\n",
    "    turn_summary_1to6[stock] = top_rows\n",
    "    \n",
    "# === Precompute volatility per Turn per stock from raw Turn Data ===\n",
    "volatility_lookup = {}\n",
    "for stock in ['TSLA', 'NFLX', 'PG', 'XOM']:\n",
    "    df = pd.read_excel(r'E:\\FYP\\FYP Symposium\\Turn Data in descending order.xlsx', sheet_name=stock)\n",
    "    vol_by_turn = df.groupby('Turn')['Close'].std().reset_index()\n",
    "    vol_by_turn.rename(columns={'Close': 'Turn_Volatility'}, inplace=True)\n",
    "    volatility_lookup[stock] = vol_by_turn\n",
    "\n",
    "# === Generate trend data table and file ===\n",
    "def generate_trend_basis(stock_df, stock_name):\n",
    "    trend_rows = []\n",
    "    def map_score(val, pos_list, neg_list):\n",
    "                val = str(val).lower()\n",
    "                if val in pos_list:\n",
    "                    return 1\n",
    "                elif val in neg_list:\n",
    "                    return -1\n",
    "                return 0\n",
    "    for turn in sorted(stock_df['Turn'].unique()):\n",
    "        visible_data = stock_df[stock_df['Turn'] <= turn].sort_values(by='Turn', ascending=False).head(5)\n",
    "        if visible_data.empty:\n",
    "            continue\n",
    "        try:\n",
    "            price_trend, volume_spike, macd_signal, bb_trend = detect_market_signals(stock_df, turn)\n",
    "            close = visible_data.iloc[0]['Close']\n",
    "            high = visible_data.iloc[0]['High']\n",
    "            low = visible_data.iloc[0]['Low']\n",
    "            vol_df = volatility_lookup[stock_name]\n",
    "            vol_row = vol_df[vol_df['Turn'] == turn]\n",
    "            volatility = vol_row['Turn_Volatility'].values[0] if not vol_row.empty else None\n",
    "            macd_hist = visible_data.iloc[0]['MACD Histogram (12,26,9)']\n",
    "            macd_hist_strength = round(macd_hist, 4)\n",
    "            volume_support = volume_spike and turn_summary_1to6[stock_name].set_index('Turn').loc[turn, 'volume_trend_7'] == 'uptrend'\n",
    "            trend_reversal = False\n",
    "            turn_list = stock_df['Turn'].sort_values(ascending=False).unique()\n",
    "            turn_idx = list(turn_list).index(turn)\n",
    "            if turn_idx + 1 < len(turn_list):\n",
    "                next_turn = turn_list[turn_idx + 1]\n",
    "                df_enriched = turn_summary_1to6.get(stock_name)\n",
    "                if df_enriched is not None and turn in df_enriched['Turn'].values and next_turn in df_enriched['Turn'].values:\n",
    "                    current_trend = df_enriched.set_index('Turn').loc[turn, 'price_trend_7']\n",
    "                    next_trend = df_enriched.set_index('Turn').loc[next_turn, 'price_trend_7']\n",
    "                    trend_reversal = current_trend != next_trend\n",
    "            \n",
    "            # --- Scoring each indicator (individual columns) ---\n",
    "            score_price_trend = map_score(price_trend, ['uptrend'], ['downtrend'])\n",
    "            score_macd_signal = map_score(macd_signal, ['buy'], ['sell'])\n",
    "            score_bb_trend = map_score(bb_trend, ['oversold'], ['overbought'])\n",
    "            score_volume_spike = 1 if volume_spike else 0\n",
    "\n",
    "            # --- Summary trend scores ---\n",
    "            summary_df = turn_summary_1to6[stock_name].set_index('Turn')\n",
    "            if turn not in summary_df.index:\n",
    "                print(f\"⚠️ Turn {turn} not found in turn_summary for {stock_name}\")\n",
    "                continue\n",
    "            summary = summary_df.loc[turn]\n",
    "\n",
    "            score_volume_trend = map_score(summary['volume_trend_7'], ['uptrend'], ['downtrend'])\n",
    "            score_macd_trend = map_score(summary['MACD_trend_7'], ['buy'], ['sell'])\n",
    "            score_bb_summary = map_score(summary['bollinger_trend_7'], ['oversold'], ['overbought'])\n",
    "            score_price_summary = map_score(summary['price_trend_7'], ['uptrend'], ['downtrend'])\n",
    "            score_trend_reversal = map_score('Yes' if trend_reversal else 'No', ['Yes'], [])\n",
    "            score_volume_support = map_score('Yes' if volume_support else 'No', ['Yes'], [])\n",
    "\n",
    "            # --- Final score calculation ---\n",
    "            \n",
    "            score = (\n",
    "                score_price_trend +\n",
    "                score_macd_signal +\n",
    "                score_bb_trend +\n",
    "                score_volume_spike +\n",
    "                score_volume_trend +\n",
    "                score_macd_trend +\n",
    "                score_bb_summary +\n",
    "                score_price_summary +\n",
    "                score_trend_reversal +\n",
    "                score_volume_support\n",
    "            )\n",
    "\n",
    "\n",
    "            score = round(score, 2)  # Ensure consistent float precision\n",
    "            \n",
    "            # Summary trend indicators\n",
    "            score += (\n",
    "                map_score(turn_summary_1to6[stock_name].set_index('Turn').loc[turn, 'volume_trend_7'], ['uptrend'], ['downtrend']) +\n",
    "                map_score(turn_summary_1to6[stock_name].set_index('Turn').loc[turn, 'MACD_trend_7'], ['buy'], ['sell']) +\n",
    "                map_score(turn_summary_1to6[stock_name].set_index('Turn').loc[turn, 'bollinger_trend_7'], ['oversold'], ['overbought']) +\n",
    "                map_score(turn_summary_1to6[stock_name].set_index('Turn').loc[turn, 'price_trend_7'], ['uptrend'], ['downtrend']) +\n",
    "                map_score('Yes' if trend_reversal else 'No', ['Yes'], []) +\n",
    "                map_score('Yes' if volume_support else 'No', ['Yes'], [])\n",
    "            )\n",
    "            # --- Final buy suggestion logic ---\n",
    "            if score >= 3.0:\n",
    "                buy_suggestion = 'Attractive'\n",
    "            elif score >= 1.0:\n",
    "                buy_suggestion = 'Cautious'\n",
    "            else:\n",
    "                buy_suggestion = 'Risky'\n",
    "\n",
    "            \n",
    "\n",
    "            trend_rows.append({\n",
    "                'Turn': turn,\n",
    "                'Close Price': close,\n",
    "                'High Price': high,\n",
    "                'Low Price': low,\n",
    "                'Volatility': volatility,\n",
    "\n",
    "                # Raw factual indicators\n",
    "                'Price Trend': price_trend,\n",
    "                'Score - Price Trend': score_price_trend,\n",
    "                'MACD Signal': macd_signal,\n",
    "                'Score - MACD Signal': score_macd_signal,\n",
    "                'Bollinger Band Trend': bb_trend,\n",
    "                'Score - Bollinger Band': score_bb_trend,\n",
    "                'Volume Spike': volume_spike,\n",
    "                'Score - Volume Spike': score_volume_spike,\n",
    "                'MACD Histogram': macd_hist,\n",
    "                'MACD Histogram Strength': macd_hist_strength,\n",
    "\n",
    "                # Summary trends\n",
    "                'price_trend_7': summary['price_trend_7'],\n",
    "                'Score - Price Trend (7)': score_price_summary,\n",
    "                'volume_trend_7': summary['volume_trend_7'],\n",
    "                'Score - Volume Trend (7)': score_volume_trend,\n",
    "                'MACD_trend_7': summary['MACD_trend_7'],\n",
    "                'Score - MACD Trend (7)': score_macd_trend,\n",
    "                'bollinger_trend_7': summary['bollinger_trend_7'],\n",
    "                'Score - Bollinger Trend (7)': score_bb_summary,\n",
    "\n",
    "                # Flags\n",
    "                'Trend Reversal Flag': 'Yes' if trend_reversal else 'No',\n",
    "                'Score - Trend Reversal': score_trend_reversal,\n",
    "                'Volume Trend Support': 'Yes' if volume_support else 'No',\n",
    "                'Score - Volume Support': score_volume_support,\n",
    "\n",
    "                # Final decision\n",
    "                'Buy Confidence Score': round(score, 2),\n",
    "                'Is Attractive to Buy?': buy_suggestion\n",
    "            })\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing Turn {turn} in {stock_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    trend_df = pd.DataFrame(trend_rows)\n",
    "    \n",
    "    # ✅ Normalize Buy Confidence Score to 1–5 after DataFrame is built\n",
    "    if 'Buy Confidence Score' in trend_df.columns:\n",
    "        min_score = trend_df['Buy Confidence Score'].min()\n",
    "        max_score = trend_df['Buy Confidence Score'].max()\n",
    "        if min_score != max_score:\n",
    "            trend_df['Buy Confidence Score_Normalized'] = (\n",
    "                (trend_df['Buy Confidence Score'] - min_score) / (max_score - min_score) * 4 + 1\n",
    "            ).round(2)\n",
    "        else:\n",
    "            trend_df['Buy Confidence Score_Normalized'] = 3\n",
    "\n",
    "    # === Combined Signal-Based Trend Reversal ===\n",
    "    trend_df['Trend Reversal Flag'] = trend_df['price_trend_7'] != trend_df['price_trend_7'].shift(-1)\n",
    "    trend_df['Trend Reversal Flag'] = trend_df['Trend Reversal Flag'].apply(lambda x: 'Yes' if x else 'No')\n",
    "\n",
    "    trend_df['Volume Trend Support'] = trend_df.apply(\n",
    "        lambda row: 'Yes' if row['Volume Spike'] and row['volume_trend_7'] == 'uptrend' else 'No',\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # === Combined Signal-Based Trend Reversal ===\n",
    "    trend_df['Trend Reversal Flag'] = trend_df['price_trend_7'] != trend_df['price_trend_7'].shift(-1)\n",
    "    trend_df['Trend Reversal Flag'] = trend_df['Trend Reversal Flag'].apply(lambda x: 'Yes' if x else 'No')\n",
    "\n",
    "    trend_df['Volume Trend Support'] = trend_df.apply(\n",
    "        lambda row: 'Yes' if row['Volume Spike'] and row['volume_trend_7'] == 'uptrend' else 'No',\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    # === Strict grouped column ordering ===\n",
    "    ordered_cols = [\n",
    "\n",
    "        # Basic Info\n",
    "        'Turn', 'Close Price', 'High Price', 'Low Price', 'Volatility',\n",
    "\n",
    "        # PRICE-RELATED\n",
    "        'Price Trend', 'Score - Price Trend',\n",
    "        'price_trend_7', 'Score - Price Trend (7)',\n",
    "        'Trend Reversal Flag', 'Score - Trend Reversal',\n",
    "\n",
    "        # MACD-RELATED\n",
    "        'MACD Signal', 'Score - MACD Signal',\n",
    "        'MACD Histogram', 'MACD Histogram Strength',\n",
    "        'MACD_trend_7', 'Score - MACD Trend (7)',\n",
    "\n",
    "        # BOLLINGER-RELATED\n",
    "        'Bollinger Band Trend', 'Score - Bollinger Band',\n",
    "        'bollinger_trend_7', 'Score - Bollinger Trend (7)',\n",
    "\n",
    "        # VOLUME-RELATED\n",
    "        'Volume Spike', 'Score - Volume Spike',\n",
    "        'volume_trend_7', 'Score - Volume Trend (7)',\n",
    "        'Volume Trend Support', 'Score - Volume Support',\n",
    "\n",
    "        # FINAL OUTPUT\n",
    "        'Buy Confidence Score', 'Buy Confidence Score_Normalized',\n",
    "        'Is Attractive to Buy?'\n",
    "    ]\n",
    "\n",
    "    # Safely reorder\n",
    "    trend_df = trend_df[[col for col in ordered_cols if col in trend_df.columns]]\n",
    "\n",
    "\n",
    "\n",
    "    return trend_df\n",
    "\n",
    "\n",
    "    \n",
    "# === Run and display ===\n",
    "trend_tables = {}\n",
    "for stock in stock_tables.keys():\n",
    "    df = generate_trend_basis(stock_tables[stock], stock)\n",
    "    df = df[df['Turn'] != 6]  # drop Turn 6\n",
    "    trend_tables[stock] = df  # <-- this line was missing\n",
    "\n",
    "# Display sample tables without color formatting\n",
    "for stock, table in trend_tables.items():\n",
    "    print(f\"\\nSample Trend Table for {stock}:\")\n",
    "    print(table.head(10))  # Show first 10 rows; adjust as needed\n",
    "    \n",
    "# Load Strategy sheet once at the top of BLOCK 5\n",
    "strategy_path = r\"E:\\FYP\\FYP Symposium\\Outputs\\Final_Merged_Log_and_Strategy_With_Scoring.xlsx\"\n",
    "strategy_df = pd.read_excel(strategy_path, sheet_name='Strategy')\n",
    "\n",
    "# Create lookup dictionary for Average Scoring\n",
    "avg_score_dict = strategy_df.set_index('Participant_ID')['Average Scoring'].to_dict()\n",
    "\n",
    "\n",
    "# ========== BLOCK 5: Enrich merged logs with trend-based market context ==========\n",
    "\n",
    "# Load merged log (⚠️ it does NOT contain 'Average Scoring')\n",
    "merged_log_path = r\"E:\\FYP\\FYP Symposium\\Renamed_Merged_Log_and_Strategy.xlsx\"\n",
    "merged_log_df = pd.read_excel(merged_log_path, sheet_name='Merged_Log')\n",
    "\n",
    "# Load Strategy sheet to get Average Scoring\n",
    "strategy_path = r\"E:\\FYP\\FYP Symposium\\Outputs\\Final_Merged_Log_and_Strategy_With_Scoring.xlsx\"\n",
    "strategy_df = pd.read_excel(strategy_path, sheet_name='Strategy')\n",
    "avg_scores = strategy_df.set_index('Participant_ID')['Average Scoring'].to_dict()\n",
    "\n",
    "# Enrichment function\n",
    "def enrich_log_with_trends(merged_log_df, trend_tables):\n",
    "    enriched_rows = []\n",
    "\n",
    "    for idx, row in merged_log_df.iterrows():\n",
    "        pid = row['Participant_ID']\n",
    "        ticker = row['ticker']\n",
    "        turn = row['turn']\n",
    "\n",
    "        try:\n",
    "            trend_row_df = trend_tables.get(ticker)\n",
    "            if trend_row_df is None:\n",
    "                print(f\"⚠️ No trend data for ticker: {ticker}\")\n",
    "                continue\n",
    "\n",
    "            trend_row = trend_row_df[trend_row_df['Turn'] == turn]\n",
    "            if trend_row.empty:\n",
    "                print(f\"⚠️ Turn {turn} not found in trend table for {ticker}\")\n",
    "                continue\n",
    "\n",
    "            trend_data = trend_row.iloc[0][['Is Attractive to Buy?']].to_dict()\n",
    "            enriched_row = row.to_dict()\n",
    "            enriched_row.update(trend_data)\n",
    "            enriched_rows.append(enriched_row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing row {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(enriched_rows)\n",
    "\n",
    "# Run enrichment\n",
    "enriched_log_df = enrich_log_with_trends(merged_log_df, trend_tables)\n",
    "\n",
    "def compute_action_signal_value(row):\n",
    "    action = str(row['action']).strip().lower()\n",
    "    signal = str(row['Is Attractive to Buy?']).strip().lower()\n",
    "    avg_score = row.get('Average Scoring', 0)\n",
    "\n",
    "    if action == 'buy' and signal == 'risky':\n",
    "        return 0\n",
    "    elif action == 'sell' and signal == 'risky':\n",
    "        return avg_score\n",
    "    elif action == 'buy' and signal == 'cautious':\n",
    "        return avg_score / 2\n",
    "    elif action == 'sell' and signal == 'cautious':\n",
    "        return avg_score / 2\n",
    "    elif action == 'buy' and signal == 'attractive':\n",
    "        return avg_score\n",
    "    elif action == 'sell' and signal == 'attractive':\n",
    "        return 0\n",
    "    return 0  # Default fallback for unknown values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ========== BLOCK 7: Add Signal Alignment Score ==========\n",
    "def compute_alignment_score(row):\n",
    "    action = str(row['action']).strip().lower()\n",
    "    signal = str(row['Is Attractive to Buy?']).strip().lower()\n",
    "\n",
    "    if signal == 'risky':\n",
    "        return 1 if action == 'sell' else 0\n",
    "    elif signal == 'cautious':\n",
    "        return 0.5\n",
    "    elif signal == 'attractive':\n",
    "        return 1 if action == 'buy' else 0\n",
    "    return 0\n",
    "\n",
    "# Apply the alignment score\n",
    "enriched_log_df['Signal Alignment Score'] = enriched_log_df.apply(compute_alignment_score, axis=1)\n",
    "\n",
    "# Add explanation column for Signal Alignment Score\n",
    "def explain_alignment_score(row):\n",
    "    pid = row['Participant_ID']\n",
    "    score = row['Signal Alignment Score']\n",
    "    avg = avg_scores.get(pid, 0)\n",
    "\n",
    "    if score == 0:\n",
    "        return \"Misaligned – Action contradicts market signal\"\n",
    "    elif score == 0.5:\n",
    "        return \"Partially Aligned – Somewhat matches the signal\"\n",
    "    elif score == 1:\n",
    "        return \"Fully Aligned – Matches the signal strength\"\n",
    "    \n",
    "enriched_log_df['Signal Alignment Explanation'] = enriched_log_df.apply(explain_alignment_score, axis=1)\n",
    "\n",
    "# # Compute overall average alignment per participant\n",
    "# overall_alignment = (\n",
    "#     enriched_log_df.groupby('Participant_ID')['Signal Alignment Score']\n",
    "#     .mean()\n",
    "#     .round(2)\n",
    "#     .to_dict()\n",
    "# )\n",
    "\n",
    "# # Map it back to each row\n",
    "# enriched_log_df['Avg Signal Alignment (Overall)'] = enriched_log_df['Participant_ID'].map(overall_alignment)\n",
    "\n",
    "# # Explain overall alignment quality\n",
    "# def explain_overall_alignment(score):\n",
    "#     if score == 0:\n",
    "#         return \"Completely Misaligned – consistently acted against signals\"\n",
    "#     elif score < 2.0:\n",
    "#         return \"Poor Alignment – often disregarded market trends\"\n",
    "#     elif score < 3.5:\n",
    "#         return \"Moderate Alignment – partial awareness of signals\"\n",
    "#     elif score < 4.5:\n",
    "#         return \"Strong Alignment – usually followed signals correctly\"\n",
    "#     else:\n",
    "#         return \"Excellent Alignment – highly in sync with market trends\"\n",
    "\n",
    "# enriched_log_df['Overall Alignment Explanation'] = enriched_log_df['Avg Signal Alignment (Overall)'].apply(explain_overall_alignment)\n",
    "\n",
    "def infer_actual_usage(row, threshold=3.5):\n",
    "    pid = row['Participant_ID']\n",
    "    avg_score = avg_score_dict.get(pid, 0)\n",
    "    signal_available = str(row.get('Signal Was Available', '')).lower() == 'yes'\n",
    "    aligned = row.get('Signal Alignment Score', 0) >= 0.5\n",
    "\n",
    "    claimed_use = avg_score >= threshold\n",
    "\n",
    "    if signal_available and claimed_use and aligned:\n",
    "        return \"✅ Used Signal – Claimed & Aligned\"\n",
    "    elif signal_available and claimed_use and not aligned:\n",
    "        return \"❌ Ignored Signal – Claimed but not aligned\"\n",
    "    elif signal_available and not claimed_use and aligned:\n",
    "        return \"⚠️ Aligned Accidentally – Didn't claim use\"\n",
    "    elif not signal_available and claimed_use:\n",
    "        return \"⚪ No Signal – Can't assess usage\"\n",
    "    else:\n",
    "        return \"❌ No Use – No claim and no signal used\"\n",
    "\n",
    "\n",
    "\n",
    "def compute_action_signal_value(row):\n",
    "    pid = row['Participant_ID']\n",
    "    action = str(row['action']).strip().lower()\n",
    "    signal = str(row['Is Attractive to Buy?']).strip().lower()\n",
    "    avg_score = avg_score_dict.get(pid, 0)  # Safe dictionary lookup\n",
    "\n",
    "    if action == 'buy' and signal == 'risky':\n",
    "        return 0\n",
    "    elif action == 'sell' and signal == 'risky':\n",
    "        return avg_score\n",
    "    elif action == 'buy' and signal == 'cautious':\n",
    "        return avg_score / 2\n",
    "    elif action == 'sell' and signal == 'cautious':\n",
    "        return avg_score / 2\n",
    "    elif action == 'buy' and signal == 'attractive':\n",
    "        return avg_score\n",
    "    elif action == 'sell' and signal == 'attractive':\n",
    "        return 0\n",
    "    return 0  # Default fallback for unknown values\n",
    "\n",
    "# Apply it\n",
    "enriched_log_df['Action Signal Value'] = enriched_log_df.apply(compute_action_signal_value, axis=1)\n",
    "\n",
    "# Save enriched merged log with signal score\n",
    "final_export_path = r\"E:\\FYP\\FYP Symposium\\Outputs\\Final_Enriched_Log_With_Signal_Score.xlsx\"\n",
    "enriched_log_df.to_excel(final_export_path, index=False)\n",
    "print(f\"✅ Final enriched log WITH signal alignment score exported to: {final_export_path}\")\n",
    "\n",
    "# ========== Export trend tables ==========\n",
    "\n",
    "export_path = r'E:\\FYP\\FYP Symposium\\Outputs\\Final_Trend_Tables.xlsx'\n",
    "with pd.ExcelWriter(export_path, engine='openpyxl') as writer:\n",
    "    for ticker, df in trend_tables.items():\n",
    "        if not df.empty:\n",
    "            df.to_excel(writer, sheet_name=ticker, index=False)\n",
    "\n",
    "print(f\"✅ Final enriched trend tables exported to: {export_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c37c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==== Block 8: Output 2 logic different logic but same inferences ====\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "base_path = r\"E:\\FYP\\FYP Symposium\"\n",
    "output_path = os.path.join(base_path, \"Output 2\", \"Participant_Indicator_Segments.xlsx\")\n",
    "\n",
    "# Load data\n",
    "log_path = os.path.join(base_path, \"Renamed_Merged_Log_and_Strategy.xlsx\")\n",
    "survey_path = os.path.join(base_path, \"Post-Survey (Responses).xlsx\")\n",
    "\n",
    "merged_df = pd.read_excel(log_path)\n",
    "post_df = pd.read_excel(survey_path)\n",
    "\n",
    "# Extract and rename indicator preferences\n",
    "rank_cols = [col for col in post_df.columns if any(k in col.lower() for k in ['graph', 'data table', 'macd', 'bollinger'])]\n",
    "preference_df = post_df[['Participant ID'] + rank_cols].copy()\n",
    "\n",
    "rename_map = {\n",
    "    col: 'Graph_Rank' for col in preference_df.columns if 'graph' in col.lower()\n",
    "}\n",
    "rename_map.update({\n",
    "    col: 'DataTable_Rank' for col in preference_df.columns if 'data table' in col.lower()\n",
    "})\n",
    "rename_map.update({\n",
    "    col: 'MACD_Rank' for col in preference_df.columns if 'macd' in col.lower()\n",
    "})\n",
    "rename_map.update({\n",
    "    col: 'BB_Rank' for col in preference_df.columns if 'bollinger' in col.lower()\n",
    "})\n",
    "preference_df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "# Compute trend signals\n",
    "merged_df_sorted = merged_df.sort_values(by=['Participant_ID', 'ticker', 'turn'])\n",
    "merged_df_sorted['Close'] = merged_df_sorted.groupby(['Participant_ID', 'ticker'])['stockportfolio_after'].shift(0)\n",
    "merged_df_sorted['Prev_Close'] = merged_df_sorted.groupby(['Participant_ID', 'ticker'])['stockportfolio_after'].shift(1)\n",
    "merged_df_sorted['price_diff'] = merged_df_sorted['Close'] - merged_df_sorted['Prev_Close']\n",
    "\n",
    "def get_trend_signal(diff):\n",
    "    if pd.isna(diff): return 'neutral'\n",
    "    if diff > 0: return 'buy'\n",
    "    elif diff < 0: return 'sell'\n",
    "    return 'neutral'\n",
    "merged_df_sorted['graph_trend_signal'] = merged_df_sorted['price_diff'].apply(get_trend_signal)\n",
    "\n",
    "def get_macd_signal(diff):\n",
    "    if pd.isna(diff): return 'neutral'\n",
    "    elif diff > 500: return 'buy'\n",
    "    elif diff < -500: return 'sell'\n",
    "    return 'neutral'\n",
    "merged_df_sorted['macd_signal'] = merged_df_sorted['price_diff'].apply(get_macd_signal)\n",
    "\n",
    "merged_df_sorted['rolling_std'] = merged_df_sorted.groupby(['Participant_ID', 'ticker'])['stockportfolio_after'].transform(lambda x: x.rolling(window=3, min_periods=2).std())\n",
    "def get_bb_signal(row):\n",
    "    std = row['rolling_std']\n",
    "    diff = row['price_diff']\n",
    "    if pd.isna(std) or std < 1000: return 'neutral'\n",
    "    elif diff > 0: return 'buy'\n",
    "    elif diff < 0: return 'sell'\n",
    "    return 'neutral'\n",
    "merged_df_sorted['bb_signal'] = merged_df_sorted.apply(get_bb_signal, axis=1)\n",
    "\n",
    "# Merge with preference\n",
    "merged = pd.merge(merged_df_sorted, preference_df, left_on='Participant_ID', right_on='Participant ID', how='left')\n",
    "merged['action_clean'] = merged['action'].str.lower().str.strip()\n",
    "\n",
    "def compute_match(merged, rank_col, signal_col, action_col='action_clean', label=''):\n",
    "    df = merged[merged[rank_col] == 1].copy()\n",
    "    df[f'{label}_match'] = df.apply(lambda row: 1 if row[action_col] == row[signal_col] else 0, axis=1)\n",
    "    score = df.groupby('Participant_ID')[f'{label}_match'].agg(['mean', 'count']).reset_index()\n",
    "    score.columns = ['Participant_ID', f'{label}_Rate', f'{label}_Turns']\n",
    "    return score\n",
    "\n",
    "graph_score = compute_match(merged, 'Graph_Rank', 'graph_trend_signal', label='Graph')\n",
    "macd_score = compute_match(merged, 'MACD_Rank', 'macd_signal', label='MACD')\n",
    "bb_score = compute_match(merged, 'BB_Rank', 'bb_signal', label='BB')\n",
    "\n",
    "combined = graph_score.merge(macd_score, on='Participant_ID', how='outer')\n",
    "combined = combined.merge(bb_score, on='Participant_ID', how='outer')\n",
    "\n",
    "# Fill missing values\n",
    "for col in combined.columns:\n",
    "    if 'Rate' in col or 'Turns' in col:\n",
    "        combined[col] = combined[col].fillna(\"0\")\n",
    "\n",
    "# Add Segments\n",
    "def segment(row):\n",
    "    segments = []\n",
    "    if isinstance(row['Graph_Rate'], float) and row['Graph_Rate'] >= 0.7:\n",
    "        segments.append('Consistent Graph User')\n",
    "    elif isinstance(row['Graph_Rate'], float):\n",
    "        segments.append('Inconsistent Graph User')\n",
    "    if isinstance(row['MACD_Rate'], float) and row['MACD_Rate'] >= 0.7:\n",
    "        segments.append('Consistent MACD User')\n",
    "    elif isinstance(row['MACD_Rate'], float):\n",
    "        segments.append('Inconsistent MACD User')\n",
    "    if isinstance(row['BB_Rate'], float) and row['BB_Rate'] >= 0.7:\n",
    "        segments.append('Consistent BB User')\n",
    "    elif isinstance(row['BB_Rate'], float):\n",
    "        segments.append('Inconsistent BB User')\n",
    "    return ', '.join(segments) if segments else 'No Top-Ranked Indicator Evaluated'\n",
    "\n",
    "combined['Segment'] = combined.apply(segment, axis=1)\n",
    "\n",
    "# Export to your folder\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "combined.to_excel(output_path, index=False)\n",
    "print(f\"Exported successfully to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b54b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=====Block 9 Map The indicators to their preferences and score them. HL and OP and volatility should be weighted and avged out=====\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your trend table Excel file\n",
    "trend_file_path = r\"E:\\FYP\\FYP Symposium\\Outputs\\Final_Trend_Tables.xlsx\"\n",
    "\n",
    "# Sheet names to load\n",
    "sheet_names = ['TSLA', 'NFLX', 'PG', 'XOM']\n",
    "\n",
    "# Create an empty DataFrame\n",
    "indicator_map = pd.DataFrame()\n",
    "\n",
    "# Loop through each sheet and combine data\n",
    "for sheet in sheet_names:\n",
    "    df = pd.read_excel(trend_file_path, sheet_name=sheet)\n",
    "    df['Ticker'] = sheet  # Add ticker column\n",
    "    indicator_map = pd.concat([indicator_map, df], ignore_index=True)\n",
    "\n",
    "# --- Price + Volatility signal ---\n",
    "def compute_price_vol_signal(row):\n",
    "    trend = str(row.get('price_trend_7', '')).lower()\n",
    "    vol = row.get('Volatility', None)\n",
    "\n",
    "    price_score = 1 if trend == 'uptrend' else -1 if trend == 'downtrend' else 0\n",
    "\n",
    "    if vol is None or pd.isna(vol):\n",
    "        vol_score = 0\n",
    "    elif vol < 10:\n",
    "        vol_score = 1\n",
    "    elif vol > 15:\n",
    "        vol_score = -1\n",
    "    else:\n",
    "        vol_score = 0\n",
    "\n",
    "    weighted_signal = 0.7 * price_score + 0.3 * vol_score\n",
    "    return 'buy' if weighted_signal > 0 else 'sell'\n",
    "\n",
    "indicator_map['PriceVol_Signal'] = indicator_map.apply(compute_price_vol_signal, axis=1)\n",
    "\n",
    "# --- MACD + Histogram signal ---\n",
    "def compute_macd_signal(row):\n",
    "    macd_trend = str(row.get('MACD_trend_7', '')).lower()\n",
    "    hist_strength = row.get('MACD Histogram Strength', 0)\n",
    "\n",
    "    macd_score = 1 if macd_trend == 'buy' else -1 if macd_trend == 'sell' else 0\n",
    "    hist_score = 1 if hist_strength > 0.5 else -1 if hist_strength < -0.5 else 0\n",
    "\n",
    "    weighted_macd = 0.6 * macd_score + 0.4 * hist_score\n",
    "    return 'buy' if weighted_macd > 0 else 'sell'\n",
    "\n",
    "indicator_map['MACD_Signal'] = indicator_map.apply(compute_macd_signal, axis=1)\n",
    "\n",
    "def score_bollinger(row):\n",
    "    # Bollinger trend mapping\n",
    "    bb = str(row['Bollinger Band Trend']).lower()\n",
    "    bb_score = 1 if bb == 'oversold' else -1 if bb == 'overbought' else 0\n",
    "    \n",
    "    # Volatility normalized contribution (higher vol = less confidence)\n",
    "    vol = row.get('Volatility', 0)\n",
    "    vol_score = -1 if vol > 10 else (1 if vol < 5 else 0)\n",
    "\n",
    "    # Weighted average\n",
    "    final_score = (0.7 * bb_score) + (0.3 * vol_score)\n",
    "\n",
    "    if final_score > 0.25:\n",
    "        return 'Buy'\n",
    "    elif final_score < -0.25:\n",
    "        return 'Sell'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "indicator_map['Bollinger_Signal'] = indicator_map.apply(score_bollinger, axis=1)\n",
    "\n",
    "def score_volume(row):\n",
    "    spike = row.get('Volume Spike', False)\n",
    "    trend = str(row.get('volume_trend_7', '')).lower()\n",
    "\n",
    "    if spike:\n",
    "        if trend == 'uptrend':\n",
    "            return 'Buy'\n",
    "        elif trend == 'downtrend':\n",
    "            return 'Sell'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "        \n",
    "indicator_map['Volume_Signal'] = indicator_map.apply(score_volume, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Preview key signals\n",
    "print(indicator_map[['Turn', 'Ticker', 'PriceVol_Signal',\n",
    "                    'MACD_Signal', 'Bollinger_Signal', 'Volume_Signal']].head(15))\n",
    "\n",
    "# File path to save\n",
    "export_path = r\"E:\\FYP\\FYP Symposium\\Output 2\\Indicator_Map_With_Signals_CLEAN.xlsx\"\n",
    "\n",
    "# Columns to keep\n",
    "final_cols = ['Turn', 'Ticker', 'PriceVol_Signal', 'MACD_Signal', 'Bollinger_Signal', 'Volume_Signal']\n",
    "\n",
    "# Filter only required columns per ticker and save to separate sheets\n",
    "with pd.ExcelWriter(export_path, engine='openpyxl') as writer:\n",
    "    for ticker, df in indicator_map.groupby('Ticker'):\n",
    "        df_final = df[final_cols].copy()\n",
    "        df_final.to_excel(writer, sheet_name=ticker, index=False)\n",
    "\n",
    "print(f\"✅ Clean indicator signal file exported to: {export_path}\")\n",
    "\n",
    "\n",
    "\n",
    "#======Decision factors ko dimagh men rakh kar decision lena hai=====\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# File path and sheet name\n",
    "file_path = r\"E:\\FYP\\FYP Symposium\\Renamed_Merged_Log_and_Strategy.xlsx\"\n",
    "sheet_name = 'Strategy'\n",
    "\n",
    "# Columns to load\n",
    "columns_to_use = [\n",
    "    'Participant_ID',\n",
    "    'Indicator_1st',\n",
    "    'Indicator_2nd',\n",
    "    'Indicator_3rd',\n",
    "    'Indicator_4th',\n",
    "    'Indicator_5th'\n",
    "]\n",
    "\n",
    "# Load only the specified columns\n",
    "decision_factor_ranked = pd.read_excel(file_path, sheet_name=sheet_name, usecols=columns_to_use)\n",
    "\n",
    "# Define the indicators we want to score\n",
    "indicators = ['MACD', 'High/Low', 'Volume', 'Open/Close', 'Bollinger Bands']\n",
    "\n",
    "# Initialize new columns with 0\n",
    "for indicator in indicators:\n",
    "    decision_factor_ranked[f'{indicator}_rank_score'] = 0\n",
    "\n",
    "# Assign scores: 5 for 1st rank, down to 1 for 5th rank\n",
    "score_map = {\n",
    "    'Indicator_1st': 5,\n",
    "    'Indicator_2nd': 4,\n",
    "    'Indicator_3rd': 3,\n",
    "    'Indicator_4th': 2,\n",
    "    'Indicator_5th': 1\n",
    "}\n",
    "\n",
    "# Loop through rows and assign scores\n",
    "for col, score in score_map.items():\n",
    "    for indicator in indicators:\n",
    "        decision_factor_ranked.loc[\n",
    "            decision_factor_ranked[col] == indicator,\n",
    "            f'{indicator}_rank_score'\n",
    "        ] += score\n",
    "\n",
    "# Calculate average score for price-based indicators: High/Low and Open/Close\n",
    "decision_factor_ranked['price_rank_score'] = (\n",
    "    decision_factor_ranked['High/Low_rank_score'] + decision_factor_ranked['Open/Close_rank_score']\n",
    ") / 2\n",
    "\n",
    "# Preview the result\n",
    "print(decision_factor_ranked.head())\n",
    "\n",
    "export_path = r\"E:\\FYP\\FYP Symposium\\Output 2\\Decision_Factor_Ranked_Scores.xlsx\"\n",
    "decision_factor_ranked.to_excel(export_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a98bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==== Block 10: Load merged log and strategy sheet to see indicator and preferences====#\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load all sheets from the Indicator Map file\n",
    "indicator_file = r\"E:\\FYP\\FYP Symposium\\Output 2\\Indicator_Map_With_Signals_CLEAN.xlsx\"\n",
    "all_sheets = pd.read_excel(indicator_file, sheet_name=None)\n",
    "\n",
    "# Combine all sheets into one DataFrame\n",
    "indicator_map = pd.concat(\n",
    "    [df.assign(Ticker=name) for name, df in all_sheets.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Ensure consistent column naming\n",
    "indicator_map.rename(columns={'Turn': 'turn'}, inplace=True)\n",
    "\n",
    "# Load the base participant data\n",
    "log_file = r\"E:\\FYP\\FYP Symposium\\Renamed_Merged_Log_and_Strategy.xlsx\"\n",
    "participant_cols = [\n",
    "    'Date', 'Real Time', 'Simulation Time', 'Seconds left',\n",
    "    'Participant_ID', 'turn', 'action', 'ticker'\n",
    "]\n",
    "participant_df = pd.read_excel(log_file, sheet_name='Merged_Log', usecols=participant_cols)\n",
    "\n",
    "# Merge signal info from indicator_map using both 'turn' and 'ticker'\n",
    "merged_df = participant_df.merge(\n",
    "    indicator_map[['turn', 'Ticker', 'PriceVol_Signal', 'MACD_Signal', 'Bollinger_Signal', 'Volume_Signal']],\n",
    "    how='left',\n",
    "    left_on=['turn', 'ticker'],\n",
    "    right_on=['turn', 'Ticker']\n",
    ")\n",
    "\n",
    "# Drop the duplicate 'Ticker' column\n",
    "merged_df.drop(columns='Ticker', inplace=True)\n",
    "\n",
    "# Load decision factor scores\n",
    "ranked_file = r\"E:\\FYP\\FYP Symposium\\Output 2\\Decision_Factor_Ranked_Scores.xlsx\"\n",
    "ranked_df = pd.read_excel(ranked_file)\n",
    "\n",
    "# Drop High/Low and Open/Close rank scores\n",
    "ranked_df = ranked_df.drop(columns=['High/Low_rank_score', 'Open/Close_rank_score'])\n",
    "\n",
    "# Merge scores based on Participant_ID\n",
    "final_df = merged_df.merge(ranked_df, on='Participant_ID', how='left')\n",
    "\n",
    "# === Assign action-based signal scores ===\n",
    "def compute_score(row, signal_col, score_col):\n",
    "    signal = str(row[signal_col]).strip().lower()\n",
    "    action = str(row['action']).strip().lower()\n",
    "    rank_score = row.get(score_col, 0)\n",
    "\n",
    "    if signal == 'neutral':\n",
    "        return rank_score / 2\n",
    "    elif signal == action:\n",
    "        return rank_score\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the logic to each indicator\n",
    "final_df['PriceVol_action_score'] = final_df.apply(lambda r: compute_score(r, 'PriceVol_Signal', 'High/Low_rank_score'), axis=1)\n",
    "final_df['MACD_action_score'] = final_df.apply(lambda r: compute_score(r, 'MACD_Signal', 'MACD_rank_score'), axis=1)\n",
    "final_df['Bollinger_action_score'] = final_df.apply(lambda r: compute_score(r, 'Bollinger_Signal', 'Bollinger Bands_rank_score'), axis=1)\n",
    "final_df['Volume_action_score'] = final_df.apply(lambda r: compute_score(r, 'Volume_Signal', 'Volume_rank_score'), axis=1)\n",
    "\n",
    "# Add Total Action Score by summing all individual action scores\n",
    "final_df['Total_action_score'] = (\n",
    "    final_df['PriceVol_action_score'] +\n",
    "    final_df['MACD_action_score'] +\n",
    "    final_df['Bollinger_action_score'] +\n",
    "    final_df['Volume_action_score']\n",
    ")\n",
    "\n",
    "final_df = final_df[final_df['turn'] != 6]\n",
    "\n",
    "# Add explanation for Total_action_score\n",
    "def explain_action_alignment(row):\n",
    "    scores = {\n",
    "        'PriceVol': row['PriceVol_action_score'],\n",
    "        'MACD': row['MACD_action_score'],\n",
    "        'Bollinger': row['Bollinger_action_score'],\n",
    "        'Volume': row['Volume_action_score']\n",
    "    }\n",
    "    strong_matches = [k for k, v in scores.items() if v >= 0.75]\n",
    "    neutral_matches = [k for k, v in scores.items() if 0 < v < 0.75]\n",
    "    misses = [k for k, v in scores.items() if v == 0]\n",
    "\n",
    "    explanation = []\n",
    "    if strong_matches:\n",
    "        explanation.append(f\"Strong alignment with {', '.join(strong_matches)}\")\n",
    "    if neutral_matches:\n",
    "        explanation.append(f\"Partially followed signals from {', '.join(neutral_matches)}\")\n",
    "    if misses:\n",
    "        explanation.append(f\"Ignored or mismatched {', '.join(misses)}\")\n",
    "\n",
    "    return \" | \".join(explanation)\n",
    "\n",
    "final_df['Action_Score_Explanation'] = final_df.apply(explain_action_alignment, axis=1)\n",
    "\n",
    "#Pulled the three columns from BLOCK 3 about knowledge from pre data if they know how to use what indicator\n",
    "\n",
    "def check_knowledge(prm2b_14a_response):\n",
    "    if pd.isna(prm2b_14a_response):\n",
    "        return {'MACD': False, 'Bollinger': False, 'Volume': False, 'HighLow_OpenClose': False}\n",
    "\n",
    "    text = prm2b_14a_response.lower()\n",
    "    \n",
    "    knowledge = {\n",
    "        'MACD': False,\n",
    "        'Bollinger': False,\n",
    "        'Volume': False,\n",
    "        'HighLow_OpenClose': False\n",
    "    }\n",
    "\n",
    "    # Check MACD knowledge\n",
    "    if 'macd' in text:\n",
    "        knowledge['MACD'] = True\n",
    "    \n",
    "    # Check Bollinger Bands\n",
    "    if 'bollinger' in text or 'bb' in text:\n",
    "        knowledge['Bollinger'] = True\n",
    "\n",
    "    # Check Volume\n",
    "    if 'volume' in text:\n",
    "        knowledge['Volume'] = True\n",
    "\n",
    "    # Check High Low Open Close\n",
    "    if 'high' in text or 'low' in text or 'open' in text or 'close' in text or 'ohlc' in text:\n",
    "        knowledge['HighLow_OpenClose'] = True\n",
    "\n",
    "    return knowledge\n",
    "\n",
    "# --- Apply to all participants ---\n",
    "\n",
    "knowledge_records = []\n",
    "\n",
    "for idx, row in strategy.iterrows():\n",
    "    participant_id = row['Participant_ID']\n",
    "    prm2b_14a = row.get('PRM2b_14a', None)\n",
    "    knowledge = check_knowledge(prm2b_14a)\n",
    "\n",
    "    knowledge_records.append({\n",
    "        'Participant_ID': participant_id,\n",
    "        'Knows_MACD': 'Yes' if knowledge['MACD'] else 'No',\n",
    "        'Knows_Bollinger': 'Yes' if knowledge['Bollinger'] else 'No',\n",
    "        'Knows_Volume': 'Yes' if knowledge['Volume'] else 'No',\n",
    "        'Knows_HighLow_OpenClose': 'Yes' if knowledge['HighLow_OpenClose'] else 'No'\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "knowledge_df = pd.DataFrame(knowledge_records)\n",
    "\n",
    "# Drop 'Knows_HighLow_OpenClose' — no longer needed\n",
    "if 'Knows_HighLow_OpenClose' in knowledge_df.columns:\n",
    "    knowledge_df.drop(columns=['Knows_HighLow_OpenClose'], inplace=True)\n",
    "\n",
    "# --- Merge Cleaned Knowledge into Strategy ---\n",
    "\n",
    "# Drop old versions to avoid MergeError\n",
    "strategy.drop(columns=['Knows_MACD', 'Knows_Bollinger', 'Knows_Volume', 'Knows_HighLow_OpenClose', 'Knowledge_Score'], errors='ignore', inplace=True)\n",
    "\n",
    "# Merge new knowledge\n",
    "strategy = pd.merge(strategy, knowledge_df, on='Participant_ID', how='left')\n",
    "print(\"\\n✅ Merged updated knowledge into strategy successfully.\")\n",
    "\n",
    "# --- Scoring Logic ---\n",
    "\n",
    "# Map Yes/No with new scoring logic (Volume: Yes = 1, No = 0.5)\n",
    "knowledge_df_numeric = knowledge_df.copy()\n",
    "knowledge_df_numeric['Knows_MACD'] = knowledge_df_numeric['Knows_MACD'].map({'Yes': 1, 'No': 0})\n",
    "knowledge_df_numeric['Knows_Bollinger'] = knowledge_df_numeric['Knows_Bollinger'].map({'Yes': 1, 'No': 0})\n",
    "knowledge_df_numeric['Knows_Volume'] = knowledge_df_numeric['Knows_Volume'].map({'Yes': 1, 'No': 0.5})\n",
    "\n",
    "# Compute score\n",
    "knowledge_df['Knowledge_Score'] = (\n",
    "    knowledge_df_numeric['Knows_MACD'] +\n",
    "    knowledge_df_numeric['Knows_Bollinger'] +\n",
    "    knowledge_df_numeric['Knows_Volume']\n",
    ")\n",
    "\n",
    "# Show sample output\n",
    "print(\"\\n✅ Final Knowledge Score (range 0.5–3.0):\")\n",
    "display(knowledge_df[['Participant_ID', 'Knows_MACD', 'Knows_Bollinger', 'Knows_Volume', 'Knowledge_Score']].head())\n",
    "\n",
    "# Optional documentation string\n",
    "knowledge_score_note = \"Knowledge_Score = Knows_MACD (1/0) + Knows_Bollinger (1/0) + Knows_Volume (1 if Yes, 0.5 if No); Range = 0.5 to 3.0\"\n",
    "\n",
    "\n",
    "final_df = final_df.merge(strategy[['Participant_ID', 'Knows_MACD', 'Knows_Bollinger', 'Knows_Volume']], on='Participant_ID', how='left')\n",
    "\n",
    "def compute_used_know_score(row, signal_col, know_col):\n",
    "    signal = str(row[signal_col]).strip().lower()\n",
    "    action = str(row['action']).strip().lower()\n",
    "    knows = row[know_col]\n",
    "\n",
    "    if signal == action and knows == 'Yes':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply logic for each indicator\n",
    "final_df['used_MACD_Know_Score'] = final_df.apply(lambda r: compute_used_know_score(r, 'MACD_Signal', 'Knows_MACD'), axis=1)\n",
    "final_df['used_Bollinger_Know_Score'] = final_df.apply(lambda r: compute_used_know_score(r, 'Bollinger_Signal', 'Knows_Bollinger'), axis=1)\n",
    "final_df['used_Volume_Know_Score'] = final_df.apply(lambda r: compute_used_know_score(r, 'Volume_Signal', 'Knows_Volume'), axis=1)\n",
    "\n",
    "# Optional: total informed usage score\n",
    "final_df['Total_Used_Know_Score'] = (\n",
    "    final_df['used_MACD_Know_Score'] +\n",
    "    final_df['used_Bollinger_Know_Score'] +\n",
    "    final_df['used_Volume_Know_Score']\n",
    ")\n",
    "\n",
    "final_df['Combined_Action_Knowledge_Score'] = (\n",
    "    final_df['Total_action_score'] + final_df['Total_Used_Know_Score']\n",
    ")\n",
    "\n",
    "\n",
    "print(final_df[['Participant_ID', 'turn', 'ticker', 'action',\n",
    "                'PriceVol_action_score', 'MACD_action_score',\n",
    "                'Bollinger_action_score', 'Volume_action_score',\n",
    "                'Total_action_score', 'Knows_MACD', 'Knows_Bollinger', 'Knows_Volume',\n",
    "                'used_MACD_Know_Score', 'used_Bollinger_Know_Score', 'used_Volume_Know_Score', 'Combined_Action_Knowledge_Score']].head(2))\n",
    "\n",
    "# Export the final dataframe\n",
    "final_df.to_excel(\n",
    "    r\"E:\\FYP\\FYP Symposium\\Output 2\\participant_indicator_preference_with_signals.xlsx\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"✅ Final participant-indicator preference file with action scores exported to: {export_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4160e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################## # BLOCK 11: Assign Rationality (Final Version)\n",
    "# --- Merge Extracted Knowledge into Strategy ---\n",
    "\n",
    "# Assuming 'knowledge_df' already created\n",
    "# Merge knowledge_df with strategy on 'Participant_ID'\n",
    "# Drop old columns if exist to avoid suffix errors\n",
    "cols_to_drop = ['Knows_MACD', 'Knows_Bollinger', 'Knows_Volume', 'Knows_HighLow_OpenClose']\n",
    "strategy = strategy.drop(columns=[col for col in cols_to_drop if col in strategy.columns], errors='ignore')\n",
    "\n",
    "# Now merge\n",
    "\n",
    "strategy = pd.merge(strategy, knowledge_df, on='Participant_ID', how='left')\n",
    "\n",
    "print(\"\\n✅ Merged Participant Knowledge into Strategy Frame.\")\n",
    "\n",
    "def assign_rationality(row):\n",
    "    participant_id = row['Participant_ID']\n",
    "    action = str(row['action']).lower()\n",
    "    ticker = str(row['ticker']).upper()\n",
    "    turn = row['turn']\n",
    "\n",
    "    participant_row = strategy[strategy['Participant_ID'] == participant_id]\n",
    "    if participant_row.empty:\n",
    "        return pd.Series(['Unknown', np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "    decision_factors = [\n",
    "        (participant_row['DecisionFactor_1st'].values[0], 0.5),\n",
    "        (participant_row['DecisionFactor_2nd'].values[0], 0.3),\n",
    "        (participant_row['DecisionFactor_3rd'].values[0], 0.2)\n",
    "    ]\n",
    "    indicators = [\n",
    "        (participant_row['Indicator_1st'].values[0], 0.5),\n",
    "        (participant_row['Indicator_2nd'].values[0], 0.4),\n",
    "        (participant_row['Indicator_3rd'].values[0], 0.3),\n",
    "        (participant_row['Indicator_4th'].values[0], 0.2),\n",
    "        (participant_row['Indicator_5th'].values[0], 0.1)\n",
    "    ]\n",
    "    known_indicators = participant_row['PRM2b_14a'].values[0]\n",
    "\n",
    "    trend_table = trend_tables.get(ticker)\n",
    "    if trend_table is None:\n",
    "        return pd.Series(['Unknown', np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "    trend_row = trend_table[trend_table['Turn'] == turn]\n",
    "    if trend_row.empty:\n",
    "        return pd.Series(['Unknown', np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "    trend_row = trend_row.iloc[0]\n",
    "\n",
    "    # Step 1: Emotionally Driven\n",
    "    if str(row['news_truth']).lower() == 'false':\n",
    "        if (str(row['news_sentiment']).lower() == 'positive' and action == 'buy') or \\\n",
    "           (str(row['news_sentiment']).lower() == 'negative' and action == 'sell'):\n",
    "            return pd.Series(['Emotionally Driven', 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "    # Step 2: Rationality Scoring\n",
    "    rationality_score = 0\n",
    "    breakdown_scores = {\n",
    "        'DecisionFactor_Score': 0,\n",
    "        'Indicator_Score': 0,\n",
    "        'Knowledge_Bonus': 0\n",
    "    }\n",
    "\n",
    "    for factor, weight in decision_factors:\n",
    "        if str(factor).lower() == 'graph':\n",
    "            if (trend_row['Price Trend'] == 'uptrend' and action == 'buy') or \\\n",
    "               (trend_row['Price Trend'] == 'downtrend' and action == 'sell'):\n",
    "                rationality_score += weight\n",
    "                breakdown_scores['DecisionFactor_Score'] += weight\n",
    "        elif str(factor).lower() == 'news':\n",
    "            if (row['news_sentiment'].lower() == 'positive' and row['news_truth'].lower() == 'true' and action == 'buy') or \\\n",
    "               (row['news_sentiment'].lower() == 'negative' and row['news_truth'].lower() == 'true' and action == 'sell'):\n",
    "                rationality_score += weight\n",
    "                breakdown_scores['DecisionFactor_Score'] += weight\n",
    "\n",
    "    for indicator, weight in indicators:\n",
    "        if pd.isna(indicator):\n",
    "            continue\n",
    "        if str(indicator).lower() == 'volume':\n",
    "            if trend_row['Volume Spike'] and action == 'buy':\n",
    "                rationality_score += weight\n",
    "                breakdown_scores['Indicator_Score'] += weight\n",
    "                if participant_row['Knows_Volume'].values[0] == 'Yes':\n",
    "                    rationality_score += 0.2\n",
    "                    breakdown_scores['Knowledge_Bonus'] += 0.2\n",
    "        elif str(indicator).lower() == 'macd':\n",
    "            if (trend_row['MACD Signal'] == 'buy' and action == 'buy') or (trend_row['MACD Signal'] == 'sell' and action == 'sell'):\n",
    "                rationality_score += weight\n",
    "                breakdown_scores['Indicator_Score'] += weight\n",
    "                if participant_row['Knows_MACD'].values[0] == 'Yes':\n",
    "                    rationality_score += 0.2\n",
    "                    breakdown_scores['Knowledge_Bonus'] += 0.2\n",
    "        elif str(indicator).lower() == 'bollinger bands':\n",
    "            if (trend_row['Bollinger Band Trend'] == 'oversold' and action == 'buy') or \\\n",
    "               (trend_row['Bollinger Band Trend'] == 'overbought' and action == 'sell'):\n",
    "                rationality_score += weight\n",
    "                breakdown_scores['Indicator_Score'] += weight\n",
    "                if participant_row['Knows_Bollinger'].values[0] == 'Yes':\n",
    "                    rationality_score += 0.2\n",
    "                    breakdown_scores['Knowledge_Bonus'] += 0.2\n",
    "\n",
    "    # Step 3: Label\n",
    "    if rationality_score >= 0.7:\n",
    "        label = 'Fully Rational'\n",
    "    elif 0.4 <= rationality_score < 0.7:\n",
    "        label = 'Slightly Rational'\n",
    "    else:\n",
    "        label = 'Irrational'\n",
    "\n",
    "    return pd.Series([\n",
    "        label,\n",
    "        rationality_score,\n",
    "        breakdown_scores['DecisionFactor_Score'],\n",
    "        breakdown_scores['Indicator_Score'],\n",
    "        breakdown_scores['Knowledge_Bonus']\n",
    "    ])\n",
    "merged_log[['rationality_label', 'rationality_score', 'DecisionFactor_Score', 'Indicator_Score', 'Knowledge_Bonus']] = merged_log.apply(assign_rationality, axis=1)\n",
    "\n",
    "\n",
    "print(\"\\n✅ Rationality Label and Score Assigned to merged_log.\")\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate Timestamp Name\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "output_path = rf'E:\\FYP\\FYP Symposium\\Outputs\\Behavioral_Rationality_Log_{current_time}.xlsx'\n",
    "\n",
    "# Save merged_log\n",
    "merged_log.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Behavioral Rationality Log Saved Successfully at {output_path}\")\n",
    "\n",
    "# Now prepare Market Signals + Rationality\n",
    "# Now prepare Market Signals + Rationality\n",
    "# Prepare Market Signals + Rationality\n",
    "market_signal_records = []\n",
    "\n",
    "for idx, row in merged_log.iterrows():\n",
    "    ticker = str(row['ticker']).upper()\n",
    "    turn = row['turn']\n",
    "    participant_id = row['Participant_ID']\n",
    "    rationality = row['rationality_label']\n",
    "    rationality_score = row['rationality_score']\n",
    "\n",
    "    trend_table = trend_tables.get(ticker)\n",
    "    if trend_table is None:\n",
    "        continue\n",
    "\n",
    "    trend_row = trend_table[trend_table['Turn'] == turn]\n",
    "    if trend_row.empty:\n",
    "        continue\n",
    "\n",
    "    trend_row = trend_row.iloc[0]\n",
    "    market_signal_records.append({\n",
    "        'Participant_ID': participant_id,\n",
    "        'Turn': turn,\n",
    "        'Ticker': ticker,\n",
    "        'Volatility': trend_row['Volatility'],\n",
    "        'Price Trend': trend_row['Price Trend'],\n",
    "        'MACD Signal': trend_row['MACD Signal'],\n",
    "        'MACD Histogram': trend_row['MACD Histogram'],\n",
    "        'Volume Spike': trend_row['Volume Spike'],\n",
    "        'Bollinger Band Trend': trend_row['Bollinger Band Trend'],\n",
    "        'Is Attractive to Buy?': trend_row['Is Attractive to Buy?'],\n",
    "        'DecisionFactor_Score': row['DecisionFactor_Score'],\n",
    "        'Indicator_Score': row['Indicator_Score'],\n",
    "        'Knowledge_Bonus': row['Knowledge_Bonus'],\n",
    "        'Rationality Score': rationality_score,\n",
    "        'Rationality Label': rationality\n",
    "    })\n",
    "\n",
    "\n",
    "market_signals_df = pd.DataFrame(market_signal_records)\n",
    "# Now open in append mode to add new sheet\n",
    "with pd.ExcelWriter(output_path, engine='openpyxl', mode='a') as writer:\n",
    "    market_signals_df.to_excel(writer, sheet_name='MarketSignals_WithRationality', index=False)\n",
    "\n",
    "print(\"\\n✅ Market Signals + Rationality Sheet Saved Successfully!\")\n",
    "\n",
    "# --- Create Participant Behavioral Summary ---\n",
    "\n",
    "# --- Create Full Participant Behavioral Summary ---\n",
    "\n",
    "summary_records = []\n",
    "\n",
    "participants = merged_log['Participant_ID'].unique()\n",
    "\n",
    "for pid in participants:\n",
    "    participant_data = merged_log[merged_log['Participant_ID'] == pid]\n",
    "    total_decisions = len(participant_data)\n",
    "\n",
    "    fully_rational = len(participant_data[participant_data['rationality_label'] == 'Fully Rational'])\n",
    "    slightly_rational = len(participant_data[participant_data['rationality_label'] == 'Slightly Rational'])\n",
    "    irrational = len(participant_data[participant_data['rationality_label'] == 'Irrational'])\n",
    "    emotionally_driven = len(participant_data[participant_data['rationality_label'] == 'Emotionally Driven'])\n",
    "\n",
    "    summary_records.append({\n",
    "        'Participant_ID': pid,\n",
    "        'Total Actions': total_decisions,\n",
    "        '# Fully Rational': fully_rational,\n",
    "        '# Slightly Rational': slightly_rational,\n",
    "        '# Irrational': irrational,\n",
    "        '# Emotionally Driven': emotionally_driven,\n",
    "        '% Fully Rational': round((fully_rational / total_decisions) * 100, 2),\n",
    "        '% Slightly Rational': round((slightly_rational / total_decisions) * 100, 2),\n",
    "        '% Irrational': round((irrational / total_decisions) * 100, 2),\n",
    "        '% Emotionally Driven': round((emotionally_driven / total_decisions) * 100, 2),\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "summary_df = pd.DataFrame(summary_records)\n",
    "\n",
    "# Determine Overall Participant Classification\n",
    "def classify_participant(row):\n",
    "    labels = {\n",
    "        'Fully Rational': row['% Fully Rational'],\n",
    "        'Slightly Rational': row['% Slightly Rational'],\n",
    "        'Irrational': row['% Irrational'],\n",
    "        'Emotionally Driven': row['% Emotionally Driven']\n",
    "    }\n",
    "    dominant = max(labels, key=labels.get)\n",
    "    return dominant\n",
    "\n",
    "summary_df['Overall Classification'] = summary_df.apply(classify_participant, axis=1)\n",
    "\n",
    "# Save to the Excel file in a new sheet\n",
    "with pd.ExcelWriter(output_path, engine='openpyxl', mode='a') as writer:\n",
    "    summary_df.to_excel(writer, sheet_name='Participant_Summary', index=False)\n",
    "\n",
    "print(\"\\n✅ Participant Behavioral Summary Sheet Saved Successfully!\")\n",
    "print(\"\\n📊 FINAL OUTPUT: Top 10 Rows of Market Signals + Rationality Data:\\n\")\n",
    "print(market_signals_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85798ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################\n",
    "# BLOCK 12: Compute Final Decision Quality Score (Rationality Proxy)\n",
    "##########################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load files\n",
    "pre_survey = pd.read_excel(r\"E:\\FYP\\FYP Symposium\\Pre-Survey Form (Responses).xlsx\")\n",
    "post_survey = pd.read_excel(r\"E:\\FYP\\FYP Symposium\\Post-Survey (Responses).xlsx\")\n",
    "merged_log = pd.read_excel(r\"E:\\FYP\\FYP Symposium\\Renamed_Merged_Log_and_Strategy.xlsx\")\n",
    "\n",
    "# Clean column headers\n",
    "pre_survey.columns = pre_survey.columns.str.strip()\n",
    "post_survey.columns = post_survey.columns.str.strip()\n",
    "merged_log.columns = merged_log.columns.str.strip()\n",
    "\n",
    "# Rename pre-survey columns\n",
    "pre_survey = pre_survey.rename(columns={\n",
    "    'Participant ID (AXXXX format)\\nIf ID is not assigned yet, kindly contact researchers before proceeding': 'Participant_ID',\n",
    "    'How confident are you in your ability to make profitable trades?': 'Confidence',\n",
    "    'When faced with Financial decisions, do you usually rely on Logic or Intuition?': 'Logic_vs_Intuition',\n",
    "    'How comfortable are you with taking risks?': 'Risk_Tolerance',\n",
    "    'How comfortable are you with uncertainty in Financial Markets?': 'Comfort_with_Uncertainty',\n",
    "    'Do you consider yourself patient or impulsive when making decisions?': 'Impulsiveness'\n",
    "})\n",
    "\n",
    "# Select pre-survey features\n",
    "pre_features = pre_survey[['Participant_ID', 'Confidence', 'Logic_vs_Intuition', 'Risk_Tolerance',\n",
    "                           'Comfort_with_Uncertainty', 'Impulsiveness']]\n",
    "\n",
    "# Rename and select post-survey columns\n",
    "post_survey = post_survey.rename(columns={\n",
    "    'Participant ID': 'Participant_ID',\n",
    "    'How much pressure did you feel during the experiment to perform well?': 'Pressure',\n",
    "    'How well do you think you have performed in the trading simulation?': 'Self_Performance',\n",
    "    'How well do you think you may have performed as compared to other participants?': 'Relative_Performance'\n",
    "})\n",
    "post_features = post_survey[['Participant_ID', 'Pressure', 'Self_Performance', 'Relative_Performance']]\n",
    "\n",
    "# Behavioral summary\n",
    "log_summary = merged_log.groupby('Participant_ID').agg(\n",
    "    Total_Trades=('action', 'count'),\n",
    "    Unique_Tickers=('ticker', pd.Series.nunique)\n",
    ").reset_index()\n",
    "\n",
    "# Final cash (profit proxy) — last row per participant\n",
    "profit_summary = merged_log.sort_values(by=['Participant_ID', 'turn']).groupby('Participant_ID').tail(1)[\n",
    "    ['Participant_ID', 'cash_after']\n",
    "].rename(columns={'cash_after': 'Final_Cash'})\n",
    "\n",
    "# Merge all features\n",
    "merged_df = pre_features.merge(post_features, on='Participant_ID', how='inner')\n",
    "merged_df = merged_df.merge(log_summary, on='Participant_ID', how='inner')\n",
    "merged_df = merged_df.merge(profit_summary, on='Participant_ID', how='left')\n",
    "\n",
    "# Log-transform cash\n",
    "merged_df['Log_Cash'] = np.log(merged_df['Final_Cash'])\n",
    "\n",
    "# Normalize using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(merged_df.drop(columns=['Participant_ID', 'Final_Cash']))\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=merged_df.columns.drop(['Participant_ID', 'Final_Cash']))\n",
    "scaled_df['Participant_ID'] = merged_df['Participant_ID']\n",
    "scaled_df['Final_Cash'] = merged_df['Final_Cash']  # Keep original value\n",
    "\n",
    "# Compute Final Decision Quality Score\n",
    "scaled_df['Final_Decision_Quality_Score'] = (\n",
    "    0.25 * scaled_df[['Confidence', 'Logic_vs_Intuition', 'Risk_Tolerance',\n",
    "                      'Comfort_with_Uncertainty', 'Impulsiveness']].mean(axis=1) +\n",
    "    0.20 * scaled_df[['Pressure', 'Self_Performance', 'Relative_Performance']].mean(axis=1) +\n",
    "    0.15 * scaled_df[['Total_Trades', 'Unique_Tickers']].mean(axis=1) +\n",
    "    0.40 * scaled_df['Log_Cash']\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Example Categorization: Rationality Label\n",
    "# ================================\n",
    "scaled_df['Rationality_Label'] = pd.cut(\n",
    "    scaled_df['Final_Decision_Quality_Score'],\n",
    "    bins=[0, 0.4, 0.7, 1.0],\n",
    "    labels=['Irrational', 'Moderate', 'Rational']\n",
    ")\n",
    "# Add Explanation based on Rationality Label and Score\n",
    "def explain_rationality(score, label):\n",
    "    if score >= 0.85:\n",
    "        return \"Highly rational; confident, logical, and performed strongly under pressure with high profit.\"\n",
    "    elif score >= 0.70:\n",
    "        return \"Generally rational; showed strong strategy use and psychological stability.\"\n",
    "    elif score >= 0.55:\n",
    "        return \"Moderately effective; balanced decision-making but either cautious or inconsistent performance.\"\n",
    "    elif score >= 0.40:\n",
    "        return \"Somewhat impulsive or stressed; moderate trading performance and mixed self-perception.\"\n",
    "    else:\n",
    "        return \"Likely struggled under pressure; low confidence, impulsive patterns, or poor final performance.\"\n",
    "\n",
    "scaled_df['Rationality_Explanation'] = scaled_df.apply(\n",
    "    lambda row: explain_rationality(row['Final_Decision_Quality_Score'], row['Rationality_Label']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Add breakdown explanation per participant\n",
    "def build_score_formula(row):\n",
    "    cog = round(row[['Confidence', 'Logic_vs_Intuition', 'Risk_Tolerance',\n",
    "                     'Comfort_with_Uncertainty', 'Impulsiveness']].mean(), 2)\n",
    "    perc = round(row[['Pressure', 'Self_Performance', 'Relative_Performance']].mean(), 2)\n",
    "    behav = round(row[['Total_Trades', 'Unique_Tickers']].mean(), 2)\n",
    "    perf = round(row['Log_Cash'], 2)\n",
    "\n",
    "    return f\"Score = 25% Cognitive ({cog}) + 20% Perception ({perc}) + 15% Behavior ({behav}) + 40% Log-Cash ({perf})\"\n",
    "\n",
    "scaled_df['Score_Breakdown'] = scaled_df.apply(build_score_formula, axis=1)\n",
    "\n",
    "\n",
    "# Save results\n",
    "scaled_df[['Participant_ID', 'Final_Decision_Quality_Score', 'Rationality_Label', 'Rationality_Explanation', 'Score_Breakdown']].to_excel(\n",
    "    r\"E:\\FYP\\FYP Symposium\\Output 2\\Final_Decision_Quality_Scores.xlsx\", index=False)\n",
    "\n",
    "print(\"✅ Final Decision Quality Scores (with final cash) computed and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

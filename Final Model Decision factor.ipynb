{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31147c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Log Columns:\n",
      "\n",
      "| Merged Log Columns    |\n",
      "|-----------------------|\n",
      "| Date                  |\n",
      "| Real Time             |\n",
      "| Simulation Time       |\n",
      "| Seconds left          |\n",
      "| Participant_ID        |\n",
      "| turn                  |\n",
      "| action                |\n",
      "| ticker                |\n",
      "| News Sentiment        |\n",
      "| News Truth            |\n",
      "| quantity              |\n",
      "| Stock Price           |\n",
      "| Total Stock           |\n",
      "| cash_before           |\n",
      "| cash_after            |\n",
      "| stockportfolio_before |\n",
      "| stockportfolio_after  |\n",
      "| Total_assets          |\n",
      "| total_TSLA_holding    |\n",
      "| total_XOM_holding     |\n",
      "| total_NFLX_holding    |\n",
      "| total_PG_holding      |\n",
      "| TSLA_value            |\n",
      "| XOM_value             |\n",
      "| NFLX_value            |\n",
      "| PG_value              |\n",
      "\n",
      "Strategy Columns:\n",
      "\n",
      "| Strategy Columns   |\n",
      "|--------------------|\n",
      "| TimePre            |\n",
      "| Name               |\n",
      "| Participant_ID     |\n",
      "| PRM1_5             |\n",
      "| PRM1_6             |\n",
      "| PRM1_7             |\n",
      "| PRM1_8             |\n",
      "| PRM1_9             |\n",
      "| PRM1_10            |\n",
      "| PRM1_11            |\n",
      "| PRM1_12            |\n",
      "| PRM1_12a           |\n",
      "| PRM2a_1            |\n",
      "| PRM2a_2            |\n",
      "| PRM2a_3            |\n",
      "| PRM2a_4            |\n",
      "| PRM2b_1            |\n",
      "| PRM2b_2            |\n",
      "| PRM2b_4            |\n",
      "| PRM2b_5            |\n",
      "| PRM2b_6            |\n",
      "| PRM2b_7            |\n",
      "| PRM2b_8            |\n",
      "| PRM2b_9            |\n",
      "| PRM2b_10           |\n",
      "| PRM2b_11           |\n",
      "| PRM2b_12           |\n",
      "| PRM2b_13           |\n",
      "| PRM2b_13a          |\n",
      "| PRM2b_14           |\n",
      "| PRM2b_14a          |\n",
      "| PRM3_1             |\n",
      "| PRM3_2             |\n",
      "| PRM3_3             |\n",
      "| PRM3_4             |\n",
      "| PRM3_5             |\n",
      "| PRM3_6             |\n",
      "| PRM3_7             |\n",
      "| PRM3_8             |\n",
      "| PRM3_9             |\n",
      "| PRM3_10            |\n",
      "| PRM3_11            |\n",
      "| PRM3_12            |\n",
      "| PRM4a_1            |\n",
      "| PRM4a_1a           |\n",
      "| PRM4a_2            |\n",
      "| PRM4a_2a           |\n",
      "| PRM4a_3            |\n",
      "| PRM4a_3a           |\n",
      "| PRM4a_3b           |\n",
      "| PRM4a_4            |\n",
      "| PRM4a_4a           |\n",
      "| PRM4a_4b           |\n",
      "| PRM4a_4c           |\n",
      "| PRM4b_1            |\n",
      "| PRM4b_2            |\n",
      "| PRM4b_3            |\n",
      "| PRM4b_4            |\n",
      "| PRM4b_5            |\n",
      "| PRM4b_5a           |\n",
      "| PRM4b_6            |\n",
      "| PRM4b_7            |\n",
      "| PRM4b_7a           |\n",
      "| PRM4b_8            |\n",
      "| PRM4c_1            |\n",
      "| PRM4c_2            |\n",
      "| PRM4c_3            |\n",
      "| PRM4c_4            |\n",
      "| PRM4c_5            |\n",
      "| PRM4c_6            |\n",
      "| PRM4c_7            |\n",
      "| PRM4c_8            |\n",
      "| PRM4c_8a           |\n",
      "| PRM4c_9            |\n",
      "| PRM2b_3            |\n",
      "| PRM4b_4a           |\n",
      "| PRM4b_5b           |\n",
      "| PRM4b_5c           |\n",
      "| TimePost           |\n",
      "| POM2_1             |\n",
      "| POM2_2             |\n",
      "| POM2_2a            |\n",
      "| POM2_3             |\n",
      "| POM2_3a            |\n",
      "| POM2_4             |\n",
      "| POM2_4a            |\n",
      "| POM2_5             |\n",
      "| POM2_5a            |\n",
      "| POM2_6             |\n",
      "| POM2_7             |\n",
      "| POM2_8             |\n",
      "| POM2_9             |\n",
      "| POM2_9a            |\n",
      "| POM3_1             |\n",
      "| POM3_2             |\n",
      "| POM3_3             |\n",
      "| POM4a_1            |\n",
      "| POM4a_2            |\n",
      "| POM4a_3            |\n",
      "| POM4a_4            |\n",
      "| POM4a_5            |\n",
      "| POM4a_6            |\n",
      "| POM4a_7            |\n",
      "| POM4a_8            |\n",
      "| POM4b_1_T          |\n",
      "| POM4b_1_X          |\n",
      "| POM4b_1_N          |\n",
      "| POM4b_1_P          |\n",
      "| POM4b_2_T          |\n",
      "| POM4b_2_X          |\n",
      "| POM4b_2_N          |\n",
      "| POM4b_2_P          |\n",
      "| POM4b_3_G          |\n",
      "| POM4b_3_DT         |\n",
      "| POM4b_3_NH         |\n",
      "| POM4b_4_OC         |\n",
      "| POM4b_4_HL         |\n",
      "| POM4b_4_V          |\n",
      "| POM4b_4_BB         |\n",
      "| POM4b_4_MACD       |\n",
      "| POM5_1             |\n",
      "| POM5_2             |\n",
      "| POM5_3             |\n",
      "| POM5_4             |\n",
      "| Stock_1st          |\n",
      "| Stock_2nd          |\n",
      "| Stock_3rd          |\n",
      "| Stock_4th          |\n",
      "| Risky_1st          |\n",
      "| Risky_2nd          |\n",
      "| Risky_3rd          |\n",
      "| Risky_4th          |\n",
      "| DecisionFactor_1st |\n",
      "| DecisionFactor_2nd |\n",
      "| DecisionFactor_3rd |\n",
      "| Indicator_1st      |\n",
      "| Indicator_2nd      |\n",
      "| Indicator_3rd      |\n",
      "| Indicator_4th      |\n",
      "| Indicator_5th      |\n",
      "          Date Real Time Simulation Time  Seconds left Participant_ID  turn  \\\n",
      "0   2025-04-23  10:50:33         0:00:55             5          E0070     1   \n",
      "1   2025-04-23  10:51:32         0:01:54            10          E0070     2   \n",
      "2   2025-04-23  10:51:40         0:02:02             2          E0070     2   \n",
      "3   2025-04-23  10:52:01         0:02:23            45          E0070     3   \n",
      "4   2025-04-23  10:52:44         0:03:06             2          E0070     3   \n",
      "5   2025-04-23  10:53:45         0:04:07             5          E0070     4   \n",
      "6   2025-04-23  10:54:15         0:04:37            38          E0070     5   \n",
      "7   2025-04-23  10:54:46         0:05:08             7          E0070     5   \n",
      "8   2025-04-23  10:54:52         0:05:14             1          E0070     5   \n",
      "9   2025-04-23  10:55:23         0:05:45            38          E0070     6   \n",
      "10  2025-04-26  16:47:17         0:00:23            37          E0169     1   \n",
      "11  2025-04-26  16:47:29         0:00:35            25          E0169     1   \n",
      "12  2025-04-26  16:48:12         0:01:18            46          E0169     2   \n",
      "13  2025-04-26  16:48:30         0:01:36            28          E0169     2   \n",
      "14  2025-04-26  16:48:36         0:01:42            21          E0169     2   \n",
      "\n",
      "   action ticker News Sentiment News Truth  ...  stockportfolio_after  \\\n",
      "0     Buy   TSLA       Negative      False  ...               3378.00   \n",
      "1    Sell   TSLA        No News    No News  ...                  0.00   \n",
      "2     Buy     PG       Positive       True  ...               2229.90   \n",
      "3    Sell     PG        No News    No News  ...                  0.00   \n",
      "4     Buy   NFLX       Positive      False  ...               9190.35   \n",
      "5     Buy     PG        Neutral    Neutral  ...               9869.90   \n",
      "6    Sell   NFLX       Negative       True  ...                809.85   \n",
      "7    Sell     PG        No News    No News  ...                  0.00   \n",
      "8     Buy   NFLX       Negative       True  ...               3065.60   \n",
      "9    Sell   NFLX        No News    No News  ...                  0.00   \n",
      "10    Buy   TSLA       Negative      False  ...               5067.00   \n",
      "11    Buy     PG        No News    No News  ...               9489.00   \n",
      "12   Sell   TSLA        No News    No News  ...               4459.80   \n",
      "13   Sell     PG       Positive       True  ...                  0.00   \n",
      "14    Buy     PG       Positive       True  ...               4459.80   \n",
      "\n",
      "    Total_assets  total_TSLA_holding  total_XOM_holding  total_NFLX_holding  \\\n",
      "0       10000.00                  10                  0                   0   \n",
      "1       10342.00                   0                  0                   0   \n",
      "2       10342.00                   0                  0                   0   \n",
      "3       10394.35                   0                  0                   0   \n",
      "4       10394.35                   0                  0                  15   \n",
      "5       10279.60                   0                  0                  15   \n",
      "6       10416.35                   0                  0                   0   \n",
      "7       10416.35                   0                  0                   0   \n",
      "8       10416.35                   0                  0                   5   \n",
      "9       10188.35                   0                  0                   0   \n",
      "10      10000.00                  15                  0                   0   \n",
      "11      10000.00                  15                  0                   0   \n",
      "12      10550.80                   0                  0                   0   \n",
      "13      10550.80                   0                  0                   0   \n",
      "14      10550.80                   0                  0                   0   \n",
      "\n",
      "    total_PG_holding  TSLA_value  XOM_value  NFLX_value  PG_value  \n",
      "0                  0      3378.0        0.0        0.00      0.00  \n",
      "1                  0         0.0        0.0        0.00      0.00  \n",
      "2                 15         0.0        0.0        0.00   2229.90  \n",
      "3                  0         0.0        0.0        0.00      0.00  \n",
      "4                  0         0.0        0.0     9190.35      0.00  \n",
      "5                  5         0.0        0.0     9075.60    794.30  \n",
      "6                  5         0.0        0.0        0.00    809.85  \n",
      "7                  0         0.0        0.0        0.00      0.00  \n",
      "8                  0         0.0        0.0     3065.60      0.00  \n",
      "9                  0         0.0        0.0        0.00      0.00  \n",
      "10                 0      5067.0        0.0        0.00      0.00  \n",
      "11                30      5067.0        0.0        0.00   4422.00  \n",
      "12                30         0.0        0.0        0.00   4459.80  \n",
      "13                 0         0.0        0.0        0.00      0.00  \n",
      "14                30         0.0        0.0        0.00   4459.80  \n",
      "\n",
      "[15 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 1: Load All Files and Print Columns Neatly\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "\n",
    "# Set paths\n",
    "merged_log_path = r'E:\\FYP\\FYP Symposium\\Merged Log (49).xlsx'\n",
    "stock_data_path = r'E:\\FYP\\FYP Symposium\\Trading Simulation Experiment Data Turn Wise.xlsx'\n",
    "dta_path = r'E:\\FYP\\FYP Symposium\\SurveysClean.dta'\n",
    "\n",
    "# Load Data\n",
    "merged_log = pd.read_excel(merged_log_path)\n",
    "stock_xls = pd.ExcelFile(stock_data_path)\n",
    "stock_tables = {stock: pd.read_excel(stock_xls, sheet_name=stock) for stock in ['TSLA', 'XOM', 'NFLX', 'PG']}\n",
    "strategy = pd.read_stata(dta_path)\n",
    "\n",
    "# Rename columns in each stock sheet\n",
    "rename_map = {\n",
    "    'trend': 'Close_price_diff',\n",
    "    'trend direction': 'price_trend_1',\n",
    "    'trend summary': 'price_trend_7',\n",
    "    'volume trend change': 'volume_diff',\n",
    "    'volume trend direction': 'volume_trend_1',\n",
    "    'volume trend summary': 'volume_trend_7',\n",
    "    'Technical Decision': 'MACD_trend',\n",
    "    'Bollinger Classification': 'bollinger_trend'\n",
    "}\n",
    "\n",
    "for stock_name, df in stock_tables.items():\n",
    "    stock_tables[stock_name].rename(columns=rename_map, inplace=True)\n",
    "\n",
    "\n",
    "# Show loaded columns for verification (pretty print using tabulate)\n",
    "print(\"Merged Log Columns:\\n\")\n",
    "print(tabulate([[col] for col in merged_log.columns], headers=[\"Merged Log Columns\"], tablefmt=\"github\"))\n",
    "\n",
    "print(\"\\nStrategy Columns:\\n\")\n",
    "print(tabulate([[col] for col in strategy.columns], headers=[\"Strategy Columns\"], tablefmt=\"github\"))\n",
    "\n",
    "print(merged_log.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92ec9120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged Log Columns (After Rename):\n",
      "\n",
      "| Merged Log Columns    |\n",
      "|-----------------------|\n",
      "| Date                  |\n",
      "| Real Time             |\n",
      "| Simulation Time       |\n",
      "| Seconds left          |\n",
      "| Participant_ID        |\n",
      "| turn                  |\n",
      "| action                |\n",
      "| ticker                |\n",
      "| news_sentiment        |\n",
      "| news_truth            |\n",
      "| quantity              |\n",
      "| Stock Price           |\n",
      "| Total Stock           |\n",
      "| cash_before           |\n",
      "| cash_after            |\n",
      "| stockportfolio_before |\n",
      "| stockportfolio_after  |\n",
      "| Total_assets          |\n",
      "| total_TSLA_holding    |\n",
      "| total_XOM_holding     |\n",
      "| total_NFLX_holding    |\n",
      "| total_PG_holding      |\n",
      "| TSLA_value            |\n",
      "| XOM_value             |\n",
      "| NFLX_value            |\n",
      "| PG_value              |\n",
      "\n",
      "Strategy Columns (After Rename):\n",
      "\n",
      "| Strategy Columns   |\n",
      "|--------------------|\n",
      "| TimePre            |\n",
      "| Name               |\n",
      "| Participant_ID     |\n",
      "| PRM1_5             |\n",
      "| PRM1_6             |\n",
      "| PRM1_7             |\n",
      "| PRM1_8             |\n",
      "| PRM1_9             |\n",
      "| PRM1_10            |\n",
      "| PRM1_11            |\n",
      "| PRM1_12            |\n",
      "| PRM1_12a           |\n",
      "| PRM2a_1            |\n",
      "| PRM2a_2            |\n",
      "| PRM2a_3            |\n",
      "| PRM2a_4            |\n",
      "| PRM2b_1            |\n",
      "| PRM2b_2            |\n",
      "| PRM2b_4            |\n",
      "| PRM2b_5            |\n",
      "| PRM2b_6            |\n",
      "| PRM2b_7            |\n",
      "| PRM2b_8            |\n",
      "| PRM2b_9            |\n",
      "| PRM2b_10           |\n",
      "| PRM2b_11           |\n",
      "| PRM2b_12           |\n",
      "| PRM2b_13           |\n",
      "| PRM2b_13a          |\n",
      "| PRM2b_14           |\n",
      "| PRM2b_14a          |\n",
      "| PRM3_1             |\n",
      "| PRM3_2             |\n",
      "| PRM3_3             |\n",
      "| PRM3_4             |\n",
      "| PRM3_5             |\n",
      "| PRM3_6             |\n",
      "| PRM3_7             |\n",
      "| PRM3_8             |\n",
      "| PRM3_9             |\n",
      "| PRM3_10            |\n",
      "| PRM3_11            |\n",
      "| PRM3_12            |\n",
      "| PRM4a_1            |\n",
      "| PRM4a_1a           |\n",
      "| PRM4a_2            |\n",
      "| PRM4a_2a           |\n",
      "| PRM4a_3            |\n",
      "| PRM4a_3a           |\n",
      "| PRM4a_3b           |\n",
      "| PRM4a_4            |\n",
      "| PRM4a_4a           |\n",
      "| PRM4a_4b           |\n",
      "| PRM4a_4c           |\n",
      "| PRM4b_1            |\n",
      "| PRM4b_2            |\n",
      "| PRM4b_3            |\n",
      "| PRM4b_4            |\n",
      "| PRM4b_5            |\n",
      "| PRM4b_5a           |\n",
      "| PRM4b_6            |\n",
      "| PRM4b_7            |\n",
      "| PRM4b_7a           |\n",
      "| PRM4b_8            |\n",
      "| PRM4c_1            |\n",
      "| PRM4c_2            |\n",
      "| PRM4c_3            |\n",
      "| PRM4c_4            |\n",
      "| PRM4c_5            |\n",
      "| PRM4c_6            |\n",
      "| PRM4c_7            |\n",
      "| PRM4c_8            |\n",
      "| PRM4c_8a           |\n",
      "| PRM4c_9            |\n",
      "| PRM2b_3            |\n",
      "| PRM4b_4a           |\n",
      "| PRM4b_5b           |\n",
      "| PRM4b_5c           |\n",
      "| TimePost           |\n",
      "| POM2_1             |\n",
      "| POM2_2             |\n",
      "| POM2_2a            |\n",
      "| POM2_3             |\n",
      "| POM2_3a            |\n",
      "| POM2_4             |\n",
      "| POM2_4a            |\n",
      "| POM2_5             |\n",
      "| POM2_5a            |\n",
      "| POM2_6             |\n",
      "| POM2_7             |\n",
      "| POM2_8             |\n",
      "| POM2_9             |\n",
      "| POM2_9a            |\n",
      "| POM3_1             |\n",
      "| POM3_2             |\n",
      "| POM3_3             |\n",
      "| POM4a_1            |\n",
      "| POM4a_2            |\n",
      "| POM4a_3            |\n",
      "| POM4a_4            |\n",
      "| POM4a_5            |\n",
      "| POM4a_6            |\n",
      "| POM4a_7            |\n",
      "| POM4a_8            |\n",
      "| POM4b_1_T          |\n",
      "| POM4b_1_X          |\n",
      "| POM4b_1_N          |\n",
      "| POM4b_1_P          |\n",
      "| POM4b_2_T          |\n",
      "| POM4b_2_X          |\n",
      "| POM4b_2_N          |\n",
      "| POM4b_2_P          |\n",
      "| POM4b_3_G          |\n",
      "| POM4b_3_DT         |\n",
      "| POM4b_3_NH         |\n",
      "| POM4b_4_OC         |\n",
      "| POM4b_4_HL         |\n",
      "| POM4b_4_V          |\n",
      "| POM4b_4_BB         |\n",
      "| POM4b_4_MACD       |\n",
      "| POM5_1             |\n",
      "| POM5_2             |\n",
      "| POM5_3             |\n",
      "| POM5_4             |\n",
      "| Stock_1st          |\n",
      "| Stock_2nd          |\n",
      "| Stock_3rd          |\n",
      "| Stock_4th          |\n",
      "| Risky_1st          |\n",
      "| Risky_2nd          |\n",
      "| Risky_3rd          |\n",
      "| Risky_4th          |\n",
      "| DecisionFactor_1st |\n",
      "| DecisionFactor_2nd |\n",
      "| DecisionFactor_3rd |\n",
      "| Indicator_1st      |\n",
      "| Indicator_2nd      |\n",
      "| Indicator_3rd      |\n",
      "| Indicator_4th      |\n",
      "| Indicator_5th      |\n",
      "The renamed merged_log and strategy have been exported to: E:\\FYP\\FYP Symposium\\Outputs\\Renamed_Merged_Log_and_Strategy.xlsx\n",
      "  Participant_ID DecisionFactor_1st DecisionFactor_2nd DecisionFactor_3rd  \\\n",
      "0          E0070              Graph         Data Table      News Headline   \n",
      "1          E0169              Graph      News Headline         Data Table   \n",
      "2          E0426      News Headline         Data Table              Graph   \n",
      "3          E0712      News Headline              Graph         Data Table   \n",
      "4          E1130              Graph      News Headline         Data Table   \n",
      "5          E1217      News Headline              Graph         Data Table   \n",
      "6          E1719      News Headline         Data Table              Graph   \n",
      "7          E2010         Data Table              Graph      News Headline   \n",
      "8          E2514         Data Table              Graph      News Headline   \n",
      "9          E3007      News Headline              Graph         Data Table   \n",
      "\n",
      "   Graph Scoring  Data Table Scoring  Average Scoring  \n",
      "0            5.0                 3.5             4.25  \n",
      "1            5.0                 2.0             3.50  \n",
      "2            2.0                 3.5             2.75  \n",
      "3            3.5                 2.0             2.75  \n",
      "4            5.0                 2.0             3.50  \n",
      "5            3.5                 2.0             2.75  \n",
      "6            2.0                 3.5             2.75  \n",
      "7            3.5                 5.0             4.25  \n",
      "8            3.5                 5.0             4.25  \n",
      "9            3.5                 2.0             2.75  \n",
      "✅ Both updated sheets exported to: E:\\FYP\\FYP Symposium\\Outputs\\Final_Merged_Log_and_Strategy_With_Scoring.xlsx\n",
      "The updated strategy with scoring has been exported to: E:\\FYP\\FYP Symposium\\Outputs\\Updated_Strategy_With_Scoring.xlsx\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 2: Rename Columns for Consistency\n",
    "\n",
    "# Rename Merged Log Columns to match our expectations\n",
    "merged_log.rename(columns={\n",
    "    'News Sentiment': 'news_sentiment',\n",
    "    'News Truth': 'news_truth'\n",
    "}, inplace=True)\n",
    "\n",
    "# Rename Strategy Columns if needed\n",
    "if 'participant_id' not in strategy.columns:\n",
    "    if 'ResponseID' in strategy.columns:\n",
    "        strategy.rename(columns={'ResponseID': 'participant_id'}, inplace=True)\n",
    "    elif 'ResponseId' in strategy.columns:\n",
    "        strategy.rename(columns={'ResponseId': 'participant_id'}, inplace=True)\n",
    "\n",
    "# Check after renaming\n",
    "print(\"Merged Log Columns (After Rename):\\n\")\n",
    "print(tabulate([[col] for col in merged_log.columns], headers=[\"Merged Log Columns\"], tablefmt=\"github\"))\n",
    "\n",
    "print(\"\\nStrategy Columns (After Rename):\\n\")\n",
    "print(tabulate([[col] for col in strategy.columns], headers=[\"Strategy Columns\"], tablefmt=\"github\"))\n",
    "\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Assuming merged_log and strategy are already available after renaming\n",
    "\n",
    "# Export both merged_log and strategy to Excel with separate sheets\n",
    "output_file = r'E:\\FYP\\FYP Symposium\\Outputs\\Renamed_Merged_Log_and_Strategy.xlsx'  # Specify the file path\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "    merged_log.to_excel(writer, sheet_name='Merged_Log', index=False)  # Save merged_log to the first sheet\n",
    "    strategy.to_excel(writer, sheet_name='Strategy', index=False)  # Save strategy to the second sheet\n",
    "\n",
    "print(f\"The renamed merged_log and strategy have been exported to: {output_file}\")\n",
    "strategy.head(15)\n",
    "\n",
    "# Define scoring function\n",
    "def assign_score(row, method_name):\n",
    "    score = 0\n",
    "    for i, weight in zip(['1st', '2nd', '3rd'], [5, 3.5, 2]):\n",
    "        factor = row.get(f'DecisionFactor_{i}', None)\n",
    "        if factor == method_name:\n",
    "            score = weight\n",
    "            break\n",
    "    return score\n",
    "\n",
    "# Add scoring for 'Graph'\n",
    "strategy['Graph Scoring'] = strategy.apply(lambda row: assign_score(row, 'Graph'), axis=1)\n",
    "\n",
    "# Add scoring for 'Data Table'\n",
    "strategy['Data Table Scoring'] = strategy.apply(lambda row: assign_score(row, 'Data Table'), axis=1)\n",
    "\n",
    "# Add column for average of Graph Scoring and Data Table Scoring\n",
    "strategy['Average Scoring'] = strategy[['Graph Scoring', 'Data Table Scoring']].mean(axis=1)\n",
    "\n",
    "# View result\n",
    "print(strategy[['Participant_ID', 'DecisionFactor_1st', 'DecisionFactor_2nd', 'DecisionFactor_3rd',\n",
    "                'Graph Scoring', 'Data Table Scoring', 'Average Scoring']].head(10))\n",
    "\n",
    "# Final export with both sheets: updated strategy and merged_log\n",
    "final_output_file = r'E:\\FYP\\FYP Symposium\\Outputs\\Final_Merged_Log_and_Strategy_With_Scoring.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(final_output_file, engine='xlsxwriter') as writer:\n",
    "    merged_log.to_excel(writer, sheet_name='Merged_Log', index=False)\n",
    "    strategy.to_excel(writer, sheet_name='Strategy', index=False)\n",
    "\n",
    "print(f\"✅ Both updated sheets exported to: {final_output_file}\")\n",
    "\n",
    "print(\"The updated strategy with scoring has been exported to: E:\\\\FYP\\\\FYP Symposium\\\\Outputs\\\\Updated_Strategy_With_Scoring.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec5e1214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Merged updated knowledge into strategy successfully.\n",
      "\n",
      "✅ Final Knowledge Score (range 0.5–3.0):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>Knows_MACD</th>\n",
       "      <th>Knows_Bollinger</th>\n",
       "      <th>Knows_Volume</th>\n",
       "      <th>Knowledge_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E0070</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E0169</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E0426</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E0712</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E1130</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Participant_ID Knows_MACD Knows_Bollinger Knows_Volume  Knowledge_Score\n",
       "0          E0070         No              No           No              0.5\n",
       "1          E0169        Yes             Yes          Yes              3.0\n",
       "2          E0426         No              No           No              0.5\n",
       "3          E0712         No              No           No              0.5\n",
       "4          E1130        Yes              No           No              1.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Updated Knowledge Data (with Knowledge_Score) exported to: E:\\FYP\\FYP Symposium\\Outputs\\Knowledge_Score_Updated.xlsx\n"
     ]
    }
   ],
   "source": [
    "# BLOCK 3: Extract PRM2b_14a indicator knowledge cleanly\n",
    "\n",
    "def check_knowledge(prm2b_14a_response):\n",
    "    if pd.isna(prm2b_14a_response):\n",
    "        return {'MACD': False, 'Bollinger': False, 'Volume': False, 'HighLow_OpenClose': False}\n",
    "\n",
    "    text = prm2b_14a_response.lower()\n",
    "    \n",
    "    knowledge = {\n",
    "        'MACD': False,\n",
    "        'Bollinger': False,\n",
    "        'Volume': False,\n",
    "        'HighLow_OpenClose': False\n",
    "    }\n",
    "\n",
    "    # Check MACD knowledge\n",
    "    if 'macd' in text:\n",
    "        knowledge['MACD'] = True\n",
    "    \n",
    "    # Check Bollinger Bands\n",
    "    if 'bollinger' in text or 'bb' in text:\n",
    "        knowledge['Bollinger'] = True\n",
    "\n",
    "    # Check Volume\n",
    "    if 'volume' in text:\n",
    "        knowledge['Volume'] = True\n",
    "\n",
    "    # Check High Low Open Close\n",
    "    if 'high' in text or 'low' in text or 'open' in text or 'close' in text or 'ohlc' in text:\n",
    "        knowledge['HighLow_OpenClose'] = True\n",
    "\n",
    "    return knowledge\n",
    "\n",
    "# --- Apply to all participants ---\n",
    "\n",
    "knowledge_records = []\n",
    "\n",
    "for idx, row in strategy.iterrows():\n",
    "    participant_id = row['Participant_ID']\n",
    "    prm2b_14a = row.get('PRM2b_14a', None)\n",
    "    knowledge = check_knowledge(prm2b_14a)\n",
    "\n",
    "    knowledge_records.append({\n",
    "        'Participant_ID': participant_id,\n",
    "        'Knows_MACD': 'Yes' if knowledge['MACD'] else 'No',\n",
    "        'Knows_Bollinger': 'Yes' if knowledge['Bollinger'] else 'No',\n",
    "        'Knows_Volume': 'Yes' if knowledge['Volume'] else 'No',\n",
    "        'Knows_HighLow_OpenClose': 'Yes' if knowledge['HighLow_OpenClose'] else 'No'\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "knowledge_df = pd.DataFrame(knowledge_records)\n",
    "\n",
    "# Drop 'Knows_HighLow_OpenClose' — no longer needed\n",
    "if 'Knows_HighLow_OpenClose' in knowledge_df.columns:\n",
    "    knowledge_df.drop(columns=['Knows_HighLow_OpenClose'], inplace=True)\n",
    "\n",
    "# --- Merge Cleaned Knowledge into Strategy ---\n",
    "\n",
    "# Drop old versions to avoid MergeError\n",
    "strategy.drop(columns=['Knows_MACD', 'Knows_Bollinger', 'Knows_Volume', 'Knows_HighLow_OpenClose', 'Knowledge_Score'], errors='ignore', inplace=True)\n",
    "\n",
    "# Merge new knowledge\n",
    "strategy = pd.merge(strategy, knowledge_df, on='Participant_ID', how='left')\n",
    "print(\"\\n✅ Merged updated knowledge into strategy successfully.\")\n",
    "\n",
    "# --- Scoring Logic ---\n",
    "\n",
    "# Map Yes/No with new scoring logic (Volume: Yes = 1, No = 0.5)\n",
    "knowledge_df_numeric = knowledge_df.copy()\n",
    "knowledge_df_numeric['Knows_MACD'] = knowledge_df_numeric['Knows_MACD'].map({'Yes': 1, 'No': 0})\n",
    "knowledge_df_numeric['Knows_Bollinger'] = knowledge_df_numeric['Knows_Bollinger'].map({'Yes': 1, 'No': 0})\n",
    "knowledge_df_numeric['Knows_Volume'] = knowledge_df_numeric['Knows_Volume'].map({'Yes': 1, 'No': 0.5})\n",
    "\n",
    "# Compute score\n",
    "knowledge_df['Knowledge_Score'] = (\n",
    "    knowledge_df_numeric['Knows_MACD'] +\n",
    "    knowledge_df_numeric['Knows_Bollinger'] +\n",
    "    knowledge_df_numeric['Knows_Volume']\n",
    ")\n",
    "\n",
    "# Show sample output\n",
    "print(\"\\n✅ Final Knowledge Score (range 0.5–3.0):\")\n",
    "display(knowledge_df[['Participant_ID', 'Knows_MACD', 'Knows_Bollinger', 'Knows_Volume', 'Knowledge_Score']].head())\n",
    "\n",
    "# Optional documentation string\n",
    "knowledge_score_note = \"Knowledge_Score = Knows_MACD (1/0) + Knows_Bollinger (1/0) + Knows_Volume (1 if Yes, 0.5 if No); Range = 0.5 to 3.0\"\n",
    "\n",
    "# Export to Excel\n",
    "output_knowledge_path = r'E:\\FYP\\FYP Symposium\\Outputs\\Knowledge_Score_Updated.xlsx'\n",
    "knowledge_df.to_excel(output_knowledge_path, index=False)\n",
    "print(f\"\\n✅ Updated Knowledge Data (with Knowledge_Score) exported to: {output_knowledge_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aebd495f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MM COMPUTERS\\AppData\\Local\\Temp\\ipykernel_26092\\3183001729.py:102: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Turn', group_keys=False).apply(summarize_trend_block)\n",
      "C:\\Users\\MM COMPUTERS\\AppData\\Local\\Temp\\ipykernel_26092\\3183001729.py:121: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Turn', group_keys=False).apply(summarize_volume_trend_block)\n",
      "C:\\Users\\MM COMPUTERS\\AppData\\Local\\Temp\\ipykernel_26092\\3183001729.py:102: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Turn', group_keys=False).apply(summarize_trend_block)\n",
      "C:\\Users\\MM COMPUTERS\\AppData\\Local\\Temp\\ipykernel_26092\\3183001729.py:121: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Turn', group_keys=False).apply(summarize_volume_trend_block)\n",
      "C:\\Users\\MM COMPUTERS\\AppData\\Local\\Temp\\ipykernel_26092\\3183001729.py:102: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Turn', group_keys=False).apply(summarize_trend_block)\n",
      "C:\\Users\\MM COMPUTERS\\AppData\\Local\\Temp\\ipykernel_26092\\3183001729.py:121: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Turn', group_keys=False).apply(summarize_volume_trend_block)\n",
      "C:\\Users\\MM COMPUTERS\\AppData\\Local\\Temp\\ipykernel_26092\\3183001729.py:102: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Turn', group_keys=False).apply(summarize_trend_block)\n",
      "C:\\Users\\MM COMPUTERS\\AppData\\Local\\Temp\\ipykernel_26092\\3183001729.py:121: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df = df.groupby('Turn', group_keys=False).apply(summarize_volume_trend_block)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported successfully with trend summaries (Close & Volume) for all sheets to: E:\\FYP\\FYP Symposium\\Outputs\\Combined_Turn_With_Trend_Summary.xlsx\n",
      "\n",
      "Sample Trend Table for TSLA:\n",
      "   Turn  Close Price  High Price  Low Price  Volatility Price Trend  \\\n",
      "0   1.0       337.80      343.99     326.20   26.239602     neutral   \n",
      "1   2.0       372.00      377.59     354.00   11.078977     neutral   \n",
      "2   3.0       336.34      340.55     316.83   17.719387     neutral   \n",
      "3   4.0       325.33      326.25     309.42   14.211373   downtrend   \n",
      "4   5.0       364.65      372.33     356.91   25.304652     uptrend   \n",
      "\n",
      "   Score - Price Trend price_trend_7  Score - Price Trend (7)  \\\n",
      "0                    0     downtrend                       -1   \n",
      "1                    0       uptrend                        1   \n",
      "2                    0     downtrend                       -1   \n",
      "3                   -1       uptrend                        1   \n",
      "4                    1       uptrend                        1   \n",
      "\n",
      "  Trend Reversal Flag  ...  Score - Bollinger Trend (7) Volume Spike  \\\n",
      "0                 Yes  ...                            0        False   \n",
      "1                 Yes  ...                            0        False   \n",
      "2                 Yes  ...                            0        False   \n",
      "3                  No  ...                            0        False   \n",
      "4                 Yes  ...                            0        False   \n",
      "\n",
      "   Score - Volume Spike  volume_trend_7  Score - Volume Trend (7)  \\\n",
      "0                     0         uptrend                         1   \n",
      "1                     0         uptrend                         1   \n",
      "2                     0         uptrend                         1   \n",
      "3                     0         uptrend                         1   \n",
      "4                     0         uptrend                         1   \n",
      "\n",
      "  Volume Trend Support  Score - Volume Support Buy Confidence Score  \\\n",
      "0                   No                       0                    0   \n",
      "1                   No                       0                    4   \n",
      "2                   No                       0                   -1   \n",
      "3                   No                       0                    2   \n",
      "4                   No                       0                    4   \n",
      "\n",
      "   Buy Confidence Score_Normalized Is Attractive to Buy?  \n",
      "0                              3.0                 Risky  \n",
      "1                              5.0            Attractive  \n",
      "2                              2.5                 Risky  \n",
      "3                              4.0              Cautious  \n",
      "4                              5.0            Attractive  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "Sample Trend Table for XOM:\n",
      "   Turn  Close Price  High Price  Low Price  Volatility Price Trend  \\\n",
      "0   1.0        64.37       64.77      63.22    0.920212     neutral   \n",
      "1   2.0        63.48       63.85      62.58    1.526913     neutral   \n",
      "2   3.0        61.58       62.16      61.42    0.770312   downtrend   \n",
      "3   4.0        61.27       61.50      60.01    0.640584   downtrend   \n",
      "4   5.0        61.89       61.95      60.42    0.904755   downtrend   \n",
      "\n",
      "   Score - Price Trend price_trend_7  Score - Price Trend (7)  \\\n",
      "0                    0       uptrend                        1   \n",
      "1                    0       uptrend                        1   \n",
      "2                   -1     downtrend                       -1   \n",
      "3                   -1       uptrend                        1   \n",
      "4                   -1       uptrend                        1   \n",
      "\n",
      "  Trend Reversal Flag  ...  Score - Bollinger Trend (7) Volume Spike  \\\n",
      "0                  No  ...                            0        False   \n",
      "1                 Yes  ...                            0        False   \n",
      "2                 Yes  ...                            0        False   \n",
      "3                  No  ...                            0        False   \n",
      "4                  No  ...                            0        False   \n",
      "\n",
      "   Score - Volume Spike  volume_trend_7  Score - Volume Trend (7)  \\\n",
      "0                     0         uptrend                         1   \n",
      "1                     0       downtrend                        -1   \n",
      "2                     0       downtrend                        -1   \n",
      "3                     0       downtrend                        -1   \n",
      "4                     0       downtrend                        -1   \n",
      "\n",
      "  Volume Trend Support  Score - Volume Support Buy Confidence Score  \\\n",
      "0                   No                       0                    4   \n",
      "1                   No                       0                    0   \n",
      "2                   No                       0                   -6   \n",
      "3                   No                       0                   -2   \n",
      "4                   No                       0                   -2   \n",
      "\n",
      "   Buy Confidence Score_Normalized Is Attractive to Buy?  \n",
      "0                             4.33            Attractive  \n",
      "1                             3.00                 Risky  \n",
      "2                             1.00                 Risky  \n",
      "3                             2.33                 Risky  \n",
      "4                             2.33                 Risky  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "Sample Trend Table for NFLX:\n",
      "   Turn  Close Price  High Price  Low Price  Volatility Price Trend  \\\n",
      "0   1.0       679.33      685.26     671.49   15.028542     neutral   \n",
      "1   2.0       658.29      661.44     651.10   15.476832     neutral   \n",
      "2   3.0       612.69      617.29     601.00   25.659311   downtrend   \n",
      "3   4.0       605.04      605.69     584.51   11.136820   downtrend   \n",
      "4   5.0       613.12      615.00     609.25   11.881903   downtrend   \n",
      "\n",
      "   Score - Price Trend price_trend_7  Score - Price Trend (7)  \\\n",
      "0                    0       uptrend                        1   \n",
      "1                    0     downtrend                       -1   \n",
      "2                   -1     downtrend                       -1   \n",
      "3                   -1       uptrend                        1   \n",
      "4                   -1     downtrend                       -1   \n",
      "\n",
      "  Trend Reversal Flag  ...  Score - Bollinger Trend (7) Volume Spike  \\\n",
      "0                 Yes  ...                            0        False   \n",
      "1                  No  ...                            0        False   \n",
      "2                 Yes  ...                            0        False   \n",
      "3                 Yes  ...                            0        False   \n",
      "4                  No  ...                            0        False   \n",
      "\n",
      "   Score - Volume Spike  volume_trend_7  Score - Volume Trend (7)  \\\n",
      "0                     0       downtrend                        -1   \n",
      "1                     0       downtrend                        -1   \n",
      "2                     0       downtrend                        -1   \n",
      "3                     0         uptrend                         1   \n",
      "4                     0         uptrend                         1   \n",
      "\n",
      "  Volume Trend Support  Score - Volume Support Buy Confidence Score  \\\n",
      "0                   No                       0                    0   \n",
      "1                   No                       0                   -4   \n",
      "2                   No                       0                   -5   \n",
      "3                   No                       0                    0   \n",
      "4                   No                       0                   -2   \n",
      "\n",
      "   Buy Confidence Score_Normalized Is Attractive to Buy?  \n",
      "0                              5.0                 Risky  \n",
      "1                              1.8                 Risky  \n",
      "2                              1.0                 Risky  \n",
      "3                              5.0                 Risky  \n",
      "4                              3.4                 Risky  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "Sample Trend Table for PG:\n",
      "   Turn  Close Price  High Price  Low Price  Volatility Price Trend  \\\n",
      "0   1.0       147.40      147.75     146.47    0.840094     neutral   \n",
      "1   2.0       148.66      148.88     147.68    0.973186     neutral   \n",
      "2   3.0       152.15      152.39     150.19    2.476546     uptrend   \n",
      "3   4.0       158.86      159.64     157.64    2.935980     uptrend   \n",
      "4   5.0       161.97      161.99     160.61    1.772159     uptrend   \n",
      "\n",
      "   Score - Price Trend price_trend_7  Score - Price Trend (7)  \\\n",
      "0                    0       uptrend                        1   \n",
      "1                    0     downtrend                       -1   \n",
      "2                    1       uptrend                        1   \n",
      "3                    1       uptrend                        1   \n",
      "4                    1       uptrend                        1   \n",
      "\n",
      "  Trend Reversal Flag  ...  Score - Bollinger Trend (7) Volume Spike  \\\n",
      "0                 Yes  ...                            0        False   \n",
      "1                 Yes  ...                            0        False   \n",
      "2                  No  ...                            0         True   \n",
      "3                  No  ...                            0        False   \n",
      "4                  No  ...                            0        False   \n",
      "\n",
      "   Score - Volume Spike  volume_trend_7  Score - Volume Trend (7)  \\\n",
      "0                     0       downtrend                        -1   \n",
      "1                     0         uptrend                         1   \n",
      "2                     1         uptrend                         1   \n",
      "3                     0         uptrend                         1   \n",
      "4                     0       downtrend                        -1   \n",
      "\n",
      "  Volume Trend Support  Score - Volume Support Buy Confidence Score  \\\n",
      "0                   No                       0                    2   \n",
      "1                   No                       0                    2   \n",
      "2                  Yes                       0                    6   \n",
      "3                   No                       0                    7   \n",
      "4                   No                       0                    4   \n",
      "\n",
      "   Buy Confidence Score_Normalized Is Attractive to Buy?  \n",
      "0                             1.00              Cautious  \n",
      "1                             1.00              Cautious  \n",
      "2                             3.67            Attractive  \n",
      "3                             4.33            Attractive  \n",
      "4                             2.33            Attractive  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for TSLA\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for TSLA\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for TSLA\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for TSLA\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for TSLA\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for TSLA\n",
      "⚠️ Turn 6 not found in trend table for TSLA\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for TSLA\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for TSLA\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for TSLA\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for TSLA\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for TSLA\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for TSLA\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for TSLA\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for TSLA\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for TSLA\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "⚠️ Turn 6 not found in trend table for NFLX\n",
      "⚠️ Turn 6 not found in trend table for PG\n",
      "⚠️ Turn 6 not found in trend table for XOM\n",
      "✅ Final enriched log WITH signal alignment score exported to: E:\\FYP\\FYP Symposium\\Outputs\\Final_Enriched_Log_With_Signal_Score.xlsx\n",
      "✅ Final enriched trend tables exported to: E:\\FYP\\FYP Symposium\\Outputs\\Final_Trend_Tables.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ========== BLOCK 4: Factual trends (MACD/Bollinger/Price/Volume signals) ==========\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 3.1: Check if participant knows indicator\n",
    "def participant_knows_indicator(prm2b_14a_response, indicator):\n",
    "    if pd.isnull(prm2b_14a_response):\n",
    "        return False\n",
    "    return indicator.lower() in prm2b_14a_response.lower()\n",
    "\n",
    "# --- 3.2: Detect market signals from price data\n",
    "def detect_market_signals(stock_df, turn, days=5):\n",
    "    visible_data = stock_df[stock_df['Turn'] <= turn].sort_values(by='Turn', ascending=False).head(days)\n",
    "\n",
    "    price_trend = 'neutral'\n",
    "    volume_spike = False\n",
    "    macd_signal = 'neutral'\n",
    "    bb_trend = 'neutral'\n",
    "\n",
    "    if len(visible_data) >= 3:\n",
    "        close_now = visible_data.iloc[0]['Close']\n",
    "        close_past = visible_data.iloc[-1]['Close']\n",
    "        change_pct = (close_now - close_past) / close_past * 100\n",
    "\n",
    "        if change_pct > 2.0:\n",
    "            price_trend = 'uptrend'\n",
    "        elif change_pct < -2.0:\n",
    "            price_trend = 'downtrend'\n",
    "        else:\n",
    "            price_trend = 'neutral'\n",
    "\n",
    "        avg_volume = visible_data['Volume'].mean()\n",
    "        curr_volume = visible_data.iloc[0]['Volume']\n",
    "        if curr_volume > 1.2 * avg_volume:\n",
    "            volume_spike = True\n",
    "\n",
    "        macd = visible_data.iloc[0]['MACD (12,26,9)']\n",
    "        signal = visible_data.iloc[0]['Signal (12,26,9)']\n",
    "        macd_hist = visible_data.iloc[0]['MACD Histogram (12,26,9)']\n",
    "\n",
    "        if macd > signal and macd_hist > 0:\n",
    "            macd_signal = 'buy'\n",
    "        elif macd < signal and macd_hist < 0:\n",
    "            macd_signal = 'sell'\n",
    "\n",
    "        close = visible_data.iloc[0]['Close']\n",
    "        top_bb = visible_data.iloc[0]['Top Bollinger Bands (20,O,2,ma,n)']\n",
    "        bottom_bb = visible_data.iloc[0]['Bottom Bollinger Bands (20,O,2,ma,n)']\n",
    "        if close > top_bb:\n",
    "            bb_trend = 'overbought'\n",
    "        elif close < bottom_bb:\n",
    "            bb_trend = 'oversold'\n",
    "        else:\n",
    "            bb_trend = 'neutral'\n",
    "\n",
    "    return price_trend, volume_spike, macd_signal, bb_trend\n",
    "\n",
    "# --- Suggest Buy Logic\n",
    "def suggest_buy_decision(price_trend, macd_signal, bb_trend, volume_spike, volatility):\n",
    "    score = 0\n",
    "    if price_trend == 'uptrend': score += 1\n",
    "    if macd_signal == 'buy': score += 1\n",
    "    if bb_trend == 'oversold': score += 1\n",
    "    if volume_spike: score += 1\n",
    "    if volatility < 10: score += 1\n",
    "    return 'Attractive' if score >= 2 else 'Risky'\n",
    "\n",
    "# --- Load raw stock data\n",
    "experiment_file_path = r'E:\\FYP\\FYP Symposium\\Trading Simulation Experiment Data Turn Wise.xlsx'\n",
    "stock_tables = {}\n",
    "\n",
    "for stock in ['TSLA', 'XOM', 'NFLX', 'PG']:\n",
    "    df = pd.read_excel(experiment_file_path, sheet_name=stock)\n",
    "    df = df.sort_values(by='Turn', ascending=False).reset_index(drop=True)\n",
    "    stock_tables[stock] = df\n",
    "\n",
    "# ========== TURN-WISE TREND METRICS CREATION ==========\n",
    "file_path = r'E:\\FYP\\FYP Symposium\\Turn Data in descending order.xlsx'\n",
    "sheet_names = ['TSLA', 'NFLX', 'PG', 'XOM']\n",
    "turn_data = {}\n",
    "\n",
    "for sheet in sheet_names:\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet)\n",
    "\n",
    "    # Price trend\n",
    "    df['trend change'] = df['Close'] - df['Close'].shift(-1)\n",
    "\n",
    "    def get_trend_direction(change):\n",
    "        if pd.isna(change): return None\n",
    "        elif change > 0: return 'uptrend'\n",
    "        elif change < 0: return 'downtrend'\n",
    "        else: return 'neutral'\n",
    "\n",
    "    df['price_trend_1'] = df['trend change'].apply(get_trend_direction)\n",
    "\n",
    "    def summarize_trend_block(group):\n",
    "        up = (group['price_trend_1'] == 'uptrend').sum()\n",
    "        down = (group['price_trend_1'] == 'downtrend').sum()\n",
    "        group['price_trend_7'] = 'uptrend' if up > down else 'downtrend' if down > up else 'equal'\n",
    "        return group\n",
    "\n",
    "    df = df.groupby('Turn', group_keys=False).apply(summarize_trend_block)\n",
    "\n",
    "    # Volume trend\n",
    "    df['volume_diff'] = df['Volume'] - df['Volume'].shift(-1)\n",
    "\n",
    "    def get_volume_trend_direction(change):\n",
    "        if pd.isna(change): return None\n",
    "        elif change > 0: return 'uptrend'\n",
    "        elif change < 0: return 'downtrend'\n",
    "        else: return 'neutral'\n",
    "\n",
    "    df['volume_trend_1'] = df['volume_diff'].apply(get_volume_trend_direction)\n",
    "\n",
    "    def summarize_volume_trend_block(group):\n",
    "        up = (group['volume_trend_1'] == 'uptrend').sum()\n",
    "        down = (group['volume_trend_1'] == 'downtrend').sum()\n",
    "        group['volume_trend_7'] = 'uptrend' if up > down else 'downtrend' if down > up else 'equal'\n",
    "        return group\n",
    "\n",
    "    df = df.groupby('Turn', group_keys=False).apply(summarize_volume_trend_block)\n",
    "\n",
    "    # MACD\n",
    "    def make_technical_decision(row):\n",
    "        try:\n",
    "            macd = row['MACD (12,26,9)']\n",
    "            signal = row['Signal (12,26,9)']\n",
    "            hist = row['MACD Histogram (12,26,9)']\n",
    "            if macd > signal > hist: return 'Buy'\n",
    "            elif macd < signal and macd < hist: return 'Sell'\n",
    "            else: return 'Neutral'\n",
    "        except: return 'Neutral'\n",
    "\n",
    "    df['MACD_trend'] = df.apply(make_technical_decision, axis=1)\n",
    "\n",
    "    # Bollinger\n",
    "    def classify_bollinger(row):\n",
    "        try:\n",
    "            if row['Close'] > row['Top Bollinger Bands (20,O,2,ma,n)']:\n",
    "                return 'Over Bought'\n",
    "            elif row['Close'] < row['Bottom Bollinger Bands (20,O,2,ma,n)']:\n",
    "                return 'Over Sold'\n",
    "            else:\n",
    "                return 'Neutral'\n",
    "        except:\n",
    "            return 'Neutral'\n",
    "\n",
    "    df['bollinger_trend'] = df.apply(classify_bollinger, axis=1)\n",
    "\n",
    "    df['MACD_trend_7'] = df.groupby('Turn')['MACD_trend'].transform(lambda x: x.mode().iloc[0] if not x.mode().empty else None)\n",
    "    df['bollinger_trend_7'] = df.groupby('Turn')['bollinger_trend'].transform(lambda x: x.mode().iloc[0] if not x.mode().empty else None)\n",
    "\n",
    "    turn_data[sheet] = df\n",
    "\n",
    "# === Export full trend summary ===\n",
    "output_path = r'E:\\FYP\\FYP Symposium\\Outputs\\Combined_Turn_With_Trend_Summary.xlsx'\n",
    "with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
    "    for stock, df in turn_data.items():\n",
    "        df.to_excel(writer, sheet_name=stock, index=False)\n",
    "\n",
    "print(f\"✅ Exported successfully with trend summaries (Close & Volume) for all sheets to: {output_path}\")\n",
    "\n",
    "# === NEW: Load only Turn 1–5 top rows for enrichment ===\n",
    "turn_summary_1to6 = {}\n",
    "for stock in ['TSLA', 'NFLX', 'PG', 'XOM']:\n",
    "    df = pd.read_excel(output_path, sheet_name=stock)\n",
    "    top_rows = df.drop_duplicates(subset='Turn', keep='first')\n",
    "    top_rows = top_rows[top_rows['Turn'].isin([6, 5, 4, 3, 2, 1])]\n",
    "    top_rows = top_rows.sort_values(by='Turn', ascending=False)\n",
    "    turn_summary_1to6[stock] = top_rows\n",
    "    \n",
    "# === Precompute volatility per Turn per stock from raw Turn Data ===\n",
    "volatility_lookup = {}\n",
    "for stock in ['TSLA', 'NFLX', 'PG', 'XOM']:\n",
    "    df = pd.read_excel(r'E:\\FYP\\FYP Symposium\\Turn Data in descending order.xlsx', sheet_name=stock)\n",
    "    vol_by_turn = df.groupby('Turn')['Close'].std().reset_index()\n",
    "    vol_by_turn.rename(columns={'Close': 'Turn_Volatility'}, inplace=True)\n",
    "    volatility_lookup[stock] = vol_by_turn\n",
    "\n",
    "# === Generate trend data table and file ===\n",
    "def generate_trend_basis(stock_df, stock_name):\n",
    "    trend_rows = []\n",
    "    def map_score(val, pos_list, neg_list):\n",
    "                val = str(val).lower()\n",
    "                if val in pos_list:\n",
    "                    return 1\n",
    "                elif val in neg_list:\n",
    "                    return -1\n",
    "                return 0\n",
    "    for turn in sorted(stock_df['Turn'].unique()):\n",
    "        visible_data = stock_df[stock_df['Turn'] <= turn].sort_values(by='Turn', ascending=False).head(5)\n",
    "        if visible_data.empty:\n",
    "            continue\n",
    "        try:\n",
    "            price_trend, volume_spike, macd_signal, bb_trend = detect_market_signals(stock_df, turn)\n",
    "            close = visible_data.iloc[0]['Close']\n",
    "            high = visible_data.iloc[0]['High']\n",
    "            low = visible_data.iloc[0]['Low']\n",
    "            vol_df = volatility_lookup[stock_name]\n",
    "            vol_row = vol_df[vol_df['Turn'] == turn]\n",
    "            volatility = vol_row['Turn_Volatility'].values[0] if not vol_row.empty else None\n",
    "            macd_hist = visible_data.iloc[0]['MACD Histogram (12,26,9)']\n",
    "            macd_hist_strength = round(macd_hist, 4)\n",
    "            volume_support = volume_spike and turn_summary_1to6[stock_name].set_index('Turn').loc[turn, 'volume_trend_7'] == 'uptrend'\n",
    "            trend_reversal = False\n",
    "            turn_list = stock_df['Turn'].sort_values(ascending=False).unique()\n",
    "            turn_idx = list(turn_list).index(turn)\n",
    "            if turn_idx + 1 < len(turn_list):\n",
    "                next_turn = turn_list[turn_idx + 1]\n",
    "                df_enriched = turn_summary_1to6.get(stock_name)\n",
    "                if df_enriched is not None and turn in df_enriched['Turn'].values and next_turn in df_enriched['Turn'].values:\n",
    "                    current_trend = df_enriched.set_index('Turn').loc[turn, 'price_trend_7']\n",
    "                    next_trend = df_enriched.set_index('Turn').loc[next_turn, 'price_trend_7']\n",
    "                    trend_reversal = current_trend != next_trend\n",
    "            \n",
    "            # --- Scoring each indicator (individual columns) ---\n",
    "            score_price_trend = map_score(price_trend, ['uptrend'], ['downtrend'])\n",
    "            score_macd_signal = map_score(macd_signal, ['buy'], ['sell'])\n",
    "            score_bb_trend = map_score(bb_trend, ['oversold'], ['overbought'])\n",
    "            score_volume_spike = 1 if volume_spike else 0\n",
    "\n",
    "            # --- Summary trend scores ---\n",
    "            summary_df = turn_summary_1to6[stock_name].set_index('Turn')\n",
    "            if turn not in summary_df.index:\n",
    "                print(f\"⚠️ Turn {turn} not found in turn_summary for {stock_name}\")\n",
    "                continue\n",
    "            summary = summary_df.loc[turn]\n",
    "\n",
    "            score_volume_trend = map_score(summary['volume_trend_7'], ['uptrend'], ['downtrend'])\n",
    "            score_macd_trend = map_score(summary['MACD_trend_7'], ['buy'], ['sell'])\n",
    "            score_bb_summary = map_score(summary['bollinger_trend_7'], ['oversold'], ['overbought'])\n",
    "            score_price_summary = map_score(summary['price_trend_7'], ['uptrend'], ['downtrend'])\n",
    "            score_trend_reversal = map_score('Yes' if trend_reversal else 'No', ['Yes'], [])\n",
    "            score_volume_support = map_score('Yes' if volume_support else 'No', ['Yes'], [])\n",
    "\n",
    "            # --- Final score calculation ---\n",
    "            \n",
    "            score = (\n",
    "                score_price_trend +\n",
    "                score_macd_signal +\n",
    "                score_bb_trend +\n",
    "                score_volume_spike +\n",
    "                score_volume_trend +\n",
    "                score_macd_trend +\n",
    "                score_bb_summary +\n",
    "                score_price_summary +\n",
    "                score_trend_reversal +\n",
    "                score_volume_support\n",
    "            )\n",
    "\n",
    "\n",
    "            score = round(score, 2)  # Ensure consistent float precision\n",
    "            \n",
    "            # Summary trend indicators\n",
    "            score += (\n",
    "                map_score(turn_summary_1to6[stock_name].set_index('Turn').loc[turn, 'volume_trend_7'], ['uptrend'], ['downtrend']) +\n",
    "                map_score(turn_summary_1to6[stock_name].set_index('Turn').loc[turn, 'MACD_trend_7'], ['buy'], ['sell']) +\n",
    "                map_score(turn_summary_1to6[stock_name].set_index('Turn').loc[turn, 'bollinger_trend_7'], ['oversold'], ['overbought']) +\n",
    "                map_score(turn_summary_1to6[stock_name].set_index('Turn').loc[turn, 'price_trend_7'], ['uptrend'], ['downtrend']) +\n",
    "                map_score('Yes' if trend_reversal else 'No', ['Yes'], []) +\n",
    "                map_score('Yes' if volume_support else 'No', ['Yes'], [])\n",
    "            )\n",
    "            # --- Final buy suggestion logic ---\n",
    "            if score >= 3.0:\n",
    "                buy_suggestion = 'Attractive'\n",
    "            elif score >= 1.0:\n",
    "                buy_suggestion = 'Cautious'\n",
    "            else:\n",
    "                buy_suggestion = 'Risky'\n",
    "\n",
    "            \n",
    "\n",
    "            trend_rows.append({\n",
    "                'Turn': turn,\n",
    "                'Close Price': close,\n",
    "                'High Price': high,\n",
    "                'Low Price': low,\n",
    "                'Volatility': volatility,\n",
    "\n",
    "                # Raw factual indicators\n",
    "                'Price Trend': price_trend,\n",
    "                'Score - Price Trend': score_price_trend,\n",
    "                'MACD Signal': macd_signal,\n",
    "                'Score - MACD Signal': score_macd_signal,\n",
    "                'Bollinger Band Trend': bb_trend,\n",
    "                'Score - Bollinger Band': score_bb_trend,\n",
    "                'Volume Spike': volume_spike,\n",
    "                'Score - Volume Spike': score_volume_spike,\n",
    "                'MACD Histogram': macd_hist,\n",
    "                'MACD Histogram Strength': macd_hist_strength,\n",
    "\n",
    "                # Summary trends\n",
    "                'price_trend_7': summary['price_trend_7'],\n",
    "                'Score - Price Trend (7)': score_price_summary,\n",
    "                'volume_trend_7': summary['volume_trend_7'],\n",
    "                'Score - Volume Trend (7)': score_volume_trend,\n",
    "                'MACD_trend_7': summary['MACD_trend_7'],\n",
    "                'Score - MACD Trend (7)': score_macd_trend,\n",
    "                'bollinger_trend_7': summary['bollinger_trend_7'],\n",
    "                'Score - Bollinger Trend (7)': score_bb_summary,\n",
    "\n",
    "                # Flags\n",
    "                'Trend Reversal Flag': 'Yes' if trend_reversal else 'No',\n",
    "                'Score - Trend Reversal': score_trend_reversal,\n",
    "                'Volume Trend Support': 'Yes' if volume_support else 'No',\n",
    "                'Score - Volume Support': score_volume_support,\n",
    "\n",
    "                # Final decision\n",
    "                'Buy Confidence Score': round(score, 2),\n",
    "                'Is Attractive to Buy?': buy_suggestion\n",
    "            })\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing Turn {turn} in {stock_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "    trend_df = pd.DataFrame(trend_rows)\n",
    "    \n",
    "    # ✅ Normalize Buy Confidence Score to 1–5 after DataFrame is built\n",
    "    if 'Buy Confidence Score' in trend_df.columns:\n",
    "        min_score = trend_df['Buy Confidence Score'].min()\n",
    "        max_score = trend_df['Buy Confidence Score'].max()\n",
    "        if min_score != max_score:\n",
    "            trend_df['Buy Confidence Score_Normalized'] = (\n",
    "                (trend_df['Buy Confidence Score'] - min_score) / (max_score - min_score) * 4 + 1\n",
    "            ).round(2)\n",
    "        else:\n",
    "            trend_df['Buy Confidence Score_Normalized'] = 3\n",
    "\n",
    "    # === Combined Signal-Based Trend Reversal ===\n",
    "    trend_df['Trend Reversal Flag'] = trend_df['price_trend_7'] != trend_df['price_trend_7'].shift(-1)\n",
    "    trend_df['Trend Reversal Flag'] = trend_df['Trend Reversal Flag'].apply(lambda x: 'Yes' if x else 'No')\n",
    "\n",
    "    trend_df['Volume Trend Support'] = trend_df.apply(\n",
    "        lambda row: 'Yes' if row['Volume Spike'] and row['volume_trend_7'] == 'uptrend' else 'No',\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # === Combined Signal-Based Trend Reversal ===\n",
    "    trend_df['Trend Reversal Flag'] = trend_df['price_trend_7'] != trend_df['price_trend_7'].shift(-1)\n",
    "    trend_df['Trend Reversal Flag'] = trend_df['Trend Reversal Flag'].apply(lambda x: 'Yes' if x else 'No')\n",
    "\n",
    "    trend_df['Volume Trend Support'] = trend_df.apply(\n",
    "        lambda row: 'Yes' if row['Volume Spike'] and row['volume_trend_7'] == 'uptrend' else 'No',\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "\n",
    "    # === Strict grouped column ordering ===\n",
    "    ordered_cols = [\n",
    "\n",
    "        # Basic Info\n",
    "        'Turn', 'Close Price', 'High Price', 'Low Price', 'Volatility',\n",
    "\n",
    "        # PRICE-RELATED\n",
    "        'Price Trend', 'Score - Price Trend',\n",
    "        'price_trend_7', 'Score - Price Trend (7)',\n",
    "        'Trend Reversal Flag', 'Score - Trend Reversal',\n",
    "\n",
    "        # MACD-RELATED\n",
    "        'MACD Signal', 'Score - MACD Signal',\n",
    "        'MACD Histogram', 'MACD Histogram Strength',\n",
    "        'MACD_trend_7', 'Score - MACD Trend (7)',\n",
    "\n",
    "        # BOLLINGER-RELATED\n",
    "        'Bollinger Band Trend', 'Score - Bollinger Band',\n",
    "        'bollinger_trend_7', 'Score - Bollinger Trend (7)',\n",
    "\n",
    "        # VOLUME-RELATED\n",
    "        'Volume Spike', 'Score - Volume Spike',\n",
    "        'volume_trend_7', 'Score - Volume Trend (7)',\n",
    "        'Volume Trend Support', 'Score - Volume Support',\n",
    "\n",
    "        # FINAL OUTPUT\n",
    "        'Buy Confidence Score', 'Buy Confidence Score_Normalized',\n",
    "        'Is Attractive to Buy?'\n",
    "    ]\n",
    "\n",
    "    # Safely reorder\n",
    "    trend_df = trend_df[[col for col in ordered_cols if col in trend_df.columns]]\n",
    "\n",
    "\n",
    "\n",
    "    return trend_df\n",
    "\n",
    "\n",
    "    \n",
    "# === Run and display ===\n",
    "trend_tables = {}\n",
    "for stock in stock_tables.keys():\n",
    "    df = generate_trend_basis(stock_tables[stock], stock)\n",
    "    df = df[df['Turn'] != 6]  # drop Turn 6\n",
    "    trend_tables[stock] = df  # <-- this line was missing\n",
    "\n",
    "# Display sample tables without color formatting\n",
    "for stock, table in trend_tables.items():\n",
    "    print(f\"\\nSample Trend Table for {stock}:\")\n",
    "    print(table.head(10))  # Show first 10 rows; adjust as needed\n",
    "    \n",
    "# Load Strategy sheet once at the top of BLOCK 5\n",
    "strategy_path = r\"E:\\FYP\\FYP Symposium\\Outputs\\Final_Merged_Log_and_Strategy_With_Scoring.xlsx\"\n",
    "strategy_df = pd.read_excel(strategy_path, sheet_name='Strategy')\n",
    "\n",
    "# Create lookup dictionary for Average Scoring\n",
    "avg_score_dict = strategy_df.set_index('Participant_ID')['Average Scoring'].to_dict()\n",
    "\n",
    "\n",
    "# ========== BLOCK 5: Enrich merged logs with trend-based market context ==========\n",
    "\n",
    "# Load merged log (⚠️ it does NOT contain 'Average Scoring')\n",
    "merged_log_path = r\"E:\\FYP\\FYP Symposium\\Renamed_Merged_Log_and_Strategy.xlsx\"\n",
    "merged_log_df = pd.read_excel(merged_log_path, sheet_name='Merged_Log')\n",
    "\n",
    "# Load Strategy sheet to get Average Scoring\n",
    "strategy_path = r\"E:\\FYP\\FYP Symposium\\Outputs\\Final_Merged_Log_and_Strategy_With_Scoring.xlsx\"\n",
    "strategy_df = pd.read_excel(strategy_path, sheet_name='Strategy')\n",
    "avg_scores = strategy_df.set_index('Participant_ID')['Average Scoring'].to_dict()\n",
    "\n",
    "# Enrichment function\n",
    "def enrich_log_with_trends(merged_log_df, trend_tables):\n",
    "    enriched_rows = []\n",
    "\n",
    "    for idx, row in merged_log_df.iterrows():\n",
    "        pid = row['Participant_ID']\n",
    "        ticker = row['ticker']\n",
    "        turn = row['turn']\n",
    "\n",
    "        try:\n",
    "            trend_row_df = trend_tables.get(ticker)\n",
    "            if trend_row_df is None:\n",
    "                print(f\"⚠️ No trend data for ticker: {ticker}\")\n",
    "                continue\n",
    "\n",
    "            trend_row = trend_row_df[trend_row_df['Turn'] == turn]\n",
    "            if trend_row.empty:\n",
    "                print(f\"⚠️ Turn {turn} not found in trend table for {ticker}\")\n",
    "                continue\n",
    "\n",
    "            trend_data = trend_row.iloc[0][['Is Attractive to Buy?']].to_dict()\n",
    "            enriched_row = row.to_dict()\n",
    "            enriched_row.update(trend_data)\n",
    "            enriched_rows.append(enriched_row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error processing row {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return pd.DataFrame(enriched_rows)\n",
    "\n",
    "# Run enrichment\n",
    "enriched_log_df = enrich_log_with_trends(merged_log_df, trend_tables)\n",
    "\n",
    "def compute_action_signal_value(row):\n",
    "    action = str(row['action']).strip().lower()\n",
    "    signal = str(row['Is Attractive to Buy?']).strip().lower()\n",
    "    avg_score = row.get('Average Scoring', 0)\n",
    "\n",
    "    if action == 'buy' and signal == 'risky':\n",
    "        return 0\n",
    "    elif action == 'sell' and signal == 'risky':\n",
    "        return avg_score\n",
    "    elif action == 'buy' and signal == 'cautious':\n",
    "        return avg_score / 2\n",
    "    elif action == 'sell' and signal == 'cautious':\n",
    "        return avg_score / 2\n",
    "    elif action == 'buy' and signal == 'attractive':\n",
    "        return avg_score\n",
    "    elif action == 'sell' and signal == 'attractive':\n",
    "        return 0\n",
    "    return 0  # Default fallback for unknown values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ========== BLOCK 7: Add Signal Alignment Score ==========\n",
    "def compute_alignment_score(row):\n",
    "    action = str(row['action']).strip().lower()\n",
    "    signal = str(row['Is Attractive to Buy?']).strip().lower()\n",
    "\n",
    "    if signal == 'risky':\n",
    "        return 1 if action == 'sell' else 0\n",
    "    elif signal == 'cautious':\n",
    "        return 0.5\n",
    "    elif signal == 'attractive':\n",
    "        return 1 if action == 'buy' else 0\n",
    "    return 0\n",
    "\n",
    "# Apply the alignment score\n",
    "enriched_log_df['Signal Alignment Score'] = enriched_log_df.apply(compute_alignment_score, axis=1)\n",
    "\n",
    "# Add explanation column for Signal Alignment Score\n",
    "def explain_alignment_score(row):\n",
    "    pid = row['Participant_ID']\n",
    "    score = row['Signal Alignment Score']\n",
    "    avg = avg_scores.get(pid, 0)\n",
    "\n",
    "    if score == 0:\n",
    "        return \"Misaligned – Action contradicts market signal\"\n",
    "    elif score == 0.5:\n",
    "        return \"Partially Aligned – Somewhat matches the signal\"\n",
    "    elif score == 1:\n",
    "        return \"Fully Aligned – Matches the signal strength\"\n",
    "    \n",
    "enriched_log_df['Signal Alignment Explanation'] = enriched_log_df.apply(explain_alignment_score, axis=1)\n",
    "\n",
    "# # Compute overall average alignment per participant\n",
    "# overall_alignment = (\n",
    "#     enriched_log_df.groupby('Participant_ID')['Signal Alignment Score']\n",
    "#     .mean()\n",
    "#     .round(2)\n",
    "#     .to_dict()\n",
    "# )\n",
    "\n",
    "# # Map it back to each row\n",
    "# enriched_log_df['Avg Signal Alignment (Overall)'] = enriched_log_df['Participant_ID'].map(overall_alignment)\n",
    "\n",
    "# # Explain overall alignment quality\n",
    "# def explain_overall_alignment(score):\n",
    "#     if score == 0:\n",
    "#         return \"Completely Misaligned – consistently acted against signals\"\n",
    "#     elif score < 2.0:\n",
    "#         return \"Poor Alignment – often disregarded market trends\"\n",
    "#     elif score < 3.5:\n",
    "#         return \"Moderate Alignment – partial awareness of signals\"\n",
    "#     elif score < 4.5:\n",
    "#         return \"Strong Alignment – usually followed signals correctly\"\n",
    "#     else:\n",
    "#         return \"Excellent Alignment – highly in sync with market trends\"\n",
    "\n",
    "# enriched_log_df['Overall Alignment Explanation'] = enriched_log_df['Avg Signal Alignment (Overall)'].apply(explain_overall_alignment)\n",
    "\n",
    "def infer_actual_usage(row, threshold=3.5):\n",
    "    pid = row['Participant_ID']\n",
    "    avg_score = avg_score_dict.get(pid, 0)\n",
    "    signal_available = str(row.get('Signal Was Available', '')).lower() == 'yes'\n",
    "    aligned = row.get('Signal Alignment Score', 0) >= 0.5\n",
    "\n",
    "    claimed_use = avg_score >= threshold\n",
    "\n",
    "    if signal_available and claimed_use and aligned:\n",
    "        return \"✅ Used Signal – Claimed & Aligned\"\n",
    "    elif signal_available and claimed_use and not aligned:\n",
    "        return \"❌ Ignored Signal – Claimed but not aligned\"\n",
    "    elif signal_available and not claimed_use and aligned:\n",
    "        return \"⚠️ Aligned Accidentally – Didn't claim use\"\n",
    "    elif not signal_available and claimed_use:\n",
    "        return \"⚪ No Signal – Can't assess usage\"\n",
    "    else:\n",
    "        return \"❌ No Use – No claim and no signal used\"\n",
    "\n",
    "\n",
    "\n",
    "def compute_action_signal_value(row):\n",
    "    pid = row['Participant_ID']\n",
    "    action = str(row['action']).strip().lower()\n",
    "    signal = str(row['Is Attractive to Buy?']).strip().lower()\n",
    "    avg_score = avg_score_dict.get(pid, 0)  # Safe dictionary lookup\n",
    "\n",
    "    if action == 'buy' and signal == 'risky':\n",
    "        return 0\n",
    "    elif action == 'sell' and signal == 'risky':\n",
    "        return avg_score\n",
    "    elif action == 'buy' and signal == 'cautious':\n",
    "        return avg_score / 2\n",
    "    elif action == 'sell' and signal == 'cautious':\n",
    "        return avg_score / 2\n",
    "    elif action == 'buy' and signal == 'attractive':\n",
    "        return avg_score\n",
    "    elif action == 'sell' and signal == 'attractive':\n",
    "        return 0\n",
    "    return 0  # Default fallback for unknown values\n",
    "\n",
    "# Apply it\n",
    "enriched_log_df['Action Signal Value'] = enriched_log_df.apply(compute_action_signal_value, axis=1)\n",
    "\n",
    "# Save enriched merged log with signal score\n",
    "final_export_path = r\"E:\\FYP\\FYP Symposium\\Outputs\\Final_Enriched_Log_With_Signal_Score.xlsx\"\n",
    "enriched_log_df.to_excel(final_export_path, index=False)\n",
    "print(f\"✅ Final enriched log WITH signal alignment score exported to: {final_export_path}\")\n",
    "\n",
    "# ========== Export trend tables ==========\n",
    "\n",
    "export_path = r'E:\\FYP\\FYP Symposium\\Outputs\\Final_Trend_Tables.xlsx'\n",
    "with pd.ExcelWriter(export_path, engine='openpyxl') as writer:\n",
    "    for ticker, df in trend_tables.items():\n",
    "        if not df.empty:\n",
    "            df.to_excel(writer, sheet_name=ticker, index=False)\n",
    "\n",
    "print(f\"✅ Final enriched trend tables exported to: {export_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89c37c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported successfully to: E:\\FYP\\FYP Symposium\\Output 2\\Participant_Indicator_Segments.xlsx\n"
     ]
    }
   ],
   "source": [
    "#==== Block 8: Output 2 logic different logic but same inferences ====\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Define file paths\n",
    "base_path = r\"E:\\FYP\\FYP Symposium\"\n",
    "output_path = os.path.join(base_path, \"Output 2\", \"Participant_Indicator_Segments.xlsx\")\n",
    "\n",
    "# Load data\n",
    "log_path = os.path.join(base_path, \"Renamed_Merged_Log_and_Strategy.xlsx\")\n",
    "survey_path = os.path.join(base_path, \"Post-Survey (Responses).xlsx\")\n",
    "\n",
    "merged_df = pd.read_excel(log_path)\n",
    "post_df = pd.read_excel(survey_path)\n",
    "\n",
    "# Extract and rename indicator preferences\n",
    "rank_cols = [col for col in post_df.columns if any(k in col.lower() for k in ['graph', 'data table', 'macd', 'bollinger'])]\n",
    "preference_df = post_df[['Participant ID'] + rank_cols].copy()\n",
    "\n",
    "rename_map = {\n",
    "    col: 'Graph_Rank' for col in preference_df.columns if 'graph' in col.lower()\n",
    "}\n",
    "rename_map.update({\n",
    "    col: 'DataTable_Rank' for col in preference_df.columns if 'data table' in col.lower()\n",
    "})\n",
    "rename_map.update({\n",
    "    col: 'MACD_Rank' for col in preference_df.columns if 'macd' in col.lower()\n",
    "})\n",
    "rename_map.update({\n",
    "    col: 'BB_Rank' for col in preference_df.columns if 'bollinger' in col.lower()\n",
    "})\n",
    "preference_df.rename(columns=rename_map, inplace=True)\n",
    "\n",
    "# Compute trend signals\n",
    "merged_df_sorted = merged_df.sort_values(by=['Participant_ID', 'ticker', 'turn'])\n",
    "merged_df_sorted['Close'] = merged_df_sorted.groupby(['Participant_ID', 'ticker'])['stockportfolio_after'].shift(0)\n",
    "merged_df_sorted['Prev_Close'] = merged_df_sorted.groupby(['Participant_ID', 'ticker'])['stockportfolio_after'].shift(1)\n",
    "merged_df_sorted['price_diff'] = merged_df_sorted['Close'] - merged_df_sorted['Prev_Close']\n",
    "\n",
    "def get_trend_signal(diff):\n",
    "    if pd.isna(diff): return 'neutral'\n",
    "    if diff > 0: return 'buy'\n",
    "    elif diff < 0: return 'sell'\n",
    "    return 'neutral'\n",
    "merged_df_sorted['graph_trend_signal'] = merged_df_sorted['price_diff'].apply(get_trend_signal)\n",
    "\n",
    "def get_macd_signal(diff):\n",
    "    if pd.isna(diff): return 'neutral'\n",
    "    elif diff > 500: return 'buy'\n",
    "    elif diff < -500: return 'sell'\n",
    "    return 'neutral'\n",
    "merged_df_sorted['macd_signal'] = merged_df_sorted['price_diff'].apply(get_macd_signal)\n",
    "\n",
    "merged_df_sorted['rolling_std'] = merged_df_sorted.groupby(['Participant_ID', 'ticker'])['stockportfolio_after'].transform(lambda x: x.rolling(window=3, min_periods=2).std())\n",
    "def get_bb_signal(row):\n",
    "    std = row['rolling_std']\n",
    "    diff = row['price_diff']\n",
    "    if pd.isna(std) or std < 1000: return 'neutral'\n",
    "    elif diff > 0: return 'buy'\n",
    "    elif diff < 0: return 'sell'\n",
    "    return 'neutral'\n",
    "merged_df_sorted['bb_signal'] = merged_df_sorted.apply(get_bb_signal, axis=1)\n",
    "\n",
    "# Merge with preference\n",
    "merged = pd.merge(merged_df_sorted, preference_df, left_on='Participant_ID', right_on='Participant ID', how='left')\n",
    "merged['action_clean'] = merged['action'].str.lower().str.strip()\n",
    "\n",
    "def compute_match(merged, rank_col, signal_col, action_col='action_clean', label=''):\n",
    "    df = merged[merged[rank_col] == 1].copy()\n",
    "    df[f'{label}_match'] = df.apply(lambda row: 1 if row[action_col] == row[signal_col] else 0, axis=1)\n",
    "    score = df.groupby('Participant_ID')[f'{label}_match'].agg(['mean', 'count']).reset_index()\n",
    "    score.columns = ['Participant_ID', f'{label}_Rate', f'{label}_Turns']\n",
    "    return score\n",
    "\n",
    "graph_score = compute_match(merged, 'Graph_Rank', 'graph_trend_signal', label='Graph')\n",
    "macd_score = compute_match(merged, 'MACD_Rank', 'macd_signal', label='MACD')\n",
    "bb_score = compute_match(merged, 'BB_Rank', 'bb_signal', label='BB')\n",
    "\n",
    "combined = graph_score.merge(macd_score, on='Participant_ID', how='outer')\n",
    "combined = combined.merge(bb_score, on='Participant_ID', how='outer')\n",
    "\n",
    "# Fill missing values\n",
    "for col in combined.columns:\n",
    "    if 'Rate' in col or 'Turns' in col:\n",
    "        combined[col] = combined[col].fillna(\"0\")\n",
    "\n",
    "# Add Segments\n",
    "def segment(row):\n",
    "    segments = []\n",
    "    if isinstance(row['Graph_Rate'], float) and row['Graph_Rate'] >= 0.7:\n",
    "        segments.append('Consistent Graph User')\n",
    "    elif isinstance(row['Graph_Rate'], float):\n",
    "        segments.append('Inconsistent Graph User')\n",
    "    if isinstance(row['MACD_Rate'], float) and row['MACD_Rate'] >= 0.7:\n",
    "        segments.append('Consistent MACD User')\n",
    "    elif isinstance(row['MACD_Rate'], float):\n",
    "        segments.append('Inconsistent MACD User')\n",
    "    if isinstance(row['BB_Rate'], float) and row['BB_Rate'] >= 0.7:\n",
    "        segments.append('Consistent BB User')\n",
    "    elif isinstance(row['BB_Rate'], float):\n",
    "        segments.append('Inconsistent BB User')\n",
    "    return ', '.join(segments) if segments else 'No Top-Ranked Indicator Evaluated'\n",
    "\n",
    "combined['Segment'] = combined.apply(segment, axis=1)\n",
    "\n",
    "# Export to your folder\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "combined.to_excel(output_path, index=False)\n",
    "print(f\"Exported successfully to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4b54b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Turn Ticker PriceVol_Signal MACD_Signal Bollinger_Signal Volume_Signal\n",
      "0      1   TSLA            sell        sell             Sell       Neutral\n",
      "1      2   TSLA             buy        sell             Sell       Neutral\n",
      "2      3   TSLA            sell        sell             Sell       Neutral\n",
      "3      4   TSLA             buy        sell             Sell       Neutral\n",
      "4      5   TSLA             buy        sell             Sell       Neutral\n",
      "5      1   NFLX             buy        sell             Sell       Neutral\n",
      "6      2   NFLX            sell        sell             Sell       Neutral\n",
      "7      3   NFLX            sell        sell              Buy       Neutral\n",
      "8      4   NFLX             buy        sell             Sell       Neutral\n",
      "9      5   NFLX            sell        sell             Sell       Neutral\n",
      "10     1     PG             buy         buy              Buy       Neutral\n",
      "11     2     PG            sell         buy              Buy       Neutral\n",
      "12     3     PG             buy        sell             Sell           Buy\n",
      "13     4     PG             buy         buy             Sell       Neutral\n",
      "14     5     PG             buy         buy              Buy       Neutral\n",
      "✅ Clean indicator signal file exported to: E:\\FYP\\FYP Symposium\\Output 2\\Indicator_Map_With_Signals_CLEAN.xlsx\n",
      "  Participant_ID    Indicator_1st    Indicator_2nd Indicator_3rd  \\\n",
      "0          E0070         High/Low           Volume    Open/Close   \n",
      "1          E0169       Open/Close  Bollinger Bands        Volume   \n",
      "2          E0426       Open/Close         High/Low        Volume   \n",
      "3          E0712       Open/Close  Bollinger Bands          MACD   \n",
      "4          E1130  Bollinger Bands             MACD        Volume   \n",
      "\n",
      "     Indicator_4th    Indicator_5th  MACD_rank_score  High/Low_rank_score  \\\n",
      "0  Bollinger Bands             MACD                1                    5   \n",
      "1         High/Low             MACD                1                    2   \n",
      "2             MACD  Bollinger Bands                2                    4   \n",
      "3           Volume         High/Low                3                    1   \n",
      "4         High/Low       Open/Close                4                    2   \n",
      "\n",
      "   Volume_rank_score  Open/Close_rank_score  Bollinger Bands_rank_score  \\\n",
      "0                  4                      3                           2   \n",
      "1                  3                      5                           4   \n",
      "2                  3                      5                           1   \n",
      "3                  2                      5                           4   \n",
      "4                  3                      1                           5   \n",
      "\n",
      "   price_rank_score  \n",
      "0               4.0  \n",
      "1               3.5  \n",
      "2               4.5  \n",
      "3               3.0  \n",
      "4               1.5  \n"
     ]
    }
   ],
   "source": [
    "#=====Block 9 Map The indicators to their preferences and score them. HL and OP and volatility should be weighted and avged out=====\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your trend table Excel file\n",
    "trend_file_path = r\"E:\\FYP\\FYP Symposium\\Outputs\\Final_Trend_Tables.xlsx\"\n",
    "\n",
    "# Sheet names to load\n",
    "sheet_names = ['TSLA', 'NFLX', 'PG', 'XOM']\n",
    "\n",
    "# Create an empty DataFrame\n",
    "indicator_map = pd.DataFrame()\n",
    "\n",
    "# Loop through each sheet and combine data\n",
    "for sheet in sheet_names:\n",
    "    df = pd.read_excel(trend_file_path, sheet_name=sheet)\n",
    "    df['Ticker'] = sheet  # Add ticker column\n",
    "    indicator_map = pd.concat([indicator_map, df], ignore_index=True)\n",
    "\n",
    "# --- Price + Volatility signal ---\n",
    "def compute_price_vol_signal(row):\n",
    "    trend = str(row.get('price_trend_7', '')).lower()\n",
    "    vol = row.get('Volatility', None)\n",
    "\n",
    "    price_score = 1 if trend == 'uptrend' else -1 if trend == 'downtrend' else 0\n",
    "\n",
    "    if vol is None or pd.isna(vol):\n",
    "        vol_score = 0\n",
    "    elif vol < 10:\n",
    "        vol_score = 1\n",
    "    elif vol > 15:\n",
    "        vol_score = -1\n",
    "    else:\n",
    "        vol_score = 0\n",
    "\n",
    "    weighted_signal = 0.7 * price_score + 0.3 * vol_score\n",
    "    return 'buy' if weighted_signal > 0 else 'sell'\n",
    "\n",
    "indicator_map['PriceVol_Signal'] = indicator_map.apply(compute_price_vol_signal, axis=1)\n",
    "\n",
    "# --- MACD + Histogram signal ---\n",
    "def compute_macd_signal(row):\n",
    "    macd_trend = str(row.get('MACD_trend_7', '')).lower()\n",
    "    hist_strength = row.get('MACD Histogram Strength', 0)\n",
    "\n",
    "    macd_score = 1 if macd_trend == 'buy' else -1 if macd_trend == 'sell' else 0\n",
    "    hist_score = 1 if hist_strength > 0.5 else -1 if hist_strength < -0.5 else 0\n",
    "\n",
    "    weighted_macd = 0.6 * macd_score + 0.4 * hist_score\n",
    "    return 'buy' if weighted_macd > 0 else 'sell'\n",
    "\n",
    "indicator_map['MACD_Signal'] = indicator_map.apply(compute_macd_signal, axis=1)\n",
    "\n",
    "def score_bollinger(row):\n",
    "    # Bollinger trend mapping\n",
    "    bb = str(row['Bollinger Band Trend']).lower()\n",
    "    bb_score = 1 if bb == 'oversold' else -1 if bb == 'overbought' else 0\n",
    "    \n",
    "    # Volatility normalized contribution (higher vol = less confidence)\n",
    "    vol = row.get('Volatility', 0)\n",
    "    vol_score = -1 if vol > 10 else (1 if vol < 5 else 0)\n",
    "\n",
    "    # Weighted average\n",
    "    final_score = (0.7 * bb_score) + (0.3 * vol_score)\n",
    "\n",
    "    if final_score > 0.25:\n",
    "        return 'Buy'\n",
    "    elif final_score < -0.25:\n",
    "        return 'Sell'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "\n",
    "indicator_map['Bollinger_Signal'] = indicator_map.apply(score_bollinger, axis=1)\n",
    "\n",
    "def score_volume(row):\n",
    "    spike = row.get('Volume Spike', False)\n",
    "    trend = str(row.get('volume_trend_7', '')).lower()\n",
    "\n",
    "    if spike:\n",
    "        if trend == 'uptrend':\n",
    "            return 'Buy'\n",
    "        elif trend == 'downtrend':\n",
    "            return 'Sell'\n",
    "        else:\n",
    "            return 'Neutral'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "        \n",
    "indicator_map['Volume_Signal'] = indicator_map.apply(score_volume, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Preview key signals\n",
    "print(indicator_map[['Turn', 'Ticker', 'PriceVol_Signal',\n",
    "                    'MACD_Signal', 'Bollinger_Signal', 'Volume_Signal']].head(15))\n",
    "\n",
    "# File path to save\n",
    "export_path = r\"E:\\FYP\\FYP Symposium\\Output 2\\Indicator_Map_With_Signals_CLEAN.xlsx\"\n",
    "\n",
    "# Columns to keep\n",
    "final_cols = ['Turn', 'Ticker', 'PriceVol_Signal', 'MACD_Signal', 'Bollinger_Signal', 'Volume_Signal']\n",
    "\n",
    "# Filter only required columns per ticker and save to separate sheets\n",
    "with pd.ExcelWriter(export_path, engine='openpyxl') as writer:\n",
    "    for ticker, df in indicator_map.groupby('Ticker'):\n",
    "        df_final = df[final_cols].copy()\n",
    "        df_final.to_excel(writer, sheet_name=ticker, index=False)\n",
    "\n",
    "print(f\"✅ Clean indicator signal file exported to: {export_path}\")\n",
    "\n",
    "\n",
    "\n",
    "#======Decision factors ko dimagh men rakh kar decision lena hai=====\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# File path and sheet name\n",
    "file_path = r\"E:\\FYP\\FYP Symposium\\Renamed_Merged_Log_and_Strategy.xlsx\"\n",
    "sheet_name = 'Strategy'\n",
    "\n",
    "# Columns to load\n",
    "columns_to_use = [\n",
    "    'Participant_ID',\n",
    "    'Indicator_1st',\n",
    "    'Indicator_2nd',\n",
    "    'Indicator_3rd',\n",
    "    'Indicator_4th',\n",
    "    'Indicator_5th'\n",
    "]\n",
    "\n",
    "# Load only the specified columns\n",
    "decision_factor_ranked = pd.read_excel(file_path, sheet_name=sheet_name, usecols=columns_to_use)\n",
    "\n",
    "# Define the indicators we want to score\n",
    "indicators = ['MACD', 'High/Low', 'Volume', 'Open/Close', 'Bollinger Bands']\n",
    "\n",
    "# Initialize new columns with 0\n",
    "for indicator in indicators:\n",
    "    decision_factor_ranked[f'{indicator}_rank_score'] = 0\n",
    "\n",
    "# Assign scores: 5 for 1st rank, down to 1 for 5th rank\n",
    "score_map = {\n",
    "    'Indicator_1st': 5,\n",
    "    'Indicator_2nd': 4,\n",
    "    'Indicator_3rd': 3,\n",
    "    'Indicator_4th': 2,\n",
    "    'Indicator_5th': 1\n",
    "}\n",
    "\n",
    "# Loop through rows and assign scores\n",
    "for col, score in score_map.items():\n",
    "    for indicator in indicators:\n",
    "        decision_factor_ranked.loc[\n",
    "            decision_factor_ranked[col] == indicator,\n",
    "            f'{indicator}_rank_score'\n",
    "        ] += score\n",
    "\n",
    "# Calculate average score for price-based indicators: High/Low and Open/Close\n",
    "decision_factor_ranked['price_rank_score'] = (\n",
    "    decision_factor_ranked['High/Low_rank_score'] + decision_factor_ranked['Open/Close_rank_score']\n",
    ") / 2\n",
    "\n",
    "# Preview the result\n",
    "print(decision_factor_ranked.head())\n",
    "\n",
    "export_path = r\"E:\\FYP\\FYP Symposium\\Output 2\\Decision_Factor_Ranked_Scores.xlsx\"\n",
    "decision_factor_ranked.to_excel(export_path, index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8a98bfad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Merged updated knowledge into strategy successfully.\n",
      "\n",
      "✅ Final Knowledge Score (range 0.5–3.0):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>Knows_MACD</th>\n",
       "      <th>Knows_Bollinger</th>\n",
       "      <th>Knows_Volume</th>\n",
       "      <th>Knowledge_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E0070</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E0169</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E0426</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E0712</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E1130</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Participant_ID Knows_MACD Knows_Bollinger Knows_Volume  Knowledge_Score\n",
       "0          E0070         No              No           No              0.5\n",
       "1          E0169        Yes             Yes          Yes              3.0\n",
       "2          E0426         No              No           No              0.5\n",
       "3          E0712         No              No           No              0.5\n",
       "4          E1130        Yes              No           No              1.5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Participant_ID  turn ticker action  PriceVol_action_score  \\\n",
      "0          E0070     1   TSLA    Buy                      0   \n",
      "1          E0070     2   TSLA   Sell                      0   \n",
      "\n",
      "   MACD_action_score  Bollinger_action_score  Volume_action_score  \\\n",
      "0                  0                       0                  2.0   \n",
      "1                  1                       2                  2.0   \n",
      "\n",
      "   Total_action_score Knows_MACD Knows_Bollinger Knows_Volume  \\\n",
      "0                 2.0         No              No           No   \n",
      "1                 5.0         No              No           No   \n",
      "\n",
      "   used_MACD_Know_Score  used_Bollinger_Know_Score  used_Volume_Know_Score  \\\n",
      "0                     0                          0                       0   \n",
      "1                     0                          0                       0   \n",
      "\n",
      "   Combined_Action_Knowledge_Score  \n",
      "0                              2.0  \n",
      "1                              5.0  \n",
      "✅ Final participant-indicator preference file with action scores exported to: E:\\FYP\\FYP Symposium\\Output 2\\Decision_Factor_Ranked_Scores.xlsx\n"
     ]
    }
   ],
   "source": [
    "#==== Block 10: Load merged log and strategy sheet to see indicator and preferences====#\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load all sheets from the Indicator Map file\n",
    "indicator_file = r\"E:\\FYP\\FYP Symposium\\Output 2\\Indicator_Map_With_Signals_CLEAN.xlsx\"\n",
    "all_sheets = pd.read_excel(indicator_file, sheet_name=None)\n",
    "\n",
    "# Combine all sheets into one DataFrame\n",
    "indicator_map = pd.concat(\n",
    "    [df.assign(Ticker=name) for name, df in all_sheets.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Ensure consistent column naming\n",
    "indicator_map.rename(columns={'Turn': 'turn'}, inplace=True)\n",
    "\n",
    "# Load the base participant data\n",
    "log_file = r\"E:\\FYP\\FYP Symposium\\Renamed_Merged_Log_and_Strategy.xlsx\"\n",
    "participant_cols = [\n",
    "    'Date', 'Real Time', 'Simulation Time', 'Seconds left',\n",
    "    'Participant_ID', 'turn', 'action', 'ticker'\n",
    "]\n",
    "participant_df = pd.read_excel(log_file, sheet_name='Merged_Log', usecols=participant_cols)\n",
    "\n",
    "# Merge signal info from indicator_map using both 'turn' and 'ticker'\n",
    "merged_df = participant_df.merge(\n",
    "    indicator_map[['turn', 'Ticker', 'PriceVol_Signal', 'MACD_Signal', 'Bollinger_Signal', 'Volume_Signal']],\n",
    "    how='left',\n",
    "    left_on=['turn', 'ticker'],\n",
    "    right_on=['turn', 'Ticker']\n",
    ")\n",
    "\n",
    "# Drop the duplicate 'Ticker' column\n",
    "merged_df.drop(columns='Ticker', inplace=True)\n",
    "\n",
    "# Load decision factor scores\n",
    "ranked_file = r\"E:\\FYP\\FYP Symposium\\Output 2\\Decision_Factor_Ranked_Scores.xlsx\"\n",
    "ranked_df = pd.read_excel(ranked_file)\n",
    "\n",
    "# Drop High/Low and Open/Close rank scores\n",
    "ranked_df = ranked_df.drop(columns=['High/Low_rank_score', 'Open/Close_rank_score'])\n",
    "\n",
    "# Merge scores based on Participant_ID\n",
    "final_df = merged_df.merge(ranked_df, on='Participant_ID', how='left')\n",
    "\n",
    "# === Assign action-based signal scores ===\n",
    "def compute_score(row, signal_col, score_col):\n",
    "    signal = str(row[signal_col]).strip().lower()\n",
    "    action = str(row['action']).strip().lower()\n",
    "    rank_score = row.get(score_col, 0)\n",
    "\n",
    "    if signal == 'neutral':\n",
    "        return rank_score / 2\n",
    "    elif signal == action:\n",
    "        return rank_score\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply the logic to each indicator\n",
    "final_df['PriceVol_action_score'] = final_df.apply(lambda r: compute_score(r, 'PriceVol_Signal', 'High/Low_rank_score'), axis=1)\n",
    "final_df['MACD_action_score'] = final_df.apply(lambda r: compute_score(r, 'MACD_Signal', 'MACD_rank_score'), axis=1)\n",
    "final_df['Bollinger_action_score'] = final_df.apply(lambda r: compute_score(r, 'Bollinger_Signal', 'Bollinger Bands_rank_score'), axis=1)\n",
    "final_df['Volume_action_score'] = final_df.apply(lambda r: compute_score(r, 'Volume_Signal', 'Volume_rank_score'), axis=1)\n",
    "\n",
    "# Add Total Action Score by summing all individual action scores\n",
    "final_df['Total_action_score'] = (\n",
    "    final_df['PriceVol_action_score'] +\n",
    "    final_df['MACD_action_score'] +\n",
    "    final_df['Bollinger_action_score'] +\n",
    "    final_df['Volume_action_score']\n",
    ")\n",
    "\n",
    "final_df = final_df[final_df['turn'] != 6]\n",
    "\n",
    "# Add explanation for Total_action_score\n",
    "def explain_action_alignment(row):\n",
    "    scores = {\n",
    "        'PriceVol': row['PriceVol_action_score'],\n",
    "        'MACD': row['MACD_action_score'],\n",
    "        'Bollinger': row['Bollinger_action_score'],\n",
    "        'Volume': row['Volume_action_score']\n",
    "    }\n",
    "    strong_matches = [k for k, v in scores.items() if v >= 0.75]\n",
    "    neutral_matches = [k for k, v in scores.items() if 0 < v < 0.75]\n",
    "    misses = [k for k, v in scores.items() if v == 0]\n",
    "\n",
    "    explanation = []\n",
    "    if strong_matches:\n",
    "        explanation.append(f\"Strong alignment with {', '.join(strong_matches)}\")\n",
    "    if neutral_matches:\n",
    "        explanation.append(f\"Partially followed signals from {', '.join(neutral_matches)}\")\n",
    "    if misses:\n",
    "        explanation.append(f\"Ignored or mismatched {', '.join(misses)}\")\n",
    "\n",
    "    return \" | \".join(explanation)\n",
    "\n",
    "final_df['Action_Score_Explanation'] = final_df.apply(explain_action_alignment, axis=1)\n",
    "\n",
    "#Pulled the three columns from BLOCK 3 about knowledge from pre data if they know how to use what indicator\n",
    "\n",
    "def check_knowledge(prm2b_14a_response):\n",
    "    if pd.isna(prm2b_14a_response):\n",
    "        return {'MACD': False, 'Bollinger': False, 'Volume': False, 'HighLow_OpenClose': False}\n",
    "\n",
    "    text = prm2b_14a_response.lower()\n",
    "    \n",
    "    knowledge = {\n",
    "        'MACD': False,\n",
    "        'Bollinger': False,\n",
    "        'Volume': False,\n",
    "        'HighLow_OpenClose': False\n",
    "    }\n",
    "\n",
    "    # Check MACD knowledge\n",
    "    if 'macd' in text:\n",
    "        knowledge['MACD'] = True\n",
    "    \n",
    "    # Check Bollinger Bands\n",
    "    if 'bollinger' in text or 'bb' in text:\n",
    "        knowledge['Bollinger'] = True\n",
    "\n",
    "    # Check Volume\n",
    "    if 'volume' in text:\n",
    "        knowledge['Volume'] = True\n",
    "\n",
    "    # Check High Low Open Close\n",
    "    if 'high' in text or 'low' in text or 'open' in text or 'close' in text or 'ohlc' in text:\n",
    "        knowledge['HighLow_OpenClose'] = True\n",
    "\n",
    "    return knowledge\n",
    "\n",
    "# --- Apply to all participants ---\n",
    "\n",
    "knowledge_records = []\n",
    "\n",
    "for idx, row in strategy.iterrows():\n",
    "    participant_id = row['Participant_ID']\n",
    "    prm2b_14a = row.get('PRM2b_14a', None)\n",
    "    knowledge = check_knowledge(prm2b_14a)\n",
    "\n",
    "    knowledge_records.append({\n",
    "        'Participant_ID': participant_id,\n",
    "        'Knows_MACD': 'Yes' if knowledge['MACD'] else 'No',\n",
    "        'Knows_Bollinger': 'Yes' if knowledge['Bollinger'] else 'No',\n",
    "        'Knows_Volume': 'Yes' if knowledge['Volume'] else 'No',\n",
    "        'Knows_HighLow_OpenClose': 'Yes' if knowledge['HighLow_OpenClose'] else 'No'\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "knowledge_df = pd.DataFrame(knowledge_records)\n",
    "\n",
    "# Drop 'Knows_HighLow_OpenClose' — no longer needed\n",
    "if 'Knows_HighLow_OpenClose' in knowledge_df.columns:\n",
    "    knowledge_df.drop(columns=['Knows_HighLow_OpenClose'], inplace=True)\n",
    "\n",
    "# --- Merge Cleaned Knowledge into Strategy ---\n",
    "\n",
    "# Drop old versions to avoid MergeError\n",
    "strategy.drop(columns=['Knows_MACD', 'Knows_Bollinger', 'Knows_Volume', 'Knows_HighLow_OpenClose', 'Knowledge_Score'], errors='ignore', inplace=True)\n",
    "\n",
    "# Merge new knowledge\n",
    "strategy = pd.merge(strategy, knowledge_df, on='Participant_ID', how='left')\n",
    "print(\"\\n✅ Merged updated knowledge into strategy successfully.\")\n",
    "\n",
    "# --- Scoring Logic ---\n",
    "\n",
    "# Map Yes/No with new scoring logic (Volume: Yes = 1, No = 0.5)\n",
    "knowledge_df_numeric = knowledge_df.copy()\n",
    "knowledge_df_numeric['Knows_MACD'] = knowledge_df_numeric['Knows_MACD'].map({'Yes': 1, 'No': 0})\n",
    "knowledge_df_numeric['Knows_Bollinger'] = knowledge_df_numeric['Knows_Bollinger'].map({'Yes': 1, 'No': 0})\n",
    "knowledge_df_numeric['Knows_Volume'] = knowledge_df_numeric['Knows_Volume'].map({'Yes': 1, 'No': 0.5})\n",
    "\n",
    "# Compute score\n",
    "knowledge_df['Knowledge_Score'] = (\n",
    "    knowledge_df_numeric['Knows_MACD'] +\n",
    "    knowledge_df_numeric['Knows_Bollinger'] +\n",
    "    knowledge_df_numeric['Knows_Volume']\n",
    ")\n",
    "\n",
    "# Show sample output\n",
    "print(\"\\n✅ Final Knowledge Score (range 0.5–3.0):\")\n",
    "display(knowledge_df[['Participant_ID', 'Knows_MACD', 'Knows_Bollinger', 'Knows_Volume', 'Knowledge_Score']].head())\n",
    "\n",
    "# Optional documentation string\n",
    "knowledge_score_note = \"Knowledge_Score = Knows_MACD (1/0) + Knows_Bollinger (1/0) + Knows_Volume (1 if Yes, 0.5 if No); Range = 0.5 to 3.0\"\n",
    "\n",
    "\n",
    "final_df = final_df.merge(strategy[['Participant_ID', 'Knows_MACD', 'Knows_Bollinger', 'Knows_Volume']], on='Participant_ID', how='left')\n",
    "\n",
    "def compute_used_know_score(row, signal_col, know_col):\n",
    "    signal = str(row[signal_col]).strip().lower()\n",
    "    action = str(row['action']).strip().lower()\n",
    "    knows = row[know_col]\n",
    "\n",
    "    if signal == action and knows == 'Yes':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply logic for each indicator\n",
    "final_df['used_MACD_Know_Score'] = final_df.apply(lambda r: compute_used_know_score(r, 'MACD_Signal', 'Knows_MACD'), axis=1)\n",
    "final_df['used_Bollinger_Know_Score'] = final_df.apply(lambda r: compute_used_know_score(r, 'Bollinger_Signal', 'Knows_Bollinger'), axis=1)\n",
    "final_df['used_Volume_Know_Score'] = final_df.apply(lambda r: compute_used_know_score(r, 'Volume_Signal', 'Knows_Volume'), axis=1)\n",
    "\n",
    "# Optional: total informed usage score\n",
    "final_df['Total_Used_Know_Score'] = (\n",
    "    final_df['used_MACD_Know_Score'] +\n",
    "    final_df['used_Bollinger_Know_Score'] +\n",
    "    final_df['used_Volume_Know_Score']\n",
    ")\n",
    "\n",
    "final_df['Combined_Action_Knowledge_Score'] = (\n",
    "    final_df['Total_action_score'] + final_df['Total_Used_Know_Score']\n",
    ")\n",
    "\n",
    "\n",
    "print(final_df[['Participant_ID', 'turn', 'ticker', 'action',\n",
    "                'PriceVol_action_score', 'MACD_action_score',\n",
    "                'Bollinger_action_score', 'Volume_action_score',\n",
    "                'Total_action_score', 'Knows_MACD', 'Knows_Bollinger', 'Knows_Volume',\n",
    "                'used_MACD_Know_Score', 'used_Bollinger_Know_Score', 'used_Volume_Know_Score', 'Combined_Action_Knowledge_Score']].head(2))\n",
    "\n",
    "# Export the final dataframe\n",
    "final_df.to_excel(\n",
    "    r\"E:\\FYP\\FYP Symposium\\Output 2\\participant_indicator_preference_with_signals.xlsx\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"✅ Final participant-indicator preference file with action scores exported to: {export_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4160e57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Merged Participant Knowledge into Strategy Frame.\n",
      "\n",
      "✅ Rationality Label and Score Assigned to merged_log.\n",
      "\n",
      "✅ Behavioral Rationality Log Saved Successfully at E:\\FYP\\FYP Symposium\\Outputs\\Behavioral_Rationality_Log_2025-05-02 05-25-59.xlsx\n",
      "\n",
      "✅ Market Signals + Rationality Sheet Saved Successfully!\n",
      "\n",
      "✅ Participant Behavioral Summary Sheet Saved Successfully!\n",
      "\n",
      "📊 FINAL OUTPUT: Top 10 Rows of Market Signals + Rationality Data:\n",
      "\n",
      "Participant_ID  Turn Ticker  Volatility Price Trend MACD Signal  MACD Histogram  Volume Spike Bollinger Band Trend Is Attractive to Buy?  DecisionFactor_Score  Indicator_Score  Knowledge_Bonus  Rationality Score  Rationality Label\n",
      "         E0070     1   TSLA   26.239602     neutral     neutral           -7.68         False              neutral                 Risky                   0.0              0.0              0.0                0.0         Irrational\n",
      "         E0070     2   TSLA   11.078977     neutral     neutral           -2.91         False              neutral            Attractive                   0.0              0.0              0.0                0.0         Irrational\n",
      "         E0070     2     PG    0.973186     neutral     neutral            0.21         False              neutral              Cautious                   0.0              0.0              0.0                0.0         Irrational\n",
      "         E0070     3     PG    2.476546     uptrend         buy            0.19          True           overbought            Attractive                   0.0              0.2              0.0                0.2         Irrational\n",
      "         E0070     3   NFLX   25.659311   downtrend        sell           -8.80         False             oversold                 Risky                   0.0              0.0              0.0                0.0 Emotionally Driven\n",
      "         E0070     4     PG    2.935980     uptrend         buy            0.93         False           overbought            Attractive                   0.5              0.1              0.0                0.6  Slightly Rational\n",
      "         E0070     5   NFLX   11.881903   downtrend         buy            1.83         False              neutral                 Risky                   0.5              0.0              0.0                0.5  Slightly Rational\n",
      "         E0070     5     PG    1.772159     uptrend         buy            0.34         False              neutral            Attractive                   0.0              0.0              0.0                0.0         Irrational\n",
      "         E0070     5   NFLX   11.881903   downtrend         buy            1.83         False              neutral                 Risky                   0.0              0.1              0.0                0.1         Irrational\n",
      "         E0169     1   TSLA   26.239602     neutral     neutral           -7.68         False              neutral                 Risky                   0.0              0.0              0.0                0.0         Irrational\n"
     ]
    }
   ],
   "source": [
    "########################## # BLOCK 11: Assign Rationality (Final Version)\n",
    "# --- Merge Extracted Knowledge into Strategy ---\n",
    "\n",
    "# Assuming 'knowledge_df' already created\n",
    "# Merge knowledge_df with strategy on 'Participant_ID'\n",
    "# Drop old columns if exist to avoid suffix errors\n",
    "cols_to_drop = ['Knows_MACD', 'Knows_Bollinger', 'Knows_Volume', 'Knows_HighLow_OpenClose']\n",
    "strategy = strategy.drop(columns=[col for col in cols_to_drop if col in strategy.columns], errors='ignore')\n",
    "\n",
    "# Now merge\n",
    "\n",
    "strategy = pd.merge(strategy, knowledge_df, on='Participant_ID', how='left')\n",
    "\n",
    "print(\"\\n✅ Merged Participant Knowledge into Strategy Frame.\")\n",
    "\n",
    "def assign_rationality(row):\n",
    "    participant_id = row['Participant_ID']\n",
    "    action = str(row['action']).lower()\n",
    "    ticker = str(row['ticker']).upper()\n",
    "    turn = row['turn']\n",
    "\n",
    "    participant_row = strategy[strategy['Participant_ID'] == participant_id]\n",
    "    if participant_row.empty:\n",
    "        return pd.Series(['Unknown', np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "    decision_factors = [\n",
    "        (participant_row['DecisionFactor_1st'].values[0], 0.5),\n",
    "        (participant_row['DecisionFactor_2nd'].values[0], 0.3),\n",
    "        (participant_row['DecisionFactor_3rd'].values[0], 0.2)\n",
    "    ]\n",
    "    indicators = [\n",
    "        (participant_row['Indicator_1st'].values[0], 0.5),\n",
    "        (participant_row['Indicator_2nd'].values[0], 0.4),\n",
    "        (participant_row['Indicator_3rd'].values[0], 0.3),\n",
    "        (participant_row['Indicator_4th'].values[0], 0.2),\n",
    "        (participant_row['Indicator_5th'].values[0], 0.1)\n",
    "    ]\n",
    "    known_indicators = participant_row['PRM2b_14a'].values[0]\n",
    "\n",
    "    trend_table = trend_tables.get(ticker)\n",
    "    if trend_table is None:\n",
    "        return pd.Series(['Unknown', np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "    trend_row = trend_table[trend_table['Turn'] == turn]\n",
    "    if trend_row.empty:\n",
    "        return pd.Series(['Unknown', np.nan, np.nan, np.nan, np.nan])\n",
    "\n",
    "    trend_row = trend_row.iloc[0]\n",
    "\n",
    "    # Step 1: Emotionally Driven\n",
    "    if str(row['news_truth']).lower() == 'false':\n",
    "        if (str(row['news_sentiment']).lower() == 'positive' and action == 'buy') or \\\n",
    "           (str(row['news_sentiment']).lower() == 'negative' and action == 'sell'):\n",
    "            return pd.Series(['Emotionally Driven', 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "    # Step 2: Rationality Scoring\n",
    "    rationality_score = 0\n",
    "    breakdown_scores = {\n",
    "        'DecisionFactor_Score': 0,\n",
    "        'Indicator_Score': 0,\n",
    "        'Knowledge_Bonus': 0\n",
    "    }\n",
    "\n",
    "    for factor, weight in decision_factors:\n",
    "        if str(factor).lower() == 'graph':\n",
    "            if (trend_row['Price Trend'] == 'uptrend' and action == 'buy') or \\\n",
    "               (trend_row['Price Trend'] == 'downtrend' and action == 'sell'):\n",
    "                rationality_score += weight\n",
    "                breakdown_scores['DecisionFactor_Score'] += weight\n",
    "        elif str(factor).lower() == 'news':\n",
    "            if (row['news_sentiment'].lower() == 'positive' and row['news_truth'].lower() == 'true' and action == 'buy') or \\\n",
    "               (row['news_sentiment'].lower() == 'negative' and row['news_truth'].lower() == 'true' and action == 'sell'):\n",
    "                rationality_score += weight\n",
    "                breakdown_scores['DecisionFactor_Score'] += weight\n",
    "\n",
    "    for indicator, weight in indicators:\n",
    "        if pd.isna(indicator):\n",
    "            continue\n",
    "        if str(indicator).lower() == 'volume':\n",
    "            if trend_row['Volume Spike'] and action == 'buy':\n",
    "                rationality_score += weight\n",
    "                breakdown_scores['Indicator_Score'] += weight\n",
    "                if participant_row['Knows_Volume'].values[0] == 'Yes':\n",
    "                    rationality_score += 0.2\n",
    "                    breakdown_scores['Knowledge_Bonus'] += 0.2\n",
    "        elif str(indicator).lower() == 'macd':\n",
    "            if (trend_row['MACD Signal'] == 'buy' and action == 'buy') or (trend_row['MACD Signal'] == 'sell' and action == 'sell'):\n",
    "                rationality_score += weight\n",
    "                breakdown_scores['Indicator_Score'] += weight\n",
    "                if participant_row['Knows_MACD'].values[0] == 'Yes':\n",
    "                    rationality_score += 0.2\n",
    "                    breakdown_scores['Knowledge_Bonus'] += 0.2\n",
    "        elif str(indicator).lower() == 'bollinger bands':\n",
    "            if (trend_row['Bollinger Band Trend'] == 'oversold' and action == 'buy') or \\\n",
    "               (trend_row['Bollinger Band Trend'] == 'overbought' and action == 'sell'):\n",
    "                rationality_score += weight\n",
    "                breakdown_scores['Indicator_Score'] += weight\n",
    "                if participant_row['Knows_Bollinger'].values[0] == 'Yes':\n",
    "                    rationality_score += 0.2\n",
    "                    breakdown_scores['Knowledge_Bonus'] += 0.2\n",
    "\n",
    "    # Step 3: Label\n",
    "    if rationality_score >= 0.7:\n",
    "        label = 'Fully Rational'\n",
    "    elif 0.4 <= rationality_score < 0.7:\n",
    "        label = 'Slightly Rational'\n",
    "    else:\n",
    "        label = 'Irrational'\n",
    "\n",
    "    return pd.Series([\n",
    "        label,\n",
    "        rationality_score,\n",
    "        breakdown_scores['DecisionFactor_Score'],\n",
    "        breakdown_scores['Indicator_Score'],\n",
    "        breakdown_scores['Knowledge_Bonus']\n",
    "    ])\n",
    "merged_log[['rationality_label', 'rationality_score', 'DecisionFactor_Score', 'Indicator_Score', 'Knowledge_Bonus']] = merged_log.apply(assign_rationality, axis=1)\n",
    "\n",
    "\n",
    "print(\"\\n✅ Rationality Label and Score Assigned to merged_log.\")\n",
    "\n",
    "from openpyxl import load_workbook\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate Timestamp Name\n",
    "current_time = datetime.now().strftime(\"%Y-%m-%d %H-%M-%S\")\n",
    "output_path = rf'E:\\FYP\\FYP Symposium\\Outputs\\Behavioral_Rationality_Log_{current_time}.xlsx'\n",
    "\n",
    "# Save merged_log\n",
    "merged_log.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Behavioral Rationality Log Saved Successfully at {output_path}\")\n",
    "\n",
    "# Now prepare Market Signals + Rationality\n",
    "# Now prepare Market Signals + Rationality\n",
    "# Prepare Market Signals + Rationality\n",
    "market_signal_records = []\n",
    "\n",
    "for idx, row in merged_log.iterrows():\n",
    "    ticker = str(row['ticker']).upper()\n",
    "    turn = row['turn']\n",
    "    participant_id = row['Participant_ID']\n",
    "    rationality = row['rationality_label']\n",
    "    rationality_score = row['rationality_score']\n",
    "\n",
    "    trend_table = trend_tables.get(ticker)\n",
    "    if trend_table is None:\n",
    "        continue\n",
    "\n",
    "    trend_row = trend_table[trend_table['Turn'] == turn]\n",
    "    if trend_row.empty:\n",
    "        continue\n",
    "\n",
    "    trend_row = trend_row.iloc[0]\n",
    "    market_signal_records.append({\n",
    "        'Participant_ID': participant_id,\n",
    "        'Turn': turn,\n",
    "        'Ticker': ticker,\n",
    "        'Volatility': trend_row['Volatility'],\n",
    "        'Price Trend': trend_row['Price Trend'],\n",
    "        'MACD Signal': trend_row['MACD Signal'],\n",
    "        'MACD Histogram': trend_row['MACD Histogram'],\n",
    "        'Volume Spike': trend_row['Volume Spike'],\n",
    "        'Bollinger Band Trend': trend_row['Bollinger Band Trend'],\n",
    "        'Is Attractive to Buy?': trend_row['Is Attractive to Buy?'],\n",
    "        'DecisionFactor_Score': row['DecisionFactor_Score'],\n",
    "        'Indicator_Score': row['Indicator_Score'],\n",
    "        'Knowledge_Bonus': row['Knowledge_Bonus'],\n",
    "        'Rationality Score': rationality_score,\n",
    "        'Rationality Label': rationality\n",
    "    })\n",
    "\n",
    "\n",
    "market_signals_df = pd.DataFrame(market_signal_records)\n",
    "# Now open in append mode to add new sheet\n",
    "with pd.ExcelWriter(output_path, engine='openpyxl', mode='a') as writer:\n",
    "    market_signals_df.to_excel(writer, sheet_name='MarketSignals_WithRationality', index=False)\n",
    "\n",
    "print(\"\\n✅ Market Signals + Rationality Sheet Saved Successfully!\")\n",
    "\n",
    "# --- Create Participant Behavioral Summary ---\n",
    "\n",
    "# --- Create Full Participant Behavioral Summary ---\n",
    "\n",
    "summary_records = []\n",
    "\n",
    "participants = merged_log['Participant_ID'].unique()\n",
    "\n",
    "for pid in participants:\n",
    "    participant_data = merged_log[merged_log['Participant_ID'] == pid]\n",
    "    total_decisions = len(participant_data)\n",
    "\n",
    "    fully_rational = len(participant_data[participant_data['rationality_label'] == 'Fully Rational'])\n",
    "    slightly_rational = len(participant_data[participant_data['rationality_label'] == 'Slightly Rational'])\n",
    "    irrational = len(participant_data[participant_data['rationality_label'] == 'Irrational'])\n",
    "    emotionally_driven = len(participant_data[participant_data['rationality_label'] == 'Emotionally Driven'])\n",
    "\n",
    "    summary_records.append({\n",
    "        'Participant_ID': pid,\n",
    "        'Total Actions': total_decisions,\n",
    "        '# Fully Rational': fully_rational,\n",
    "        '# Slightly Rational': slightly_rational,\n",
    "        '# Irrational': irrational,\n",
    "        '# Emotionally Driven': emotionally_driven,\n",
    "        '% Fully Rational': round((fully_rational / total_decisions) * 100, 2),\n",
    "        '% Slightly Rational': round((slightly_rational / total_decisions) * 100, 2),\n",
    "        '% Irrational': round((irrational / total_decisions) * 100, 2),\n",
    "        '% Emotionally Driven': round((emotionally_driven / total_decisions) * 100, 2),\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "summary_df = pd.DataFrame(summary_records)\n",
    "\n",
    "# Determine Overall Participant Classification\n",
    "def classify_participant(row):\n",
    "    labels = {\n",
    "        'Fully Rational': row['% Fully Rational'],\n",
    "        'Slightly Rational': row['% Slightly Rational'],\n",
    "        'Irrational': row['% Irrational'],\n",
    "        'Emotionally Driven': row['% Emotionally Driven']\n",
    "    }\n",
    "    dominant = max(labels, key=labels.get)\n",
    "    return dominant\n",
    "\n",
    "summary_df['Overall Classification'] = summary_df.apply(classify_participant, axis=1)\n",
    "\n",
    "# Save to the Excel file in a new sheet\n",
    "with pd.ExcelWriter(output_path, engine='openpyxl', mode='a') as writer:\n",
    "    summary_df.to_excel(writer, sheet_name='Participant_Summary', index=False)\n",
    "\n",
    "print(\"\\n✅ Participant Behavioral Summary Sheet Saved Successfully!\")\n",
    "print(\"\\n📊 FINAL OUTPUT: Top 10 Rows of Market Signals + Rationality Data:\\n\")\n",
    "print(market_signals_df.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b85798ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Final Decision Quality Scores (with final cash) computed and saved.\n"
     ]
    }
   ],
   "source": [
    "##########################\n",
    "# BLOCK 12: Compute Final Decision Quality Score (Rationality Proxy)\n",
    "##########################\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load files\n",
    "pre_survey = pd.read_excel(r\"E:\\FYP\\FYP Symposium\\Pre-Survey Form (Responses).xlsx\")\n",
    "post_survey = pd.read_excel(r\"E:\\FYP\\FYP Symposium\\Post-Survey (Responses).xlsx\")\n",
    "merged_log = pd.read_excel(r\"E:\\FYP\\FYP Symposium\\Renamed_Merged_Log_and_Strategy.xlsx\")\n",
    "\n",
    "# Clean column headers\n",
    "pre_survey.columns = pre_survey.columns.str.strip()\n",
    "post_survey.columns = post_survey.columns.str.strip()\n",
    "merged_log.columns = merged_log.columns.str.strip()\n",
    "\n",
    "# Rename pre-survey columns\n",
    "pre_survey = pre_survey.rename(columns={\n",
    "    'Participant ID (AXXXX format)\\nIf ID is not assigned yet, kindly contact researchers before proceeding': 'Participant_ID',\n",
    "    'How confident are you in your ability to make profitable trades?': 'Confidence',\n",
    "    'When faced with Financial decisions, do you usually rely on Logic or Intuition?': 'Logic_vs_Intuition',\n",
    "    'How comfortable are you with taking risks?': 'Risk_Tolerance',\n",
    "    'How comfortable are you with uncertainty in Financial Markets?': 'Comfort_with_Uncertainty',\n",
    "    'Do you consider yourself patient or impulsive when making decisions?': 'Impulsiveness'\n",
    "})\n",
    "\n",
    "# Select pre-survey features\n",
    "pre_features = pre_survey[['Participant_ID', 'Confidence', 'Logic_vs_Intuition', 'Risk_Tolerance',\n",
    "                           'Comfort_with_Uncertainty', 'Impulsiveness']]\n",
    "\n",
    "# Rename and select post-survey columns\n",
    "post_survey = post_survey.rename(columns={\n",
    "    'Participant ID': 'Participant_ID',\n",
    "    'How much pressure did you feel during the experiment to perform well?': 'Pressure',\n",
    "    'How well do you think you have performed in the trading simulation?': 'Self_Performance',\n",
    "    'How well do you think you may have performed as compared to other participants?': 'Relative_Performance'\n",
    "})\n",
    "post_features = post_survey[['Participant_ID', 'Pressure', 'Self_Performance', 'Relative_Performance']]\n",
    "\n",
    "# Behavioral summary\n",
    "log_summary = merged_log.groupby('Participant_ID').agg(\n",
    "    Total_Trades=('action', 'count'),\n",
    "    Unique_Tickers=('ticker', pd.Series.nunique)\n",
    ").reset_index()\n",
    "\n",
    "# Final cash (profit proxy) — last row per participant\n",
    "profit_summary = merged_log.sort_values(by=['Participant_ID', 'turn']).groupby('Participant_ID').tail(1)[\n",
    "    ['Participant_ID', 'cash_after']\n",
    "].rename(columns={'cash_after': 'Final_Cash'})\n",
    "\n",
    "# Merge all features\n",
    "merged_df = pre_features.merge(post_features, on='Participant_ID', how='inner')\n",
    "merged_df = merged_df.merge(log_summary, on='Participant_ID', how='inner')\n",
    "merged_df = merged_df.merge(profit_summary, on='Participant_ID', how='left')\n",
    "\n",
    "# Log-transform cash\n",
    "merged_df['Log_Cash'] = np.log(merged_df['Final_Cash'])\n",
    "\n",
    "# Normalize using MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(merged_df.drop(columns=['Participant_ID', 'Final_Cash']))\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=merged_df.columns.drop(['Participant_ID', 'Final_Cash']))\n",
    "scaled_df['Participant_ID'] = merged_df['Participant_ID']\n",
    "scaled_df['Final_Cash'] = merged_df['Final_Cash']  # Keep original value\n",
    "\n",
    "# Compute Final Decision Quality Score\n",
    "scaled_df['Final_Decision_Quality_Score'] = (\n",
    "    0.25 * scaled_df[['Confidence', 'Logic_vs_Intuition', 'Risk_Tolerance',\n",
    "                      'Comfort_with_Uncertainty', 'Impulsiveness']].mean(axis=1) +\n",
    "    0.20 * scaled_df[['Pressure', 'Self_Performance', 'Relative_Performance']].mean(axis=1) +\n",
    "    0.15 * scaled_df[['Total_Trades', 'Unique_Tickers']].mean(axis=1) +\n",
    "    0.40 * scaled_df['Log_Cash']\n",
    ")\n",
    "\n",
    "# ================================\n",
    "# Example Categorization: Rationality Label\n",
    "# ================================\n",
    "scaled_df['Rationality_Label'] = pd.cut(\n",
    "    scaled_df['Final_Decision_Quality_Score'],\n",
    "    bins=[0, 0.4, 0.7, 1.0],\n",
    "    labels=['Irrational', 'Moderate', 'Rational']\n",
    ")\n",
    "# Add Explanation based on Rationality Label and Score\n",
    "def explain_rationality(score, label):\n",
    "    if score >= 0.85:\n",
    "        return \"Highly rational; confident, logical, and performed strongly under pressure with high profit.\"\n",
    "    elif score >= 0.70:\n",
    "        return \"Generally rational; showed strong strategy use and psychological stability.\"\n",
    "    elif score >= 0.55:\n",
    "        return \"Moderately effective; balanced decision-making but either cautious or inconsistent performance.\"\n",
    "    elif score >= 0.40:\n",
    "        return \"Somewhat impulsive or stressed; moderate trading performance and mixed self-perception.\"\n",
    "    else:\n",
    "        return \"Likely struggled under pressure; low confidence, impulsive patterns, or poor final performance.\"\n",
    "\n",
    "scaled_df['Rationality_Explanation'] = scaled_df.apply(\n",
    "    lambda row: explain_rationality(row['Final_Decision_Quality_Score'], row['Rationality_Label']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Add breakdown explanation per participant\n",
    "def build_score_formula(row):\n",
    "    cog = round(row[['Confidence', 'Logic_vs_Intuition', 'Risk_Tolerance',\n",
    "                     'Comfort_with_Uncertainty', 'Impulsiveness']].mean(), 2)\n",
    "    perc = round(row[['Pressure', 'Self_Performance', 'Relative_Performance']].mean(), 2)\n",
    "    behav = round(row[['Total_Trades', 'Unique_Tickers']].mean(), 2)\n",
    "    perf = round(row['Log_Cash'], 2)\n",
    "\n",
    "    return f\"Score = 25% Cognitive ({cog}) + 20% Perception ({perc}) + 15% Behavior ({behav}) + 40% Log-Cash ({perf})\"\n",
    "\n",
    "scaled_df['Score_Breakdown'] = scaled_df.apply(build_score_formula, axis=1)\n",
    "\n",
    "\n",
    "# Save results\n",
    "scaled_df[['Participant_ID', 'Final_Decision_Quality_Score', 'Rationality_Label', 'Rationality_Explanation', 'Score_Breakdown']].to_excel(\n",
    "    r\"E:\\FYP\\FYP Symposium\\Output 2\\Final_Decision_Quality_Scores.xlsx\", index=False)\n",
    "\n",
    "print(\"✅ Final Decision Quality Scores (with final cash) computed and saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
